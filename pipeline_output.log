2026-01-27 18:36:06,733 - __main__ - INFO - ================================================================================
2026-01-27 18:36:06,733 - __main__ - INFO - SENTIMENT ANALYSIS PIPELINE STARTED
2026-01-27 18:36:06,733 - __main__ - INFO - ================================================================================
2026-01-27 18:36:06,733 - __main__ - INFO - 
[Step 1/5] Loading comments data...
2026-01-27 18:36:06,735 - sentiment_analysis.batch_inference - INFO - Loaded 2113 comments from /home/saipraneethmeduri/Desktop/Projects/sentiment_analysis_comments/comments_data/entity_comments_details_20.csv
2026-01-27 18:36:06,735 - __main__ - INFO - 
[Step 2/5] Loading models...
2026-01-27 18:36:06,735 - __main__ - INFO - Model cache directory: /home/saipraneethmeduri/Desktop/Projects/sentiment_analysis_comments/models
2026-01-27 18:36:06,735 - sentiment_analysis.model_loader - INFO - Using CPU for inference
2026-01-27 18:36:06,736 - sentiment_analysis.model_loader - INFO - Loading 4 models on cpu...
2026-01-27 18:36:06,736 - sentiment_analysis.model_loader - INFO - [1/4] Loading google/muril-base-cased...
2026-01-27 18:36:07,053 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/google/muril-base-cased/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-01-27 18:36:07,072 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/google/muril-base-cased/afd9f36c7923d54e97903922ff1b260d091d202f/config.json "HTTP/1.1 200 OK"
2026-01-27 18:36:07,325 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/google/muril-base-cased/resolve/main/model.safetensors "HTTP/1.1 404 Not Found"
2026-01-27 18:36:07,589 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/google/muril-base-cased "HTTP/1.1 200 OK"
Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]Loading weights:   1%|          | 1/199 [00:00<00:00, 31300.78it/s, Materializing param=bert.embeddings.LayerNorm.bias]Loading weights:   1%|          | 1/199 [00:00<00:00, 13443.28it/s, Materializing param=bert.embeddings.LayerNorm.bias]Loading weights:   1%|          | 2/199 [00:00<00:00, 11683.30it/s, Materializing param=bert.embeddings.LayerNorm.weight]Loading weights:   1%|          | 2/199 [00:00<00:00, 10082.46it/s, Materializing param=bert.embeddings.LayerNorm.weight]Loading weights:   2%|▏         | 3/199 [00:00<00:00, 11893.11it/s, Materializing param=bert.embeddings.position_embeddings.weight]Loading weights:   2%|▏         | 3/199 [00:00<00:00, 10547.29it/s, Materializing param=bert.embeddings.position_embeddings.weight]Loading weights:   2%|▏         | 4/199 [00:00<00:00, 12061.26it/s, Materializing param=bert.embeddings.token_type_embeddings.weight]Loading weights:   2%|▏         | 4/199 [00:00<00:00, 11305.40it/s, Materializing param=bert.embeddings.token_type_embeddings.weight]Loading weights:   3%|▎         | 5/199 [00:00<00:00, 12702.31it/s, Materializing param=bert.embeddings.word_embeddings.weight]      Loading weights:   3%|▎         | 5/199 [00:00<00:00, 11875.15it/s, Materializing param=bert.embeddings.word_embeddings.weight]Loading weights:   3%|▎         | 6/199 [00:00<00:00, 12601.81it/s, Materializing param=bert.encoder.layer.0.attention.output.LayerNorm.bias]Loading weights:   3%|▎         | 6/199 [00:00<00:00, 11966.63it/s, Materializing param=bert.encoder.layer.0.attention.output.LayerNorm.bias]Loading weights:   4%|▎         | 7/199 [00:00<00:00, 12628.01it/s, Materializing param=bert.encoder.layer.0.attention.output.LayerNorm.weight]Loading weights:   4%|▎         | 7/199 [00:00<00:00, 12117.26it/s, Materializing param=bert.encoder.layer.0.attention.output.LayerNorm.weight]Loading weights:   4%|▍         | 8/199 [00:00<00:00, 12861.03it/s, Materializing param=bert.encoder.layer.0.attention.output.dense.bias]      Loading weights:   4%|▍         | 8/199 [00:00<00:00, 12436.78it/s, Materializing param=bert.encoder.layer.0.attention.output.dense.bias]Loading weights:   5%|▍         | 9/199 [00:00<00:00, 13194.25it/s, Materializing param=bert.encoder.layer.0.attention.output.dense.weight]Loading weights:   5%|▍         | 9/199 [00:00<00:00, 12800.52it/s, Materializing param=bert.encoder.layer.0.attention.output.dense.weight]Loading weights:   5%|▌         | 10/199 [00:00<00:00, 13521.29it/s, Materializing param=bert.encoder.layer.0.attention.self.key.bias]     Loading weights:   5%|▌         | 10/199 [00:00<00:00, 13156.54it/s, Materializing param=bert.encoder.layer.0.attention.self.key.bias]Loading weights:   6%|▌         | 11/199 [00:00<00:00, 13801.18it/s, Materializing param=bert.encoder.layer.0.attention.self.key.weight]Loading weights:   6%|▌         | 11/199 [00:00<00:00, 13451.12it/s, Materializing param=bert.encoder.layer.0.attention.self.key.weight]Loading weights:   6%|▌         | 12/199 [00:00<00:00, 14055.19it/s, Materializing param=bert.encoder.layer.0.attention.self.query.bias]Loading weights:   6%|▌         | 12/199 [00:00<00:00, 13744.31it/s, Materializing param=bert.encoder.layer.0.attention.self.query.bias]Loading weights:   7%|▋         | 13/199 [00:00<00:00, 14303.76it/s, Materializing param=bert.encoder.layer.0.attention.self.query.weight]Loading weights:   7%|▋         | 13/199 [00:00<00:00, 13991.78it/s, Materializing param=bert.encoder.layer.0.attention.self.query.weight]Loading weights:   7%|▋         | 14/199 [00:00<00:00, 14495.25it/s, Materializing param=bert.encoder.layer.0.attention.self.value.bias]  Loading weights:   7%|▋         | 14/199 [00:00<00:00, 14214.54it/s, Materializing param=bert.encoder.layer.0.attention.self.value.bias]Loading weights:   8%|▊         | 15/199 [00:00<00:00, 14713.41it/s, Materializing param=bert.encoder.layer.0.attention.self.value.weight]Loading weights:   8%|▊         | 15/199 [00:00<00:00, 14436.57it/s, Materializing param=bert.encoder.layer.0.attention.self.value.weight]Loading weights:   8%|▊         | 16/199 [00:00<00:00, 14919.71it/s, Materializing param=bert.encoder.layer.0.intermediate.dense.bias]    Loading weights:   8%|▊         | 16/199 [00:00<00:00, 14636.61it/s, Materializing param=bert.encoder.layer.0.intermediate.dense.bias]Loading weights:   9%|▊         | 17/199 [00:00<00:00, 15077.85it/s, Materializing param=bert.encoder.layer.0.intermediate.dense.weight]Loading weights:   9%|▊         | 17/199 [00:00<00:00, 14817.78it/s, Materializing param=bert.encoder.layer.0.intermediate.dense.weight]Loading weights:   9%|▉         | 18/199 [00:00<00:00, 15242.78it/s, Materializing param=bert.encoder.layer.0.output.LayerNorm.bias]    Loading weights:   9%|▉         | 18/199 [00:00<00:00, 15000.49it/s, Materializing param=bert.encoder.layer.0.output.LayerNorm.bias]Loading weights:  10%|▉         | 19/199 [00:00<00:00, 15387.48it/s, Materializing param=bert.encoder.layer.0.output.LayerNorm.weight]Loading weights:  10%|▉         | 19/199 [00:00<00:00, 15159.17it/s, Materializing param=bert.encoder.layer.0.output.LayerNorm.weight]Loading weights:  10%|█         | 20/199 [00:00<00:00, 15534.46it/s, Materializing param=bert.encoder.layer.0.output.dense.bias]      Loading weights:  10%|█         | 20/199 [00:00<00:00, 15307.68it/s, Materializing param=bert.encoder.layer.0.output.dense.bias]Loading weights:  11%|█         | 21/199 [00:00<00:00, 15681.04it/s, Materializing param=bert.encoder.layer.0.output.dense.weight]Loading weights:  11%|█         | 21/199 [00:00<00:00, 15463.55it/s, Materializing param=bert.encoder.layer.0.output.dense.weight]Loading weights:  11%|█         | 22/199 [00:00<00:00, 15827.56it/s, Materializing param=bert.encoder.layer.1.attention.output.LayerNorm.bias]Loading weights:  11%|█         | 22/199 [00:00<00:00, 15610.67it/s, Materializing param=bert.encoder.layer.1.attention.output.LayerNorm.bias]Loading weights:  12%|█▏        | 23/199 [00:00<00:00, 15716.68it/s, Materializing param=bert.encoder.layer.1.attention.output.LayerNorm.weight]Loading weights:  12%|█▏        | 23/199 [00:00<00:00, 15482.10it/s, Materializing param=bert.encoder.layer.1.attention.output.LayerNorm.weight]Loading weights:  12%|█▏        | 24/199 [00:00<00:00, 15738.48it/s, Materializing param=bert.encoder.layer.1.attention.output.dense.bias]      Loading weights:  12%|█▏        | 24/199 [00:00<00:00, 15522.48it/s, Materializing param=bert.encoder.layer.1.attention.output.dense.bias]Loading weights:  13%|█▎        | 25/199 [00:00<00:00, 15806.09it/s, Materializing param=bert.encoder.layer.1.attention.output.dense.weight]Loading weights:  13%|█▎        | 25/199 [00:00<00:00, 15615.43it/s, Materializing param=bert.encoder.layer.1.attention.output.dense.weight]Loading weights:  13%|█▎        | 26/199 [00:00<00:00, 15896.78it/s, Materializing param=bert.encoder.layer.1.attention.self.key.bias]      Loading weights:  13%|█▎        | 26/199 [00:00<00:00, 15718.06it/s, Materializing param=bert.encoder.layer.1.attention.self.key.bias]Loading weights:  14%|█▎        | 27/199 [00:00<00:00, 15992.97it/s, Materializing param=bert.encoder.layer.1.attention.self.key.weight]Loading weights:  14%|█▎        | 27/199 [00:00<00:00, 15818.72it/s, Materializing param=bert.encoder.layer.1.attention.self.key.weight]Loading weights:  14%|█▍        | 28/199 [00:00<00:00, 16067.93it/s, Materializing param=bert.encoder.layer.1.attention.self.query.bias]Loading weights:  14%|█▍        | 28/199 [00:00<00:00, 15893.97it/s, Materializing param=bert.encoder.layer.1.attention.self.query.bias]Loading weights:  15%|█▍        | 29/199 [00:00<00:00, 16151.22it/s, Materializing param=bert.encoder.layer.1.attention.self.query.weight]Loading weights:  15%|█▍        | 29/199 [00:00<00:00, 15975.15it/s, Materializing param=bert.encoder.layer.1.attention.self.query.weight]Loading weights:  15%|█▌        | 30/199 [00:00<00:00, 16213.00it/s, Materializing param=bert.encoder.layer.1.attention.self.value.bias]  Loading weights:  15%|█▌        | 30/199 [00:00<00:00, 16049.63it/s, Materializing param=bert.encoder.layer.1.attention.self.value.bias]Loading weights:  16%|█▌        | 31/199 [00:00<00:00, 16289.58it/s, Materializing param=bert.encoder.layer.1.attention.self.value.weight]Loading weights:  16%|█▌        | 31/199 [00:00<00:00, 16129.94it/s, Materializing param=bert.encoder.layer.1.attention.self.value.weight]Loading weights:  16%|█▌        | 32/199 [00:00<00:00, 16362.03it/s, Materializing param=bert.encoder.layer.1.intermediate.dense.bias]    Loading weights:  16%|█▌        | 32/199 [00:00<00:00, 16209.87it/s, Materializing param=bert.encoder.layer.1.intermediate.dense.bias]Loading weights:  17%|█▋        | 33/199 [00:00<00:00, 16428.73it/s, Materializing param=bert.encoder.layer.1.intermediate.dense.weight]Loading weights:  17%|█▋        | 33/199 [00:00<00:00, 16272.28it/s, Materializing param=bert.encoder.layer.1.intermediate.dense.weight]Loading weights:  17%|█▋        | 34/199 [00:00<00:00, 16484.38it/s, Materializing param=bert.encoder.layer.1.output.LayerNorm.bias]    Loading weights:  17%|█▋        | 34/199 [00:00<00:00, 16223.70it/s, Materializing param=bert.encoder.layer.1.output.LayerNorm.bias]Loading weights:  18%|█▊        | 35/199 [00:00<00:00, 16387.66it/s, Materializing param=bert.encoder.layer.1.output.LayerNorm.weight]Loading weights:  18%|█▊        | 35/199 [00:00<00:00, 16222.86it/s, Materializing param=bert.encoder.layer.1.output.LayerNorm.weight]Loading weights:  18%|█▊        | 36/199 [00:00<00:00, 16378.67it/s, Materializing param=bert.encoder.layer.1.output.dense.bias]      Loading weights:  18%|█▊        | 36/199 [00:00<00:00, 16180.34it/s, Materializing param=bert.encoder.layer.1.output.dense.bias]Loading weights:  19%|█▊        | 37/199 [00:00<00:00, 16352.92it/s, Materializing param=bert.encoder.layer.1.output.dense.weight]Loading weights:  19%|█▊        | 37/199 [00:00<00:00, 16200.99it/s, Materializing param=bert.encoder.layer.1.output.dense.weight]Loading weights:  19%|█▉        | 38/199 [00:00<00:00, 16385.68it/s, Materializing param=bert.encoder.layer.2.attention.output.LayerNorm.bias]Loading weights:  19%|█▉        | 38/199 [00:00<00:00, 16235.46it/s, Materializing param=bert.encoder.layer.2.attention.output.LayerNorm.bias]Loading weights:  20%|█▉        | 39/199 [00:00<00:00, 16365.97it/s, Materializing param=bert.encoder.layer.2.attention.output.LayerNorm.weight]Loading weights:  20%|█▉        | 39/199 [00:00<00:00, 16211.88it/s, Materializing param=bert.encoder.layer.2.attention.output.LayerNorm.weight]Loading weights:  20%|██        | 40/199 [00:00<00:00, 16198.91it/s, Materializing param=bert.encoder.layer.2.attention.output.dense.bias]      Loading weights:  20%|██        | 40/199 [00:00<00:00, 15992.01it/s, Materializing param=bert.encoder.layer.2.attention.output.dense.bias]Loading weights:  21%|██        | 41/199 [00:00<00:00, 16141.02it/s, Materializing param=bert.encoder.layer.2.attention.output.dense.weight]Loading weights:  21%|██        | 41/199 [00:00<00:00, 16020.73it/s, Materializing param=bert.encoder.layer.2.attention.output.dense.weight]Loading weights:  21%|██        | 42/199 [00:00<00:00, 16200.18it/s, Materializing param=bert.encoder.layer.2.attention.self.key.bias]      Loading weights:  21%|██        | 42/199 [00:00<00:00, 16080.40it/s, Materializing param=bert.encoder.layer.2.attention.self.key.bias]Loading weights:  22%|██▏       | 43/199 [00:00<00:00, 16214.61it/s, Materializing param=bert.encoder.layer.2.attention.self.key.weight]Loading weights:  22%|██▏       | 43/199 [00:00<00:00, 16100.26it/s, Materializing param=bert.encoder.layer.2.attention.self.key.weight]Loading weights:  22%|██▏       | 44/199 [00:00<00:00, 16265.59it/s, Materializing param=bert.encoder.layer.2.attention.self.query.bias]Loading weights:  22%|██▏       | 44/199 [00:00<00:00, 16158.78it/s, Materializing param=bert.encoder.layer.2.attention.self.query.bias]Loading weights:  23%|██▎       | 45/199 [00:00<00:00, 16332.96it/s, Materializing param=bert.encoder.layer.2.attention.self.query.weight]Loading weights:  23%|██▎       | 45/199 [00:00<00:00, 16224.85it/s, Materializing param=bert.encoder.layer.2.attention.self.query.weight]Loading weights:  23%|██▎       | 46/199 [00:00<00:00, 16393.74it/s, Materializing param=bert.encoder.layer.2.attention.self.value.bias]  Loading weights:  23%|██▎       | 46/199 [00:00<00:00, 16283.06it/s, Materializing param=bert.encoder.layer.2.attention.self.value.bias]Loading weights:  24%|██▎       | 47/199 [00:00<00:00, 16438.65it/s, Materializing param=bert.encoder.layer.2.attention.self.value.weight]Loading weights:  24%|██▎       | 47/199 [00:00<00:00, 16335.12it/s, Materializing param=bert.encoder.layer.2.attention.self.value.weight]Loading weights:  24%|██▍       | 48/199 [00:00<00:00, 16488.66it/s, Materializing param=bert.encoder.layer.2.intermediate.dense.bias]    Loading weights:  24%|██▍       | 48/199 [00:00<00:00, 16386.67it/s, Materializing param=bert.encoder.layer.2.intermediate.dense.bias]Loading weights:  25%|██▍       | 49/199 [00:00<00:00, 16536.92it/s, Materializing param=bert.encoder.layer.2.intermediate.dense.weight]Loading weights:  25%|██▍       | 49/199 [00:00<00:00, 16436.41it/s, Materializing param=bert.encoder.layer.2.intermediate.dense.weight]Loading weights:  25%|██▌       | 50/199 [00:00<00:00, 16590.08it/s, Materializing param=bert.encoder.layer.2.output.LayerNorm.bias]    Loading weights:  25%|██▌       | 50/199 [00:00<00:00, 16489.64it/s, Materializing param=bert.encoder.layer.2.output.LayerNorm.bias]Loading weights:  26%|██▌       | 51/199 [00:00<00:00, 16625.95it/s, Materializing param=bert.encoder.layer.2.output.LayerNorm.weight]Loading weights:  26%|██▌       | 51/199 [00:00<00:00, 16525.77it/s, Materializing param=bert.encoder.layer.2.output.LayerNorm.weight]Loading weights:  26%|██▌       | 52/199 [00:00<00:00, 16675.88it/s, Materializing param=bert.encoder.layer.2.output.dense.bias]      Loading weights:  26%|██▌       | 52/199 [00:00<00:00, 16579.54it/s, Materializing param=bert.encoder.layer.2.output.dense.bias]Loading weights:  27%|██▋       | 53/199 [00:00<00:00, 16609.24it/s, Materializing param=bert.encoder.layer.2.output.dense.weight]Loading weights:  27%|██▋       | 53/199 [00:00<00:00, 16495.85it/s, Materializing param=bert.encoder.layer.2.output.dense.weight]Loading weights:  27%|██▋       | 54/199 [00:00<00:00, 16608.67it/s, Materializing param=bert.encoder.layer.3.attention.output.LayerNorm.bias]Loading weights:  27%|██▋       | 54/199 [00:00<00:00, 16498.57it/s, Materializing param=bert.encoder.layer.3.attention.output.LayerNorm.bias]Loading weights:  28%|██▊       | 55/199 [00:00<00:00, 16597.36it/s, Materializing param=bert.encoder.layer.3.attention.output.LayerNorm.weight]Loading weights:  28%|██▊       | 55/199 [00:00<00:00, 16491.76it/s, Materializing param=bert.encoder.layer.3.attention.output.LayerNorm.weight]Loading weights:  28%|██▊       | 56/199 [00:00<00:00, 16602.89it/s, Materializing param=bert.encoder.layer.3.attention.output.dense.bias]      Loading weights:  28%|██▊       | 56/199 [00:00<00:00, 16497.93it/s, Materializing param=bert.encoder.layer.3.attention.output.dense.bias]Loading weights:  29%|██▊       | 57/199 [00:00<00:00, 16548.44it/s, Materializing param=bert.encoder.layer.3.attention.output.dense.weight]Loading weights:  29%|██▊       | 57/199 [00:00<00:00, 16436.94it/s, Materializing param=bert.encoder.layer.3.attention.output.dense.weight]Loading weights:  29%|██▉       | 58/199 [00:00<00:00, 16559.09it/s, Materializing param=bert.encoder.layer.3.attention.self.key.bias]      Loading weights:  29%|██▉       | 58/199 [00:00<00:00, 16468.29it/s, Materializing param=bert.encoder.layer.3.attention.self.key.bias]Loading weights:  30%|██▉       | 59/199 [00:00<00:00, 16590.50it/s, Materializing param=bert.encoder.layer.3.attention.self.key.weight]Loading weights:  30%|██▉       | 59/199 [00:00<00:00, 16502.00it/s, Materializing param=bert.encoder.layer.3.attention.self.key.weight]Loading weights:  30%|███       | 60/199 [00:00<00:00, 16628.67it/s, Materializing param=bert.encoder.layer.3.attention.self.query.bias]Loading weights:  30%|███       | 60/199 [00:00<00:00, 16535.79it/s, Materializing param=bert.encoder.layer.3.attention.self.query.bias]Loading weights:  31%|███       | 61/199 [00:00<00:00, 16640.82it/s, Materializing param=bert.encoder.layer.3.attention.self.query.weight]Loading weights:  31%|███       | 61/199 [00:00<00:00, 16548.25it/s, Materializing param=bert.encoder.layer.3.attention.self.query.weight]Loading weights:  31%|███       | 62/199 [00:00<00:00, 16654.72it/s, Materializing param=bert.encoder.layer.3.attention.self.value.bias]  Loading weights:  31%|███       | 62/199 [00:00<00:00, 16566.66it/s, Materializing param=bert.encoder.layer.3.attention.self.value.bias]Loading weights:  32%|███▏      | 63/199 [00:00<00:00, 16673.47it/s, Materializing param=bert.encoder.layer.3.attention.self.value.weight]Loading weights:  32%|███▏      | 63/199 [00:00<00:00, 16590.77it/s, Materializing param=bert.encoder.layer.3.attention.self.value.weight]Loading weights:  32%|███▏      | 64/199 [00:00<00:00, 16701.02it/s, Materializing param=bert.encoder.layer.3.intermediate.dense.bias]    Loading weights:  32%|███▏      | 64/199 [00:00<00:00, 16613.16it/s, Materializing param=bert.encoder.layer.3.intermediate.dense.bias]Loading weights:  33%|███▎      | 65/199 [00:00<00:00, 16711.40it/s, Materializing param=bert.encoder.layer.3.intermediate.dense.weight]Loading weights:  33%|███▎      | 65/199 [00:00<00:00, 16627.82it/s, Materializing param=bert.encoder.layer.3.intermediate.dense.weight]Loading weights:  33%|███▎      | 66/199 [00:00<00:00, 16731.58it/s, Materializing param=bert.encoder.layer.3.output.LayerNorm.bias]    Loading weights:  33%|███▎      | 66/199 [00:00<00:00, 16649.07it/s, Materializing param=bert.encoder.layer.3.output.LayerNorm.bias]Loading weights:  34%|███▎      | 67/199 [00:00<00:00, 16757.21it/s, Materializing param=bert.encoder.layer.3.output.LayerNorm.weight]Loading weights:  34%|███▎      | 67/199 [00:00<00:00, 16674.68it/s, Materializing param=bert.encoder.layer.3.output.LayerNorm.weight]Loading weights:  34%|███▍      | 68/199 [00:00<00:00, 16781.16it/s, Materializing param=bert.encoder.layer.3.output.dense.bias]      Loading weights:  34%|███▍      | 68/199 [00:00<00:00, 16701.57it/s, Materializing param=bert.encoder.layer.3.output.dense.bias]Loading weights:  35%|███▍      | 69/199 [00:00<00:00, 16807.42it/s, Materializing param=bert.encoder.layer.3.output.dense.weight]Loading weights:  35%|███▍      | 69/199 [00:00<00:00, 16732.60it/s, Materializing param=bert.encoder.layer.3.output.dense.weight]Loading weights:  35%|███▌      | 70/199 [00:00<00:00, 16840.73it/s, Materializing param=bert.encoder.layer.4.attention.output.LayerNorm.bias]Loading weights:  35%|███▌      | 70/199 [00:00<00:00, 16767.63it/s, Materializing param=bert.encoder.layer.4.attention.output.LayerNorm.bias]Loading weights:  36%|███▌      | 71/199 [00:00<00:00, 16864.63it/s, Materializing param=bert.encoder.layer.4.attention.output.LayerNorm.weight]Loading weights:  36%|███▌      | 71/199 [00:00<00:00, 16788.57it/s, Materializing param=bert.encoder.layer.4.attention.output.LayerNorm.weight]Loading weights:  36%|███▌      | 72/199 [00:00<00:00, 16878.49it/s, Materializing param=bert.encoder.layer.4.attention.output.dense.bias]      Loading weights:  36%|███▌      | 72/199 [00:00<00:00, 16804.29it/s, Materializing param=bert.encoder.layer.4.attention.output.dense.bias]Loading weights:  37%|███▋      | 73/199 [00:00<00:00, 16895.72it/s, Materializing param=bert.encoder.layer.4.attention.output.dense.weight]Loading weights:  37%|███▋      | 73/199 [00:00<00:00, 16819.61it/s, Materializing param=bert.encoder.layer.4.attention.output.dense.weight]Loading weights:  37%|███▋      | 74/199 [00:00<00:00, 16914.36it/s, Materializing param=bert.encoder.layer.4.attention.self.key.bias]      Loading weights:  37%|███▋      | 74/199 [00:00<00:00, 16842.77it/s, Materializing param=bert.encoder.layer.4.attention.self.key.bias]Loading weights:  38%|███▊      | 75/199 [00:00<00:00, 16913.43it/s, Materializing param=bert.encoder.layer.4.attention.self.key.weight]Loading weights:  38%|███▊      | 75/199 [00:00<00:00, 16792.44it/s, Materializing param=bert.encoder.layer.4.attention.self.key.weight]Loading weights:  38%|███▊      | 76/199 [00:00<00:00, 16862.42it/s, Materializing param=bert.encoder.layer.4.attention.self.query.bias]Loading weights:  38%|███▊      | 76/199 [00:00<00:00, 16778.10it/s, Materializing param=bert.encoder.layer.4.attention.self.query.bias]Loading weights:  39%|███▊      | 77/199 [00:00<00:00, 16841.08it/s, Materializing param=bert.encoder.layer.4.attention.self.query.weight]Loading weights:  39%|███▊      | 77/199 [00:00<00:00, 16771.12it/s, Materializing param=bert.encoder.layer.4.attention.self.query.weight]Loading weights:  39%|███▉      | 78/199 [00:00<00:00, 16862.83it/s, Materializing param=bert.encoder.layer.4.attention.self.value.bias]  Loading weights:  39%|███▉      | 78/199 [00:00<00:00, 16796.17it/s, Materializing param=bert.encoder.layer.4.attention.self.value.bias]Loading weights:  40%|███▉      | 79/199 [00:00<00:00, 16838.60it/s, Materializing param=bert.encoder.layer.4.attention.self.value.weight]Loading weights:  40%|███▉      | 79/199 [00:00<00:00, 16761.09it/s, Materializing param=bert.encoder.layer.4.attention.self.value.weight]Loading weights:  40%|████      | 80/199 [00:00<00:00, 16827.70it/s, Materializing param=bert.encoder.layer.4.intermediate.dense.bias]    Loading weights:  40%|████      | 80/199 [00:00<00:00, 16757.94it/s, Materializing param=bert.encoder.layer.4.intermediate.dense.bias]Loading weights:  41%|████      | 81/199 [00:00<00:00, 16839.58it/s, Materializing param=bert.encoder.layer.4.intermediate.dense.weight]Loading weights:  41%|████      | 81/199 [00:00<00:00, 16768.94it/s, Materializing param=bert.encoder.layer.4.intermediate.dense.weight]Loading weights:  41%|████      | 82/199 [00:00<00:00, 16850.37it/s, Materializing param=bert.encoder.layer.4.output.LayerNorm.bias]    Loading weights:  41%|████      | 82/199 [00:00<00:00, 16781.31it/s, Materializing param=bert.encoder.layer.4.output.LayerNorm.bias]Loading weights:  42%|████▏     | 83/199 [00:00<00:00, 16865.00it/s, Materializing param=bert.encoder.layer.4.output.LayerNorm.weight]Loading weights:  42%|████▏     | 83/199 [00:00<00:00, 16795.02it/s, Materializing param=bert.encoder.layer.4.output.LayerNorm.weight]Loading weights:  42%|████▏     | 84/199 [00:00<00:00, 16876.06it/s, Materializing param=bert.encoder.layer.4.output.dense.bias]      Loading weights:  42%|████▏     | 84/199 [00:00<00:00, 16808.43it/s, Materializing param=bert.encoder.layer.4.output.dense.bias]Loading weights:  43%|████▎     | 85/199 [00:00<00:00, 16887.68it/s, Materializing param=bert.encoder.layer.4.output.dense.weight]Loading weights:  43%|████▎     | 85/199 [00:00<00:00, 16814.41it/s, Materializing param=bert.encoder.layer.4.output.dense.weight]Loading weights:  43%|████▎     | 86/199 [00:00<00:00, 16875.33it/s, Materializing param=bert.encoder.layer.5.attention.output.LayerNorm.bias]Loading weights:  43%|████▎     | 86/199 [00:00<00:00, 16777.22it/s, Materializing param=bert.encoder.layer.5.attention.output.LayerNorm.bias]Loading weights:  44%|████▎     | 87/199 [00:00<00:00, 16810.45it/s, Materializing param=bert.encoder.layer.5.attention.output.LayerNorm.weight]Loading weights:  44%|████▎     | 87/199 [00:00<00:00, 16730.29it/s, Materializing param=bert.encoder.layer.5.attention.output.LayerNorm.weight]Loading weights:  44%|████▍     | 88/199 [00:00<00:00, 16768.07it/s, Materializing param=bert.encoder.layer.5.attention.output.dense.bias]      Loading weights:  44%|████▍     | 88/199 [00:00<00:00, 16670.37it/s, Materializing param=bert.encoder.layer.5.attention.output.dense.bias]Loading weights:  45%|████▍     | 89/199 [00:00<00:00, 16721.60it/s, Materializing param=bert.encoder.layer.5.attention.output.dense.weight]Loading weights:  45%|████▍     | 89/199 [00:00<00:00, 16597.44it/s, Materializing param=bert.encoder.layer.5.attention.output.dense.weight]Loading weights:  45%|████▌     | 90/199 [00:00<00:00, 16652.14it/s, Materializing param=bert.encoder.layer.5.attention.self.key.bias]      Loading weights:  45%|████▌     | 90/199 [00:00<00:00, 16567.36it/s, Materializing param=bert.encoder.layer.5.attention.self.key.bias]Loading weights:  46%|████▌     | 91/199 [00:00<00:00, 16570.36it/s, Materializing param=bert.encoder.layer.5.attention.self.key.weight]Loading weights:  46%|████▌     | 91/199 [00:00<00:00, 16504.44it/s, Materializing param=bert.encoder.layer.5.attention.self.key.weight]Loading weights:  46%|████▌     | 92/199 [00:00<00:00, 16564.76it/s, Materializing param=bert.encoder.layer.5.attention.self.query.bias]Loading weights:  46%|████▌     | 92/199 [00:00<00:00, 16496.77it/s, Materializing param=bert.encoder.layer.5.attention.self.query.bias]Loading weights:  47%|████▋     | 93/199 [00:00<00:00, 16554.36it/s, Materializing param=bert.encoder.layer.5.attention.self.query.weight]Loading weights:  47%|████▋     | 93/199 [00:00<00:00, 16492.76it/s, Materializing param=bert.encoder.layer.5.attention.self.query.weight]Loading weights:  47%|████▋     | 94/199 [00:00<00:00, 16553.91it/s, Materializing param=bert.encoder.layer.5.attention.self.value.bias]  Loading weights:  47%|████▋     | 94/199 [00:00<00:00, 16488.84it/s, Materializing param=bert.encoder.layer.5.attention.self.value.bias]Loading weights:  48%|████▊     | 95/199 [00:00<00:00, 16550.04it/s, Materializing param=bert.encoder.layer.5.attention.self.value.weight]Loading weights:  48%|████▊     | 95/199 [00:00<00:00, 16487.73it/s, Materializing param=bert.encoder.layer.5.attention.self.value.weight]Loading weights:  48%|████▊     | 96/199 [00:00<00:00, 16551.02it/s, Materializing param=bert.encoder.layer.5.intermediate.dense.bias]    Loading weights:  48%|████▊     | 96/199 [00:00<00:00, 16490.01it/s, Materializing param=bert.encoder.layer.5.intermediate.dense.bias]Loading weights:  49%|████▊     | 97/199 [00:00<00:00, 16552.65it/s, Materializing param=bert.encoder.layer.5.intermediate.dense.weight]Loading weights:  49%|████▊     | 97/199 [00:00<00:00, 16490.92it/s, Materializing param=bert.encoder.layer.5.intermediate.dense.weight]Loading weights:  49%|████▉     | 98/199 [00:00<00:00, 16552.91it/s, Materializing param=bert.encoder.layer.5.output.LayerNorm.bias]    Loading weights:  49%|████▉     | 98/199 [00:00<00:00, 16493.13it/s, Materializing param=bert.encoder.layer.5.output.LayerNorm.bias]Loading weights:  50%|████▉     | 99/199 [00:00<00:00, 16547.89it/s, Materializing param=bert.encoder.layer.5.output.LayerNorm.weight]Loading weights:  50%|████▉     | 99/199 [00:00<00:00, 16489.40it/s, Materializing param=bert.encoder.layer.5.output.LayerNorm.weight]Loading weights:  50%|█████     | 100/199 [00:00<00:00, 16549.49it/s, Materializing param=bert.encoder.layer.5.output.dense.bias]     Loading weights:  50%|█████     | 100/199 [00:00<00:00, 16489.64it/s, Materializing param=bert.encoder.layer.5.output.dense.bias]Loading weights:  51%|█████     | 101/199 [00:00<00:00, 16545.90it/s, Materializing param=bert.encoder.layer.5.output.dense.weight]Loading weights:  51%|█████     | 101/199 [00:00<00:00, 16486.02it/s, Materializing param=bert.encoder.layer.5.output.dense.weight]Loading weights:  51%|█████▏    | 102/199 [00:00<00:00, 16546.22it/s, Materializing param=bert.encoder.layer.6.attention.output.LayerNorm.bias]Loading weights:  51%|█████▏    | 102/199 [00:00<00:00, 16486.28it/s, Materializing param=bert.encoder.layer.6.attention.output.LayerNorm.bias]Loading weights:  52%|█████▏    | 103/199 [00:00<00:00, 16528.80it/s, Materializing param=bert.encoder.layer.6.attention.output.LayerNorm.weight]Loading weights:  52%|█████▏    | 103/199 [00:00<00:00, 16468.94it/s, Materializing param=bert.encoder.layer.6.attention.output.LayerNorm.weight]Loading weights:  52%|█████▏    | 104/199 [00:00<00:00, 16523.64it/s, Materializing param=bert.encoder.layer.6.attention.output.dense.bias]      Loading weights:  52%|█████▏    | 104/199 [00:00<00:00, 16466.88it/s, Materializing param=bert.encoder.layer.6.attention.output.dense.bias]Loading weights:  53%|█████▎    | 105/199 [00:00<00:00, 16519.82it/s, Materializing param=bert.encoder.layer.6.attention.output.dense.weight]Loading weights:  53%|█████▎    | 105/199 [00:00<00:00, 16458.09it/s, Materializing param=bert.encoder.layer.6.attention.output.dense.weight]Loading weights:  53%|█████▎    | 106/199 [00:00<00:00, 16498.91it/s, Materializing param=bert.encoder.layer.6.attention.self.key.bias]      Loading weights:  53%|█████▎    | 106/199 [00:00<00:00, 16437.30it/s, Materializing param=bert.encoder.layer.6.attention.self.key.bias]Loading weights:  54%|█████▍    | 107/199 [00:00<00:00, 16487.53it/s, Materializing param=bert.encoder.layer.6.attention.self.key.weight]Loading weights:  54%|█████▍    | 107/199 [00:00<00:00, 16431.39it/s, Materializing param=bert.encoder.layer.6.attention.self.key.weight]Loading weights:  54%|█████▍    | 108/199 [00:00<00:00, 16482.96it/s, Materializing param=bert.encoder.layer.6.attention.self.query.bias]Loading weights:  54%|█████▍    | 108/199 [00:00<00:00, 16427.97it/s, Materializing param=bert.encoder.layer.6.attention.self.query.bias]Loading weights:  55%|█████▍    | 109/199 [00:00<00:00, 16476.71it/s, Materializing param=bert.encoder.layer.6.attention.self.query.weight]Loading weights:  55%|█████▍    | 109/199 [00:00<00:00, 16419.31it/s, Materializing param=bert.encoder.layer.6.attention.self.query.weight]Loading weights:  55%|█████▌    | 110/199 [00:00<00:00, 16472.33it/s, Materializing param=bert.encoder.layer.6.attention.self.value.bias]  Loading weights:  55%|█████▌    | 110/199 [00:00<00:00, 16417.23it/s, Materializing param=bert.encoder.layer.6.attention.self.value.bias]Loading weights:  56%|█████▌    | 111/199 [00:00<00:00, 16468.61it/s, Materializing param=bert.encoder.layer.6.attention.self.value.weight]Loading weights:  56%|█████▌    | 111/199 [00:00<00:00, 16416.93it/s, Materializing param=bert.encoder.layer.6.attention.self.value.weight]Loading weights:  56%|█████▋    | 112/199 [00:00<00:00, 16469.01it/s, Materializing param=bert.encoder.layer.6.intermediate.dense.bias]    Loading weights:  56%|█████▋    | 112/199 [00:00<00:00, 16416.06it/s, Materializing param=bert.encoder.layer.6.intermediate.dense.bias]Loading weights:  57%|█████▋    | 113/199 [00:00<00:00, 16466.54it/s, Materializing param=bert.encoder.layer.6.intermediate.dense.weight]Loading weights:  57%|█████▋    | 113/199 [00:00<00:00, 16414.64it/s, Materializing param=bert.encoder.layer.6.intermediate.dense.weight]Loading weights:  57%|█████▋    | 114/199 [00:00<00:00, 16469.78it/s, Materializing param=bert.encoder.layer.6.output.LayerNorm.bias]    Loading weights:  57%|█████▋    | 114/199 [00:00<00:00, 16416.63it/s, Materializing param=bert.encoder.layer.6.output.LayerNorm.bias]Loading weights:  58%|█████▊    | 115/199 [00:00<00:00, 16469.59it/s, Materializing param=bert.encoder.layer.6.output.LayerNorm.weight]Loading weights:  58%|█████▊    | 115/199 [00:00<00:00, 16417.46it/s, Materializing param=bert.encoder.layer.6.output.LayerNorm.weight]Loading weights:  58%|█████▊    | 116/199 [00:00<00:00, 16464.95it/s, Materializing param=bert.encoder.layer.6.output.dense.bias]      Loading weights:  58%|█████▊    | 116/199 [00:00<00:00, 16412.19it/s, Materializing param=bert.encoder.layer.6.output.dense.bias]Loading weights:  59%|█████▉    | 117/199 [00:00<00:00, 16456.52it/s, Materializing param=bert.encoder.layer.6.output.dense.weight]Loading weights:  59%|█████▉    | 117/199 [00:00<00:00, 16404.81it/s, Materializing param=bert.encoder.layer.6.output.dense.weight]Loading weights:  59%|█████▉    | 118/199 [00:00<00:00, 16455.91it/s, Materializing param=bert.encoder.layer.7.attention.output.LayerNorm.bias]Loading weights:  59%|█████▉    | 118/199 [00:00<00:00, 16405.72it/s, Materializing param=bert.encoder.layer.7.attention.output.LayerNorm.bias]Loading weights:  60%|█████▉    | 119/199 [00:00<00:00, 16449.88it/s, Materializing param=bert.encoder.layer.7.attention.output.LayerNorm.weight]Loading weights:  60%|█████▉    | 119/199 [00:00<00:00, 16401.23it/s, Materializing param=bert.encoder.layer.7.attention.output.LayerNorm.weight]Loading weights:  60%|██████    | 120/199 [00:00<00:00, 16449.33it/s, Materializing param=bert.encoder.layer.7.attention.output.dense.bias]      Loading weights:  60%|██████    | 120/199 [00:00<00:00, 16400.02it/s, Materializing param=bert.encoder.layer.7.attention.output.dense.bias]Loading weights:  61%|██████    | 121/199 [00:00<00:00, 16447.18it/s, Materializing param=bert.encoder.layer.7.attention.output.dense.weight]Loading weights:  61%|██████    | 121/199 [00:00<00:00, 16396.17it/s, Materializing param=bert.encoder.layer.7.attention.output.dense.weight]Loading weights:  61%|██████▏   | 122/199 [00:00<00:00, 16431.35it/s, Materializing param=bert.encoder.layer.7.attention.self.key.bias]      Loading weights:  61%|██████▏   | 122/199 [00:00<00:00, 16380.85it/s, Materializing param=bert.encoder.layer.7.attention.self.key.bias]Loading weights:  62%|██████▏   | 123/199 [00:00<00:00, 16425.73it/s, Materializing param=bert.encoder.layer.7.attention.self.key.weight]Loading weights:  62%|██████▏   | 123/199 [00:00<00:00, 16377.76it/s, Materializing param=bert.encoder.layer.7.attention.self.key.weight]Loading weights:  62%|██████▏   | 124/199 [00:00<00:00, 16424.88it/s, Materializing param=bert.encoder.layer.7.attention.self.query.bias]Loading weights:  62%|██████▏   | 124/199 [00:00<00:00, 16373.17it/s, Materializing param=bert.encoder.layer.7.attention.self.query.bias]Loading weights:  63%|██████▎   | 125/199 [00:00<00:00, 16418.89it/s, Materializing param=bert.encoder.layer.7.attention.self.query.weight]Loading weights:  63%|██████▎   | 125/199 [00:00<00:00, 16369.68it/s, Materializing param=bert.encoder.layer.7.attention.self.query.weight]Loading weights:  63%|██████▎   | 126/199 [00:00<00:00, 16415.55it/s, Materializing param=bert.encoder.layer.7.attention.self.value.bias]  Loading weights:  63%|██████▎   | 126/199 [00:00<00:00, 16365.23it/s, Materializing param=bert.encoder.layer.7.attention.self.value.bias]Loading weights:  64%|██████▍   | 127/199 [00:00<00:00, 16411.26it/s, Materializing param=bert.encoder.layer.7.attention.self.value.weight]Loading weights:  64%|██████▍   | 127/199 [00:00<00:00, 16364.37it/s, Materializing param=bert.encoder.layer.7.attention.self.value.weight]Loading weights:  64%|██████▍   | 128/199 [00:00<00:00, 16411.04it/s, Materializing param=bert.encoder.layer.7.intermediate.dense.bias]    Loading weights:  64%|██████▍   | 128/199 [00:00<00:00, 16364.02it/s, Materializing param=bert.encoder.layer.7.intermediate.dense.bias]Loading weights:  65%|██████▍   | 129/199 [00:00<00:00, 16411.33it/s, Materializing param=bert.encoder.layer.7.intermediate.dense.weight]Loading weights:  65%|██████▍   | 129/199 [00:00<00:00, 16367.15it/s, Materializing param=bert.encoder.layer.7.intermediate.dense.weight]Loading weights:  65%|██████▌   | 130/199 [00:00<00:00, 16417.05it/s, Materializing param=bert.encoder.layer.7.output.LayerNorm.bias]    Loading weights:  65%|██████▌   | 130/199 [00:00<00:00, 16372.19it/s, Materializing param=bert.encoder.layer.7.output.LayerNorm.bias]Loading weights:  66%|██████▌   | 131/199 [00:00<00:00, 16416.31it/s, Materializing param=bert.encoder.layer.7.output.LayerNorm.weight]Loading weights:  66%|██████▌   | 131/199 [00:00<00:00, 16373.75it/s, Materializing param=bert.encoder.layer.7.output.LayerNorm.weight]Loading weights:  66%|██████▋   | 132/199 [00:00<00:00, 16416.55it/s, Materializing param=bert.encoder.layer.7.output.dense.bias]      Loading weights:  66%|██████▋   | 132/199 [00:00<00:00, 16369.47it/s, Materializing param=bert.encoder.layer.7.output.dense.bias]Loading weights:  67%|██████▋   | 133/199 [00:00<00:00, 16411.96it/s, Materializing param=bert.encoder.layer.7.output.dense.weight]Loading weights:  67%|██████▋   | 133/199 [00:00<00:00, 16368.14it/s, Materializing param=bert.encoder.layer.7.output.dense.weight]Loading weights:  67%|██████▋   | 134/199 [00:00<00:00, 16411.75it/s, Materializing param=bert.encoder.layer.8.attention.output.LayerNorm.bias]Loading weights:  67%|██████▋   | 134/199 [00:00<00:00, 16365.39it/s, Materializing param=bert.encoder.layer.8.attention.output.LayerNorm.bias]Loading weights:  68%|██████▊   | 135/199 [00:00<00:00, 16401.56it/s, Materializing param=bert.encoder.layer.8.attention.output.LayerNorm.weight]Loading weights:  68%|██████▊   | 135/199 [00:00<00:00, 16357.02it/s, Materializing param=bert.encoder.layer.8.attention.output.LayerNorm.weight]Loading weights:  68%|██████▊   | 136/199 [00:00<00:00, 16398.60it/s, Materializing param=bert.encoder.layer.8.attention.output.dense.bias]      Loading weights:  68%|██████▊   | 136/199 [00:00<00:00, 16353.94it/s, Materializing param=bert.encoder.layer.8.attention.output.dense.bias]Loading weights:  69%|██████▉   | 137/199 [00:00<00:00, 16395.22it/s, Materializing param=bert.encoder.layer.8.attention.output.dense.weight]Loading weights:  69%|██████▉   | 137/199 [00:00<00:00, 16351.36it/s, Materializing param=bert.encoder.layer.8.attention.output.dense.weight]Loading weights:  69%|██████▉   | 138/199 [00:00<00:00, 16386.32it/s, Materializing param=bert.encoder.layer.8.attention.self.key.bias]      Loading weights:  69%|██████▉   | 138/199 [00:00<00:00, 16340.98it/s, Materializing param=bert.encoder.layer.8.attention.self.key.bias]Loading weights:  70%|██████▉   | 139/199 [00:00<00:00, 16381.70it/s, Materializing param=bert.encoder.layer.8.attention.self.key.weight]Loading weights:  70%|██████▉   | 139/199 [00:00<00:00, 16339.00it/s, Materializing param=bert.encoder.layer.8.attention.self.key.weight]Loading weights:  70%|███████   | 140/199 [00:00<00:00, 16378.52it/s, Materializing param=bert.encoder.layer.8.attention.self.query.bias]Loading weights:  70%|███████   | 140/199 [00:00<00:00, 16337.50it/s, Materializing param=bert.encoder.layer.8.attention.self.query.bias]Loading weights:  71%|███████   | 141/199 [00:00<00:00, 16379.01it/s, Materializing param=bert.encoder.layer.8.attention.self.query.weight]Loading weights:  71%|███████   | 141/199 [00:00<00:00, 16335.58it/s, Materializing param=bert.encoder.layer.8.attention.self.query.weight]Loading weights:  71%|███████▏  | 142/199 [00:00<00:00, 16376.34it/s, Materializing param=bert.encoder.layer.8.attention.self.value.bias]  Loading weights:  71%|███████▏  | 142/199 [00:00<00:00, 16334.57it/s, Materializing param=bert.encoder.layer.8.attention.self.value.bias]Loading weights:  72%|███████▏  | 143/199 [00:00<00:00, 16374.16it/s, Materializing param=bert.encoder.layer.8.attention.self.value.weight]Loading weights:  72%|███████▏  | 143/199 [00:00<00:00, 16333.14it/s, Materializing param=bert.encoder.layer.8.attention.self.value.weight]Loading weights:  72%|███████▏  | 144/199 [00:00<00:00, 16371.57it/s, Materializing param=bert.encoder.layer.8.intermediate.dense.bias]    Loading weights:  72%|███████▏  | 144/199 [00:00<00:00, 16330.40it/s, Materializing param=bert.encoder.layer.8.intermediate.dense.bias]Loading weights:  73%|███████▎  | 145/199 [00:00<00:00, 16369.01it/s, Materializing param=bert.encoder.layer.8.intermediate.dense.weight]Loading weights:  73%|███████▎  | 145/199 [00:00<00:00, 16329.45it/s, Materializing param=bert.encoder.layer.8.intermediate.dense.weight]Loading weights:  73%|███████▎  | 146/199 [00:00<00:00, 16371.30it/s, Materializing param=bert.encoder.layer.8.output.LayerNorm.bias]    Loading weights:  73%|███████▎  | 146/199 [00:00<00:00, 16329.39it/s, Materializing param=bert.encoder.layer.8.output.LayerNorm.bias]Loading weights:  74%|███████▍  | 147/199 [00:00<00:00, 16364.00it/s, Materializing param=bert.encoder.layer.8.output.LayerNorm.weight]Loading weights:  74%|███████▍  | 147/199 [00:00<00:00, 16324.14it/s, Materializing param=bert.encoder.layer.8.output.LayerNorm.weight]Loading weights:  74%|███████▍  | 148/199 [00:00<00:00, 16363.70it/s, Materializing param=bert.encoder.layer.8.output.dense.bias]      Loading weights:  74%|███████▍  | 148/199 [00:00<00:00, 16324.97it/s, Materializing param=bert.encoder.layer.8.output.dense.bias]Loading weights:  75%|███████▍  | 149/199 [00:00<00:00, 16363.84it/s, Materializing param=bert.encoder.layer.8.output.dense.weight]Loading weights:  75%|███████▍  | 149/199 [00:00<00:00, 16326.22it/s, Materializing param=bert.encoder.layer.8.output.dense.weight]Loading weights:  75%|███████▌  | 150/199 [00:00<00:00, 16366.95it/s, Materializing param=bert.encoder.layer.9.attention.output.LayerNorm.bias]Loading weights:  75%|███████▌  | 150/199 [00:00<00:00, 16326.60it/s, Materializing param=bert.encoder.layer.9.attention.output.LayerNorm.bias]Loading weights:  76%|███████▌  | 151/199 [00:00<00:00, 16360.72it/s, Materializing param=bert.encoder.layer.9.attention.output.LayerNorm.weight]Loading weights:  76%|███████▌  | 151/199 [00:00<00:00, 16321.51it/s, Materializing param=bert.encoder.layer.9.attention.output.LayerNorm.weight]Loading weights:  76%|███████▋  | 152/199 [00:00<00:00, 16359.20it/s, Materializing param=bert.encoder.layer.9.attention.output.dense.bias]      Loading weights:  76%|███████▋  | 152/199 [00:00<00:00, 16320.67it/s, Materializing param=bert.encoder.layer.9.attention.output.dense.bias]Loading weights:  77%|███████▋  | 153/199 [00:00<00:00, 16358.11it/s, Materializing param=bert.encoder.layer.9.attention.output.dense.weight]Loading weights:  77%|███████▋  | 153/199 [00:00<00:00, 16320.66it/s, Materializing param=bert.encoder.layer.9.attention.output.dense.weight]Loading weights:  77%|███████▋  | 154/199 [00:00<00:00, 16347.10it/s, Materializing param=bert.encoder.layer.9.attention.self.key.bias]      Loading weights:  77%|███████▋  | 154/199 [00:00<00:00, 16309.95it/s, Materializing param=bert.encoder.layer.9.attention.self.key.bias]Loading weights:  78%|███████▊  | 155/199 [00:00<00:00, 16348.57it/s, Materializing param=bert.encoder.layer.9.attention.self.key.weight]Loading weights:  78%|███████▊  | 155/199 [00:00<00:00, 16312.06it/s, Materializing param=bert.encoder.layer.9.attention.self.key.weight]Loading weights:  78%|███████▊  | 156/199 [00:00<00:00, 16349.20it/s, Materializing param=bert.encoder.layer.9.attention.self.query.bias]Loading weights:  78%|███████▊  | 156/199 [00:00<00:00, 16313.33it/s, Materializing param=bert.encoder.layer.9.attention.self.query.bias]Loading weights:  79%|███████▉  | 157/199 [00:00<00:00, 16351.05it/s, Materializing param=bert.encoder.layer.9.attention.self.query.weight]Loading weights:  79%|███████▉  | 157/199 [00:00<00:00, 16315.40it/s, Materializing param=bert.encoder.layer.9.attention.self.query.weight]Loading weights:  79%|███████▉  | 158/199 [00:00<00:00, 16352.47it/s, Materializing param=bert.encoder.layer.9.attention.self.value.bias]  Loading weights:  79%|███████▉  | 158/199 [00:00<00:00, 16317.03it/s, Materializing param=bert.encoder.layer.9.attention.self.value.bias]Loading weights:  80%|███████▉  | 159/199 [00:00<00:00, 16352.66it/s, Materializing param=bert.encoder.layer.9.attention.self.value.weight]Loading weights:  80%|███████▉  | 159/199 [00:00<00:00, 16314.66it/s, Materializing param=bert.encoder.layer.9.attention.self.value.weight]Loading weights:  80%|████████  | 160/199 [00:00<00:00, 16350.07it/s, Materializing param=bert.encoder.layer.9.intermediate.dense.bias]    Loading weights:  80%|████████  | 160/199 [00:00<00:00, 16313.90it/s, Materializing param=bert.encoder.layer.9.intermediate.dense.bias]Loading weights:  81%|████████  | 161/199 [00:00<00:00, 16349.09it/s, Materializing param=bert.encoder.layer.9.intermediate.dense.weight]Loading weights:  81%|████████  | 161/199 [00:00<00:00, 16312.36it/s, Materializing param=bert.encoder.layer.9.intermediate.dense.weight]Loading weights:  81%|████████▏ | 162/199 [00:00<00:00, 16349.31it/s, Materializing param=bert.encoder.layer.9.output.LayerNorm.bias]    Loading weights:  81%|████████▏ | 162/199 [00:00<00:00, 16313.98it/s, Materializing param=bert.encoder.layer.9.output.LayerNorm.bias]Loading weights:  82%|████████▏ | 163/199 [00:00<00:00, 16350.30it/s, Materializing param=bert.encoder.layer.9.output.LayerNorm.weight]Loading weights:  82%|████████▏ | 163/199 [00:00<00:00, 16314.41it/s, Materializing param=bert.encoder.layer.9.output.LayerNorm.weight]Loading weights:  82%|████████▏ | 164/199 [00:00<00:00, 16353.23it/s, Materializing param=bert.encoder.layer.9.output.dense.bias]      Loading weights:  82%|████████▏ | 164/199 [00:00<00:00, 16319.09it/s, Materializing param=bert.encoder.layer.9.output.dense.bias]Loading weights:  83%|████████▎ | 165/199 [00:00<00:00, 16359.60it/s, Materializing param=bert.encoder.layer.9.output.dense.weight]Loading weights:  83%|████████▎ | 165/199 [00:00<00:00, 16323.71it/s, Materializing param=bert.encoder.layer.9.output.dense.weight]Loading weights:  83%|████████▎ | 166/199 [00:00<00:00, 16360.52it/s, Materializing param=bert.encoder.layer.10.attention.output.LayerNorm.bias]Loading weights:  83%|████████▎ | 166/199 [00:00<00:00, 16322.54it/s, Materializing param=bert.encoder.layer.10.attention.output.LayerNorm.bias]Loading weights:  84%|████████▍ | 167/199 [00:00<00:00, 16351.11it/s, Materializing param=bert.encoder.layer.10.attention.output.LayerNorm.weight]Loading weights:  84%|████████▍ | 167/199 [00:00<00:00, 16315.69it/s, Materializing param=bert.encoder.layer.10.attention.output.LayerNorm.weight]Loading weights:  84%|████████▍ | 168/199 [00:00<00:00, 16349.41it/s, Materializing param=bert.encoder.layer.10.attention.output.dense.bias]      Loading weights:  84%|████████▍ | 168/199 [00:00<00:00, 16314.20it/s, Materializing param=bert.encoder.layer.10.attention.output.dense.bias]Loading weights:  85%|████████▍ | 169/199 [00:00<00:00, 16348.48it/s, Materializing param=bert.encoder.layer.10.attention.output.dense.weight]Loading weights:  85%|████████▍ | 169/199 [00:00<00:00, 16313.86it/s, Materializing param=bert.encoder.layer.10.attention.output.dense.weight]Loading weights:  85%|████████▌ | 170/199 [00:00<00:00, 16348.69it/s, Materializing param=bert.encoder.layer.10.attention.self.key.bias]      Loading weights:  85%|████████▌ | 170/199 [00:00<00:00, 16309.80it/s, Materializing param=bert.encoder.layer.10.attention.self.key.bias]Loading weights:  86%|████████▌ | 171/199 [00:00<00:00, 16343.68it/s, Materializing param=bert.encoder.layer.10.attention.self.key.weight]Loading weights:  86%|████████▌ | 171/199 [00:00<00:00, 16310.60it/s, Materializing param=bert.encoder.layer.10.attention.self.key.weight]Loading weights:  86%|████████▋ | 172/199 [00:00<00:00, 16345.02it/s, Materializing param=bert.encoder.layer.10.attention.self.query.bias]Loading weights:  86%|████████▋ | 172/199 [00:00<00:00, 16312.13it/s, Materializing param=bert.encoder.layer.10.attention.self.query.bias]Loading weights:  87%|████████▋ | 173/199 [00:00<00:00, 16345.25it/s, Materializing param=bert.encoder.layer.10.attention.self.query.weight]Loading weights:  87%|████████▋ | 173/199 [00:00<00:00, 16313.28it/s, Materializing param=bert.encoder.layer.10.attention.self.query.weight]Loading weights:  87%|████████▋ | 174/199 [00:00<00:00, 16348.40it/s, Materializing param=bert.encoder.layer.10.attention.self.value.bias]  Loading weights:  87%|████████▋ | 174/199 [00:00<00:00, 16313.68it/s, Materializing param=bert.encoder.layer.10.attention.self.value.bias]Loading weights:  88%|████████▊ | 175/199 [00:00<00:00, 16343.14it/s, Materializing param=bert.encoder.layer.10.attention.self.value.weight]Loading weights:  88%|████████▊ | 175/199 [00:00<00:00, 16309.01it/s, Materializing param=bert.encoder.layer.10.attention.self.value.weight]Loading weights:  88%|████████▊ | 176/199 [00:00<00:00, 16339.76it/s, Materializing param=bert.encoder.layer.10.intermediate.dense.bias]    Loading weights:  88%|████████▊ | 176/199 [00:00<00:00, 16306.55it/s, Materializing param=bert.encoder.layer.10.intermediate.dense.bias]Loading weights:  89%|████████▉ | 177/199 [00:00<00:00, 16339.65it/s, Materializing param=bert.encoder.layer.10.intermediate.dense.weight]Loading weights:  89%|████████▉ | 177/199 [00:00<00:00, 16306.63it/s, Materializing param=bert.encoder.layer.10.intermediate.dense.weight]Loading weights:  89%|████████▉ | 178/199 [00:00<00:00, 16339.89it/s, Materializing param=bert.encoder.layer.10.output.LayerNorm.bias]    Loading weights:  89%|████████▉ | 178/199 [00:00<00:00, 16306.70it/s, Materializing param=bert.encoder.layer.10.output.LayerNorm.bias]Loading weights:  90%|████████▉ | 179/199 [00:00<00:00, 16336.94it/s, Materializing param=bert.encoder.layer.10.output.LayerNorm.weight]Loading weights:  90%|████████▉ | 179/199 [00:00<00:00, 16301.47it/s, Materializing param=bert.encoder.layer.10.output.LayerNorm.weight]Loading weights:  90%|█████████ | 180/199 [00:00<00:00, 16329.78it/s, Materializing param=bert.encoder.layer.10.output.dense.bias]      Loading weights:  90%|█████████ | 180/199 [00:00<00:00, 16297.35it/s, Materializing param=bert.encoder.layer.10.output.dense.bias]Loading weights:  91%|█████████ | 181/199 [00:00<00:00, 16329.02it/s, Materializing param=bert.encoder.layer.10.output.dense.weight]Loading weights:  91%|█████████ | 181/199 [00:00<00:00, 16296.08it/s, Materializing param=bert.encoder.layer.10.output.dense.weight]Loading weights:  91%|█████████▏| 182/199 [00:00<00:00, 16329.33it/s, Materializing param=bert.encoder.layer.11.attention.output.LayerNorm.bias]Loading weights:  91%|█████████▏| 182/199 [00:00<00:00, 16296.56it/s, Materializing param=bert.encoder.layer.11.attention.output.LayerNorm.bias]Loading weights:  92%|█████████▏| 183/199 [00:00<00:00, 16323.72it/s, Materializing param=bert.encoder.layer.11.attention.output.LayerNorm.weight]Loading weights:  92%|█████████▏| 183/199 [00:00<00:00, 16289.08it/s, Materializing param=bert.encoder.layer.11.attention.output.LayerNorm.weight]Loading weights:  92%|█████████▏| 184/199 [00:00<00:00, 16319.21it/s, Materializing param=bert.encoder.layer.11.attention.output.dense.bias]      Loading weights:  92%|█████████▏| 184/199 [00:00<00:00, 16286.50it/s, Materializing param=bert.encoder.layer.11.attention.output.dense.bias]Loading weights:  93%|█████████▎| 185/199 [00:00<00:00, 16315.79it/s, Materializing param=bert.encoder.layer.11.attention.output.dense.weight]Loading weights:  93%|█████████▎| 185/199 [00:00<00:00, 16284.63it/s, Materializing param=bert.encoder.layer.11.attention.output.dense.weight]Loading weights:  93%|█████████▎| 186/199 [00:00<00:00, 16315.81it/s, Materializing param=bert.encoder.layer.11.attention.self.key.bias]      Loading weights:  93%|█████████▎| 186/199 [00:00<00:00, 16280.74it/s, Materializing param=bert.encoder.layer.11.attention.self.key.bias]Loading weights:  94%|█████████▍| 187/199 [00:00<00:00, 16310.41it/s, Materializing param=bert.encoder.layer.11.attention.self.key.weight]Loading weights:  94%|█████████▍| 187/199 [00:00<00:00, 16279.26it/s, Materializing param=bert.encoder.layer.11.attention.self.key.weight]Loading weights:  94%|█████████▍| 188/199 [00:00<00:00, 16311.47it/s, Materializing param=bert.encoder.layer.11.attention.self.query.bias]Loading weights:  94%|█████████▍| 188/199 [00:00<00:00, 16281.16it/s, Materializing param=bert.encoder.layer.11.attention.self.query.bias]Loading weights:  95%|█████████▍| 189/199 [00:00<00:00, 16313.20it/s, Materializing param=bert.encoder.layer.11.attention.self.query.weight]Loading weights:  95%|█████████▍| 189/199 [00:00<00:00, 16282.70it/s, Materializing param=bert.encoder.layer.11.attention.self.query.weight]Loading weights:  95%|█████████▌| 190/199 [00:00<00:00, 16314.57it/s, Materializing param=bert.encoder.layer.11.attention.self.value.bias]  Loading weights:  95%|█████████▌| 190/199 [00:00<00:00, 16285.56it/s, Materializing param=bert.encoder.layer.11.attention.self.value.bias]Loading weights:  96%|█████████▌| 191/199 [00:00<00:00, 16317.92it/s, Materializing param=bert.encoder.layer.11.attention.self.value.weight]Loading weights:  96%|█████████▌| 191/199 [00:00<00:00, 16289.39it/s, Materializing param=bert.encoder.layer.11.attention.self.value.weight]Loading weights:  96%|█████████▋| 192/199 [00:00<00:00, 16321.24it/s, Materializing param=bert.encoder.layer.11.intermediate.dense.bias]    Loading weights:  96%|█████████▋| 192/199 [00:00<00:00, 16292.18it/s, Materializing param=bert.encoder.layer.11.intermediate.dense.bias]Loading weights:  97%|█████████▋| 193/199 [00:00<00:00, 16324.86it/s, Materializing param=bert.encoder.layer.11.intermediate.dense.weight]Loading weights:  97%|█████████▋| 193/199 [00:00<00:00, 16295.61it/s, Materializing param=bert.encoder.layer.11.intermediate.dense.weight]Loading weights:  97%|█████████▋| 194/199 [00:00<00:00, 16328.44it/s, Materializing param=bert.encoder.layer.11.output.LayerNorm.bias]    Loading weights:  97%|█████████▋| 194/199 [00:00<00:00, 16299.33it/s, Materializing param=bert.encoder.layer.11.output.LayerNorm.bias]Loading weights:  98%|█████████▊| 195/199 [00:00<00:00, 16329.37it/s, Materializing param=bert.encoder.layer.11.output.LayerNorm.weight]Loading weights:  98%|█████████▊| 195/199 [00:00<00:00, 16299.43it/s, Materializing param=bert.encoder.layer.11.output.LayerNorm.weight]Loading weights:  98%|█████████▊| 196/199 [00:00<00:00, 16330.62it/s, Materializing param=bert.encoder.layer.11.output.dense.bias]      Loading weights:  98%|█████████▊| 196/199 [00:00<00:00, 16301.16it/s, Materializing param=bert.encoder.layer.11.output.dense.bias]Loading weights:  99%|█████████▉| 197/199 [00:00<00:00, 16333.48it/s, Materializing param=bert.encoder.layer.11.output.dense.weight]Loading weights:  99%|█████████▉| 197/199 [00:00<00:00, 16306.40it/s, Materializing param=bert.encoder.layer.11.output.dense.weight]Loading weights:  99%|█████████▉| 198/199 [00:00<00:00, 16337.91it/s, Materializing param=bert.pooler.dense.bias]                   Loading weights:  99%|█████████▉| 198/199 [00:00<00:00, 16309.99it/s, Materializing param=bert.pooler.dense.bias]Loading weights: 100%|██████████| 199/199 [00:00<00:00, 16340.70it/s, Materializing param=bert.pooler.dense.weight]Loading weights: 100%|██████████| 199/199 [00:00<00:00, 16313.55it/s, Materializing param=bert.pooler.dense.weight]Loading weights: 100%|██████████| 199/199 [00:00<00:00, 16232.65it/s, Materializing param=bert.pooler.dense.weight]
BertForSequenceClassification LOAD REPORT from: google/muril-base-cased
Key                                        | Status     | 
-------------------------------------------+------------+-
cls.predictions.transform.dense.bias       | UNEXPECTED | 
cls.seq_relationship.weight                | UNEXPECTED | 
cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | 
cls.predictions.transform.dense.weight     | UNEXPECTED | 
cls.predictions.transform.LayerNorm.weight | UNEXPECTED | 
bert.embeddings.position_ids               | UNEXPECTED | 
cls.predictions.decoder.weight             | UNEXPECTED | 
cls.seq_relationship.bias                  | UNEXPECTED | 
cls.predictions.decoder.bias               | UNEXPECTED | 
cls.predictions.bias                       | UNEXPECTED | 
classifier.weight                          | MISSING    | 
classifier.bias                            | MISSING    | 

Notes:
- UNEXPECTED	:can be ignored when loading from different task/architecture; not ok if you expect identical arch.
- MISSING	:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.
2026-01-27 18:36:07,936 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/google/muril-base-cased/commits/main "HTTP/1.1 200 OK"
2026-01-27 18:36:08,291 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/google/muril-base-cased/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-01-27 18:36:08,293 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/google/muril-base-cased/discussions?p=0 "HTTP/1.1 200 OK"
2026-01-27 18:36:08,536 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/google/muril-base-cased/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-01-27 18:36:08,661 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/google/muril-base-cased/commits/refs%2Fpr%2F3 "HTTP/1.1 200 OK"
2026-01-27 18:36:08,914 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/google/muril-base-cased/resolve/refs%2Fpr%2F3/model.safetensors.index.json "HTTP/1.1 404 Not Found"
2026-01-27 18:36:08,917 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/google/muril-base-cased "HTTP/1.1 200 OK"
2026-01-27 18:36:08,918 - sentiment_analysis.model_loader - INFO - Loaded model google/muril-base-cased in 2182.53ms on cpu
2026-01-27 18:36:08,918 - sentiment_analysis.model_loader - INFO - [2/4] Loading FacebookAI/xlm-roberta-base...
2026-01-27 18:36:09,164 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/google/muril-base-cased/resolve/refs%2Fpr%2F3/model.safetensors "HTTP/1.1 302 Found"
2026-01-27 18:36:09,203 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/FacebookAI/xlm-roberta-base/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-01-27 18:36:09,220 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/FacebookAI/xlm-roberta-base/e73636d4f797dec63c3081bb6ed5c7b0bb3f2089/config.json "HTTP/1.1 200 OK"
Loading weights:   0%|          | 0/197 [00:00<?, ?it/s]Loading weights:   1%|          | 1/197 [00:00<00:00, 77672.30it/s, Materializing param=roberta.embeddings.LayerNorm.bias]Loading weights:   1%|          | 1/197 [00:00<00:00, 12945.38it/s, Materializing param=roberta.embeddings.LayerNorm.bias]Loading weights:   1%|          | 2/197 [00:00<00:00, 2112.47it/s, Materializing param=roberta.embeddings.LayerNorm.weight]Loading weights:   1%|          | 2/197 [00:00<00:00, 2041.02it/s, Materializing param=roberta.embeddings.LayerNorm.weight]Loading weights:   2%|▏         | 3/197 [00:00<00:00, 2033.11it/s, Materializing param=roberta.embeddings.position_embeddings.weight]Loading weights:   2%|▏         | 3/197 [00:00<00:00, 1987.51it/s, Materializing param=roberta.embeddings.position_embeddings.weight]Loading weights:   2%|▏         | 4/197 [00:00<00:00, 2546.63it/s, Materializing param=roberta.embeddings.token_type_embeddings.weight]Loading weights:   2%|▏         | 4/197 [00:00<00:00, 2499.96it/s, Materializing param=roberta.embeddings.token_type_embeddings.weight]Loading weights:   3%|▎         | 5/197 [00:00<00:00, 3014.88it/s, Materializing param=roberta.embeddings.word_embeddings.weight]      Loading weights:   3%|▎         | 5/197 [00:00<00:00, 2967.11it/s, Materializing param=roberta.embeddings.word_embeddings.weight]Loading weights:   3%|▎         | 6/197 [00:00<00:00, 3460.17it/s, Materializing param=roberta.encoder.layer.0.attention.output.LayerNorm.bias]Loading weights:   3%|▎         | 6/197 [00:00<00:00, 3404.47it/s, Materializing param=roberta.encoder.layer.0.attention.output.LayerNorm.bias]Loading weights:   4%|▎         | 7/197 [00:00<00:00, 3823.93it/s, Materializing param=roberta.encoder.layer.0.attention.output.LayerNorm.weight]Loading weights:   4%|▎         | 7/197 [00:00<00:00, 3776.22it/s, Materializing param=roberta.encoder.layer.0.attention.output.LayerNorm.weight]Loading weights:   4%|▍         | 8/197 [00:00<00:00, 3998.86it/s, Materializing param=roberta.encoder.layer.0.attention.output.dense.bias]      Loading weights:   4%|▍         | 8/197 [00:00<00:00, 3948.97it/s, Materializing param=roberta.encoder.layer.0.attention.output.dense.bias]Loading weights:   5%|▍         | 9/197 [00:00<00:00, 4308.72it/s, Materializing param=roberta.encoder.layer.0.attention.output.dense.weight]Loading weights:   5%|▍         | 9/197 [00:00<00:00, 4256.74it/s, Materializing param=roberta.encoder.layer.0.attention.output.dense.weight]Loading weights:   5%|▌         | 10/197 [00:00<00:00, 4597.00it/s, Materializing param=roberta.encoder.layer.0.attention.self.key.bias]     Loading weights:   5%|▌         | 10/197 [00:00<00:00, 4545.19it/s, Materializing param=roberta.encoder.layer.0.attention.self.key.bias]Loading weights:   6%|▌         | 11/197 [00:00<00:00, 4331.74it/s, Materializing param=roberta.encoder.layer.0.attention.self.key.weight]Loading weights:   6%|▌         | 11/197 [00:00<00:00, 4273.95it/s, Materializing param=roberta.encoder.layer.0.attention.self.key.weight]Loading weights:   6%|▌         | 12/197 [00:00<00:00, 4538.88it/s, Materializing param=roberta.encoder.layer.0.attention.self.query.bias]Loading weights:   6%|▌         | 12/197 [00:00<00:00, 4497.51it/s, Materializing param=roberta.encoder.layer.0.attention.self.query.bias]Loading weights:   7%|▋         | 13/197 [00:00<00:00, 4750.89it/s, Materializing param=roberta.encoder.layer.0.attention.self.query.weight]Loading weights:   7%|▋         | 13/197 [00:00<00:00, 4707.82it/s, Materializing param=roberta.encoder.layer.0.attention.self.query.weight]Loading weights:   7%|▋         | 14/197 [00:00<00:00, 4967.45it/s, Materializing param=roberta.encoder.layer.0.attention.self.value.bias]  Loading weights:   7%|▋         | 14/197 [00:00<00:00, 4922.07it/s, Materializing param=roberta.encoder.layer.0.attention.self.value.bias]Loading weights:   8%|▊         | 15/197 [00:00<00:00, 5166.25it/s, Materializing param=roberta.encoder.layer.0.attention.self.value.weight]Loading weights:   8%|▊         | 15/197 [00:00<00:00, 5125.84it/s, Materializing param=roberta.encoder.layer.0.attention.self.value.weight]Loading weights:   8%|▊         | 16/197 [00:00<00:00, 5361.42it/s, Materializing param=roberta.encoder.layer.0.intermediate.dense.bias]    Loading weights:   8%|▊         | 16/197 [00:00<00:00, 5315.97it/s, Materializing param=roberta.encoder.layer.0.intermediate.dense.bias]Loading weights:   9%|▊         | 17/197 [00:00<00:00, 5264.95it/s, Materializing param=roberta.encoder.layer.0.intermediate.dense.weight]Loading weights:   9%|▊         | 17/197 [00:00<00:00, 5224.06it/s, Materializing param=roberta.encoder.layer.0.intermediate.dense.weight]Loading weights:   9%|▉         | 18/197 [00:00<00:00, 5421.33it/s, Materializing param=roberta.encoder.layer.0.output.LayerNorm.bias]    Loading weights:   9%|▉         | 18/197 [00:00<00:00, 5365.85it/s, Materializing param=roberta.encoder.layer.0.output.LayerNorm.bias]Loading weights:  10%|▉         | 19/197 [00:00<00:00, 5283.20it/s, Materializing param=roberta.encoder.layer.0.output.LayerNorm.weight]Loading weights:  10%|▉         | 19/197 [00:00<00:00, 5228.43it/s, Materializing param=roberta.encoder.layer.0.output.LayerNorm.weight]Loading weights:  10%|█         | 20/197 [00:00<00:00, 5423.20it/s, Materializing param=roberta.encoder.layer.0.output.dense.bias]      Loading weights:  10%|█         | 20/197 [00:00<00:00, 5385.60it/s, Materializing param=roberta.encoder.layer.0.output.dense.bias]Loading weights:  11%|█         | 21/197 [00:00<00:00, 5336.59it/s, Materializing param=roberta.encoder.layer.0.output.dense.weight]Loading weights:  11%|█         | 21/197 [00:00<00:00, 5301.26it/s, Materializing param=roberta.encoder.layer.0.output.dense.weight]Loading weights:  11%|█         | 22/197 [00:00<00:00, 5036.28it/s, Materializing param=roberta.encoder.layer.1.attention.output.LayerNorm.bias]Loading weights:  11%|█         | 22/197 [00:00<00:00, 4857.07it/s, Materializing param=roberta.encoder.layer.1.attention.output.LayerNorm.bias]Loading weights:  12%|█▏        | 23/197 [00:00<00:00, 4973.14it/s, Materializing param=roberta.encoder.layer.1.attention.output.LayerNorm.weight]Loading weights:  12%|█▏        | 23/197 [00:00<00:00, 4939.53it/s, Materializing param=roberta.encoder.layer.1.attention.output.LayerNorm.weight]Loading weights:  12%|█▏        | 24/197 [00:00<00:00, 5080.67it/s, Materializing param=roberta.encoder.layer.1.attention.output.dense.bias]      Loading weights:  12%|█▏        | 24/197 [00:00<00:00, 5052.36it/s, Materializing param=roberta.encoder.layer.1.attention.output.dense.bias]Loading weights:  13%|█▎        | 25/197 [00:00<00:00, 5185.58it/s, Materializing param=roberta.encoder.layer.1.attention.output.dense.weight]Loading weights:  13%|█▎        | 25/197 [00:00<00:00, 5155.75it/s, Materializing param=roberta.encoder.layer.1.attention.output.dense.weight]Loading weights:  13%|█▎        | 26/197 [00:00<00:00, 5291.21it/s, Materializing param=roberta.encoder.layer.1.attention.self.key.bias]      Loading weights:  13%|█▎        | 26/197 [00:00<00:00, 5261.60it/s, Materializing param=roberta.encoder.layer.1.attention.self.key.bias]Loading weights:  14%|█▎        | 27/197 [00:00<00:00, 5386.52it/s, Materializing param=roberta.encoder.layer.1.attention.self.key.weight]Loading weights:  14%|█▎        | 27/197 [00:00<00:00, 5343.57it/s, Materializing param=roberta.encoder.layer.1.attention.self.key.weight]Loading weights:  14%|█▍        | 28/197 [00:00<00:00, 5440.59it/s, Materializing param=roberta.encoder.layer.1.attention.self.query.bias]Loading weights:  14%|█▍        | 28/197 [00:00<00:00, 5410.26it/s, Materializing param=roberta.encoder.layer.1.attention.self.query.bias]Loading weights:  15%|█▍        | 29/197 [00:00<00:00, 5176.17it/s, Materializing param=roberta.encoder.layer.1.attention.self.query.weight]Loading weights:  15%|█▍        | 29/197 [00:00<00:00, 5147.47it/s, Materializing param=roberta.encoder.layer.1.attention.self.query.weight]Loading weights:  15%|█▌        | 30/197 [00:00<00:00, 5215.71it/s, Materializing param=roberta.encoder.layer.1.attention.self.value.bias]  Loading weights:  15%|█▌        | 30/197 [00:00<00:00, 5192.26it/s, Materializing param=roberta.encoder.layer.1.attention.self.value.bias]Loading weights:  16%|█▌        | 31/197 [00:00<00:00, 5284.00it/s, Materializing param=roberta.encoder.layer.1.attention.self.value.weight]Loading weights:  16%|█▌        | 31/197 [00:00<00:00, 5259.42it/s, Materializing param=roberta.encoder.layer.1.attention.self.value.weight]Loading weights:  16%|█▌        | 32/197 [00:00<00:00, 5373.22it/s, Materializing param=roberta.encoder.layer.1.intermediate.dense.bias]    Loading weights:  16%|█▌        | 32/197 [00:00<00:00, 5349.45it/s, Materializing param=roberta.encoder.layer.1.intermediate.dense.bias]Loading weights:  17%|█▋        | 33/197 [00:00<00:00, 5104.44it/s, Materializing param=roberta.encoder.layer.1.intermediate.dense.weight]Loading weights:  17%|█▋        | 33/197 [00:00<00:00, 5076.17it/s, Materializing param=roberta.encoder.layer.1.intermediate.dense.weight]Loading weights:  17%|█▋        | 34/197 [00:00<00:00, 5187.76it/s, Materializing param=roberta.encoder.layer.1.output.LayerNorm.bias]    Loading weights:  17%|█▋        | 34/197 [00:00<00:00, 5169.71it/s, Materializing param=roberta.encoder.layer.1.output.LayerNorm.bias]Loading weights:  18%|█▊        | 35/197 [00:00<00:00, 5278.70it/s, Materializing param=roberta.encoder.layer.1.output.LayerNorm.weight]Loading weights:  18%|█▊        | 35/197 [00:00<00:00, 5261.11it/s, Materializing param=roberta.encoder.layer.1.output.LayerNorm.weight]Loading weights:  18%|█▊        | 36/197 [00:00<00:00, 5370.81it/s, Materializing param=roberta.encoder.layer.1.output.dense.bias]      Loading weights:  18%|█▊        | 36/197 [00:00<00:00, 5352.72it/s, Materializing param=roberta.encoder.layer.1.output.dense.bias]Loading weights:  19%|█▉        | 37/197 [00:00<00:00, 5461.72it/s, Materializing param=roberta.encoder.layer.1.output.dense.weight]Loading weights:  19%|█▉        | 37/197 [00:00<00:00, 5441.80it/s, Materializing param=roberta.encoder.layer.1.output.dense.weight]Loading weights:  19%|█▉        | 38/197 [00:00<00:00, 5545.13it/s, Materializing param=roberta.encoder.layer.2.attention.output.LayerNorm.bias]Loading weights:  19%|█▉        | 38/197 [00:00<00:00, 5523.99it/s, Materializing param=roberta.encoder.layer.2.attention.output.LayerNorm.bias]Loading weights:  20%|█▉        | 39/197 [00:00<00:00, 5620.07it/s, Materializing param=roberta.encoder.layer.2.attention.output.LayerNorm.weight]Loading weights:  20%|█▉        | 39/197 [00:00<00:00, 5600.26it/s, Materializing param=roberta.encoder.layer.2.attention.output.LayerNorm.weight]Loading weights:  20%|██        | 40/197 [00:00<00:00, 5699.17it/s, Materializing param=roberta.encoder.layer.2.attention.output.dense.bias]      Loading weights:  20%|██        | 40/197 [00:00<00:00, 5679.49it/s, Materializing param=roberta.encoder.layer.2.attention.output.dense.bias]Loading weights:  21%|██        | 41/197 [00:00<00:00, 5589.32it/s, Materializing param=roberta.encoder.layer.2.attention.output.dense.weight]Loading weights:  21%|██        | 41/197 [00:00<00:00, 5569.22it/s, Materializing param=roberta.encoder.layer.2.attention.output.dense.weight]Loading weights:  21%|██▏       | 42/197 [00:00<00:00, 5419.66it/s, Materializing param=roberta.encoder.layer.2.attention.self.key.bias]      Loading weights:  21%|██▏       | 42/197 [00:00<00:00, 5395.43it/s, Materializing param=roberta.encoder.layer.2.attention.self.key.bias]Loading weights:  22%|██▏       | 43/197 [00:00<00:00, 5480.92it/s, Materializing param=roberta.encoder.layer.2.attention.self.key.weight]Loading weights:  22%|██▏       | 43/197 [00:00<00:00, 5462.33it/s, Materializing param=roberta.encoder.layer.2.attention.self.key.weight]Loading weights:  22%|██▏       | 44/197 [00:00<00:00, 5547.69it/s, Materializing param=roberta.encoder.layer.2.attention.self.query.bias]Loading weights:  22%|██▏       | 44/197 [00:00<00:00, 5530.90it/s, Materializing param=roberta.encoder.layer.2.attention.self.query.bias]Loading weights:  23%|██▎       | 45/197 [00:00<00:00, 5617.04it/s, Materializing param=roberta.encoder.layer.2.attention.self.query.weight]Loading weights:  23%|██▎       | 45/197 [00:00<00:00, 5600.54it/s, Materializing param=roberta.encoder.layer.2.attention.self.query.weight]Loading weights:  23%|██▎       | 46/197 [00:00<00:00, 5682.00it/s, Materializing param=roberta.encoder.layer.2.attention.self.value.bias]  Loading weights:  23%|██▎       | 46/197 [00:00<00:00, 5664.48it/s, Materializing param=roberta.encoder.layer.2.attention.self.value.bias]Loading weights:  24%|██▍       | 47/197 [00:00<00:00, 5751.32it/s, Materializing param=roberta.encoder.layer.2.attention.self.value.weight]Loading weights:  24%|██▍       | 47/197 [00:00<00:00, 5732.92it/s, Materializing param=roberta.encoder.layer.2.attention.self.value.weight]Loading weights:  24%|██▍       | 48/197 [00:00<00:00, 5816.84it/s, Materializing param=roberta.encoder.layer.2.intermediate.dense.bias]    Loading weights:  24%|██▍       | 48/197 [00:00<00:00, 5795.91it/s, Materializing param=roberta.encoder.layer.2.intermediate.dense.bias]Loading weights:  25%|██▍       | 49/197 [00:00<00:00, 5875.72it/s, Materializing param=roberta.encoder.layer.2.intermediate.dense.weight]Loading weights:  25%|██▍       | 49/197 [00:00<00:00, 5859.30it/s, Materializing param=roberta.encoder.layer.2.intermediate.dense.weight]Loading weights:  25%|██▌       | 50/197 [00:00<00:00, 5784.93it/s, Materializing param=roberta.encoder.layer.2.output.LayerNorm.bias]    Loading weights:  25%|██▌       | 50/197 [00:00<00:00, 5766.32it/s, Materializing param=roberta.encoder.layer.2.output.LayerNorm.bias]Loading weights:  26%|██▌       | 51/197 [00:00<00:00, 5845.16it/s, Materializing param=roberta.encoder.layer.2.output.LayerNorm.weight]Loading weights:  26%|██▌       | 51/197 [00:00<00:00, 5829.07it/s, Materializing param=roberta.encoder.layer.2.output.LayerNorm.weight]Loading weights:  26%|██▋       | 52/197 [00:00<00:00, 5906.83it/s, Materializing param=roberta.encoder.layer.2.output.dense.bias]      Loading weights:  26%|██▋       | 52/197 [00:00<00:00, 5889.44it/s, Materializing param=roberta.encoder.layer.2.output.dense.bias]Loading weights:  27%|██▋       | 53/197 [00:00<00:00, 5822.83it/s, Materializing param=roberta.encoder.layer.2.output.dense.weight]Loading weights:  27%|██▋       | 53/197 [00:00<00:00, 5805.95it/s, Materializing param=roberta.encoder.layer.2.output.dense.weight]Loading weights:  27%|██▋       | 54/197 [00:00<00:00, 5781.11it/s, Materializing param=roberta.encoder.layer.3.attention.output.LayerNorm.bias]Loading weights:  27%|██▋       | 54/197 [00:00<00:00, 5761.26it/s, Materializing param=roberta.encoder.layer.3.attention.output.LayerNorm.bias]Loading weights:  28%|██▊       | 55/197 [00:00<00:00, 5783.80it/s, Materializing param=roberta.encoder.layer.3.attention.output.LayerNorm.weight]Loading weights:  28%|██▊       | 55/197 [00:00<00:00, 5474.03it/s, Materializing param=roberta.encoder.layer.3.attention.output.LayerNorm.weight]Loading weights:  28%|██▊       | 56/197 [00:00<00:00, 5435.17it/s, Materializing param=roberta.encoder.layer.3.attention.output.dense.bias]      Loading weights:  28%|██▊       | 56/197 [00:00<00:00, 5403.04it/s, Materializing param=roberta.encoder.layer.3.attention.output.dense.bias]Loading weights:  29%|██▉       | 57/197 [00:00<00:00, 5390.89it/s, Materializing param=roberta.encoder.layer.3.attention.output.dense.weight]Loading weights:  29%|██▉       | 57/197 [00:00<00:00, 5363.92it/s, Materializing param=roberta.encoder.layer.3.attention.output.dense.weight]Loading weights:  29%|██▉       | 58/197 [00:00<00:00, 5384.93it/s, Materializing param=roberta.encoder.layer.3.attention.self.key.bias]      Loading weights:  29%|██▉       | 58/197 [00:00<00:00, 5359.19it/s, Materializing param=roberta.encoder.layer.3.attention.self.key.bias]Loading weights:  30%|██▉       | 59/197 [00:00<00:00, 5396.19it/s, Materializing param=roberta.encoder.layer.3.attention.self.key.weight]Loading weights:  30%|██▉       | 59/197 [00:00<00:00, 5372.06it/s, Materializing param=roberta.encoder.layer.3.attention.self.key.weight]Loading weights:  30%|███       | 60/197 [00:00<00:00, 5339.32it/s, Materializing param=roberta.encoder.layer.3.attention.self.query.bias]Loading weights:  30%|███       | 60/197 [00:00<00:00, 5299.97it/s, Materializing param=roberta.encoder.layer.3.attention.self.query.bias]Loading weights:  31%|███       | 61/197 [00:00<00:00, 5291.24it/s, Materializing param=roberta.encoder.layer.3.attention.self.query.weight]Loading weights:  31%|███       | 61/197 [00:00<00:00, 5267.27it/s, Materializing param=roberta.encoder.layer.3.attention.self.query.weight]Loading weights:  31%|███▏      | 62/197 [00:00<00:00, 5293.79it/s, Materializing param=roberta.encoder.layer.3.attention.self.value.bias]  Loading weights:  31%|███▏      | 62/197 [00:00<00:00, 5272.11it/s, Materializing param=roberta.encoder.layer.3.attention.self.value.bias]Loading weights:  32%|███▏      | 63/197 [00:00<00:00, 5312.77it/s, Materializing param=roberta.encoder.layer.3.attention.self.value.weight]Loading weights:  32%|███▏      | 63/197 [00:00<00:00, 5292.55it/s, Materializing param=roberta.encoder.layer.3.attention.self.value.weight]Loading weights:  32%|███▏      | 64/197 [00:00<00:00, 5198.51it/s, Materializing param=roberta.encoder.layer.3.intermediate.dense.bias]    Loading weights:  32%|███▏      | 64/197 [00:00<00:00, 5177.45it/s, Materializing param=roberta.encoder.layer.3.intermediate.dense.bias]Loading weights:  33%|███▎      | 65/197 [00:00<00:00, 5211.71it/s, Materializing param=roberta.encoder.layer.3.intermediate.dense.weight]Loading weights:  33%|███▎      | 65/197 [00:00<00:00, 5192.45it/s, Materializing param=roberta.encoder.layer.3.intermediate.dense.weight]Loading weights:  34%|███▎      | 66/197 [00:00<00:00, 5125.80it/s, Materializing param=roberta.encoder.layer.3.output.LayerNorm.bias]    Loading weights:  34%|███▎      | 66/197 [00:00<00:00, 5096.83it/s, Materializing param=roberta.encoder.layer.3.output.LayerNorm.bias]Loading weights:  34%|███▍      | 67/197 [00:00<00:00, 5091.00it/s, Materializing param=roberta.encoder.layer.3.output.LayerNorm.weight]Loading weights:  34%|███▍      | 67/197 [00:00<00:00, 5073.08it/s, Materializing param=roberta.encoder.layer.3.output.LayerNorm.weight]Loading weights:  35%|███▍      | 68/197 [00:00<00:00, 5013.41it/s, Materializing param=roberta.encoder.layer.3.output.dense.bias]      Loading weights:  35%|███▍      | 68/197 [00:00<00:00, 4995.58it/s, Materializing param=roberta.encoder.layer.3.output.dense.bias]Loading weights:  35%|███▌      | 69/197 [00:00<00:00, 5003.06it/s, Materializing param=roberta.encoder.layer.3.output.dense.weight]Loading weights:  35%|███▌      | 69/197 [00:00<00:00, 4986.25it/s, Materializing param=roberta.encoder.layer.3.output.dense.weight]Loading weights:  36%|███▌      | 70/197 [00:00<00:00, 4997.38it/s, Materializing param=roberta.encoder.layer.4.attention.output.LayerNorm.bias]Loading weights:  36%|███▌      | 70/197 [00:00<00:00, 4932.40it/s, Materializing param=roberta.encoder.layer.4.attention.output.LayerNorm.bias]Loading weights:  36%|███▌      | 71/197 [00:00<00:00, 4957.23it/s, Materializing param=roberta.encoder.layer.4.attention.output.LayerNorm.weight]Loading weights:  36%|███▌      | 71/197 [00:00<00:00, 4941.19it/s, Materializing param=roberta.encoder.layer.4.attention.output.LayerNorm.weight]Loading weights:  37%|███▋      | 72/197 [00:00<00:00, 4976.27it/s, Materializing param=roberta.encoder.layer.4.attention.output.dense.bias]      Loading weights:  37%|███▋      | 72/197 [00:00<00:00, 4960.74it/s, Materializing param=roberta.encoder.layer.4.attention.output.dense.bias]Loading weights:  37%|███▋      | 73/197 [00:00<00:00, 4980.55it/s, Materializing param=roberta.encoder.layer.4.attention.output.dense.weight]Loading weights:  37%|███▋      | 73/197 [00:00<00:00, 4902.24it/s, Materializing param=roberta.encoder.layer.4.attention.output.dense.weight]Loading weights:  38%|███▊      | 74/197 [00:00<00:00, 4845.05it/s, Materializing param=roberta.encoder.layer.4.attention.self.key.bias]      Loading weights:  38%|███▊      | 74/197 [00:00<00:00, 4820.44it/s, Materializing param=roberta.encoder.layer.4.attention.self.key.bias]Loading weights:  38%|███▊      | 75/197 [00:00<00:00, 4854.59it/s, Materializing param=roberta.encoder.layer.4.attention.self.key.weight]Loading weights:  38%|███▊      | 75/197 [00:00<00:00, 4844.43it/s, Materializing param=roberta.encoder.layer.4.attention.self.key.weight]Loading weights:  39%|███▊      | 76/197 [00:00<00:00, 4888.09it/s, Materializing param=roberta.encoder.layer.4.attention.self.query.bias]Loading weights:  39%|███▊      | 76/197 [00:00<00:00, 4879.71it/s, Materializing param=roberta.encoder.layer.4.attention.self.query.bias]Loading weights:  39%|███▉      | 77/197 [00:00<00:00, 4924.09it/s, Materializing param=roberta.encoder.layer.4.attention.self.query.weight]Loading weights:  39%|███▉      | 77/197 [00:00<00:00, 4915.25it/s, Materializing param=roberta.encoder.layer.4.attention.self.query.weight]Loading weights:  40%|███▉      | 78/197 [00:00<00:00, 4957.88it/s, Materializing param=roberta.encoder.layer.4.attention.self.value.bias]  Loading weights:  40%|███▉      | 78/197 [00:00<00:00, 4949.71it/s, Materializing param=roberta.encoder.layer.4.attention.self.value.bias]Loading weights:  40%|████      | 79/197 [00:00<00:00, 4947.37it/s, Materializing param=roberta.encoder.layer.4.attention.self.value.weight]Loading weights:  40%|████      | 79/197 [00:00<00:00, 4923.70it/s, Materializing param=roberta.encoder.layer.4.attention.self.value.weight]Loading weights:  41%|████      | 80/197 [00:00<00:00, 4943.71it/s, Materializing param=roberta.encoder.layer.4.intermediate.dense.bias]    Loading weights:  41%|████      | 80/197 [00:00<00:00, 4926.51it/s, Materializing param=roberta.encoder.layer.4.intermediate.dense.bias]Loading weights:  41%|████      | 81/197 [00:00<00:00, 4950.22it/s, Materializing param=roberta.encoder.layer.4.intermediate.dense.weight]Loading weights:  41%|████      | 81/197 [00:00<00:00, 4934.62it/s, Materializing param=roberta.encoder.layer.4.intermediate.dense.weight]Loading weights:  42%|████▏     | 82/197 [00:00<00:00, 4932.99it/s, Materializing param=roberta.encoder.layer.4.output.LayerNorm.bias]    Loading weights:  42%|████▏     | 82/197 [00:00<00:00, 4917.19it/s, Materializing param=roberta.encoder.layer.4.output.LayerNorm.bias]Loading weights:  42%|████▏     | 83/197 [00:00<00:00, 4943.58it/s, Materializing param=roberta.encoder.layer.4.output.LayerNorm.weight]Loading weights:  42%|████▏     | 83/197 [00:00<00:00, 4928.96it/s, Materializing param=roberta.encoder.layer.4.output.LayerNorm.weight]Loading weights:  43%|████▎     | 84/197 [00:00<00:00, 4956.90it/s, Materializing param=roberta.encoder.layer.4.output.dense.bias]      Loading weights:  43%|████▎     | 84/197 [00:00<00:00, 4942.71it/s, Materializing param=roberta.encoder.layer.4.output.dense.bias]Loading weights:  43%|████▎     | 85/197 [00:00<00:00, 4970.39it/s, Materializing param=roberta.encoder.layer.4.output.dense.weight]Loading weights:  43%|████▎     | 85/197 [00:00<00:00, 4956.70it/s, Materializing param=roberta.encoder.layer.4.output.dense.weight]Loading weights:  44%|████▎     | 86/197 [00:00<00:00, 4981.70it/s, Materializing param=roberta.encoder.layer.5.attention.output.LayerNorm.bias]Loading weights:  44%|████▎     | 86/197 [00:00<00:00, 4966.13it/s, Materializing param=roberta.encoder.layer.5.attention.output.LayerNorm.bias]Loading weights:  44%|████▍     | 87/197 [00:00<00:00, 4989.81it/s, Materializing param=roberta.encoder.layer.5.attention.output.LayerNorm.weight]Loading weights:  44%|████▍     | 87/197 [00:00<00:00, 4975.92it/s, Materializing param=roberta.encoder.layer.5.attention.output.LayerNorm.weight]Loading weights:  45%|████▍     | 88/197 [00:00<00:00, 5001.88it/s, Materializing param=roberta.encoder.layer.5.attention.output.dense.bias]      Loading weights:  45%|████▍     | 88/197 [00:00<00:00, 4988.29it/s, Materializing param=roberta.encoder.layer.5.attention.output.dense.bias]Loading weights:  45%|████▌     | 89/197 [00:00<00:00, 5009.03it/s, Materializing param=roberta.encoder.layer.5.attention.output.dense.weight]Loading weights:  45%|████▌     | 89/197 [00:00<00:00, 4996.16it/s, Materializing param=roberta.encoder.layer.5.attention.output.dense.weight]Loading weights:  46%|████▌     | 90/197 [00:00<00:00, 5023.45it/s, Materializing param=roberta.encoder.layer.5.attention.self.key.bias]      Loading weights:  46%|████▌     | 90/197 [00:00<00:00, 5010.78it/s, Materializing param=roberta.encoder.layer.5.attention.self.key.bias]Loading weights:  46%|████▌     | 91/197 [00:00<00:00, 5038.97it/s, Materializing param=roberta.encoder.layer.5.attention.self.key.weight]Loading weights:  46%|████▌     | 91/197 [00:00<00:00, 5026.69it/s, Materializing param=roberta.encoder.layer.5.attention.self.key.weight]Loading weights:  47%|████▋     | 92/197 [00:00<00:00, 5049.15it/s, Materializing param=roberta.encoder.layer.5.attention.self.query.bias]Loading weights:  47%|████▋     | 92/197 [00:00<00:00, 5036.89it/s, Materializing param=roberta.encoder.layer.5.attention.self.query.bias]Loading weights:  47%|████▋     | 93/197 [00:00<00:00, 5063.35it/s, Materializing param=roberta.encoder.layer.5.attention.self.query.weight]Loading weights:  47%|████▋     | 93/197 [00:00<00:00, 5051.28it/s, Materializing param=roberta.encoder.layer.5.attention.self.query.weight]Loading weights:  48%|████▊     | 94/197 [00:00<00:00, 5028.95it/s, Materializing param=roberta.encoder.layer.5.attention.self.value.bias]  Loading weights:  48%|████▊     | 94/197 [00:00<00:00, 5016.41it/s, Materializing param=roberta.encoder.layer.5.attention.self.value.bias]Loading weights:  48%|████▊     | 95/197 [00:00<00:00, 5041.36it/s, Materializing param=roberta.encoder.layer.5.attention.self.value.weight]Loading weights:  48%|████▊     | 95/197 [00:00<00:00, 5029.01it/s, Materializing param=roberta.encoder.layer.5.attention.self.value.weight]Loading weights:  49%|████▊     | 96/197 [00:00<00:00, 5055.47it/s, Materializing param=roberta.encoder.layer.5.intermediate.dense.bias]    Loading weights:  49%|████▊     | 96/197 [00:00<00:00, 5043.82it/s, Materializing param=roberta.encoder.layer.5.intermediate.dense.bias]Loading weights:  49%|████▉     | 97/197 [00:00<00:00, 5070.89it/s, Materializing param=roberta.encoder.layer.5.intermediate.dense.weight]Loading weights:  49%|████▉     | 97/197 [00:00<00:00, 5059.54it/s, Materializing param=roberta.encoder.layer.5.intermediate.dense.weight]Loading weights:  50%|████▉     | 98/197 [00:00<00:00, 5081.55it/s, Materializing param=roberta.encoder.layer.5.output.LayerNorm.bias]    Loading weights:  50%|████▉     | 98/197 [00:00<00:00, 5069.96it/s, Materializing param=roberta.encoder.layer.5.output.LayerNorm.bias]Loading weights:  50%|█████     | 99/197 [00:00<00:00, 4959.82it/s, Materializing param=roberta.encoder.layer.5.output.LayerNorm.weight]Loading weights:  50%|█████     | 99/197 [00:00<00:00, 4943.70it/s, Materializing param=roberta.encoder.layer.5.output.LayerNorm.weight]Loading weights:  51%|█████     | 100/197 [00:00<00:00, 4913.09it/s, Materializing param=roberta.encoder.layer.5.output.dense.bias]     Loading weights:  51%|█████     | 100/197 [00:00<00:00, 4886.19it/s, Materializing param=roberta.encoder.layer.5.output.dense.bias]Loading weights:  51%|█████▏    | 101/197 [00:00<00:00, 4906.76it/s, Materializing param=roberta.encoder.layer.5.output.dense.weight]Loading weights:  51%|█████▏    | 101/197 [00:00<00:00, 4895.19it/s, Materializing param=roberta.encoder.layer.5.output.dense.weight]Loading weights:  52%|█████▏    | 102/197 [00:00<00:00, 4848.91it/s, Materializing param=roberta.encoder.layer.6.attention.output.LayerNorm.bias]Loading weights:  52%|█████▏    | 102/197 [00:00<00:00, 4820.12it/s, Materializing param=roberta.encoder.layer.6.attention.output.LayerNorm.bias]Loading weights:  52%|█████▏    | 103/197 [00:00<00:00, 4824.81it/s, Materializing param=roberta.encoder.layer.6.attention.output.LayerNorm.weight]Loading weights:  52%|█████▏    | 103/197 [00:00<00:00, 4814.16it/s, Materializing param=roberta.encoder.layer.6.attention.output.LayerNorm.weight]Loading weights:  53%|█████▎    | 104/197 [00:00<00:00, 4830.33it/s, Materializing param=roberta.encoder.layer.6.attention.output.dense.bias]      Loading weights:  53%|█████▎    | 104/197 [00:00<00:00, 4772.83it/s, Materializing param=roberta.encoder.layer.6.attention.output.dense.bias]Loading weights:  53%|█████▎    | 105/197 [00:00<00:00, 4761.62it/s, Materializing param=roberta.encoder.layer.6.attention.output.dense.weight]Loading weights:  53%|█████▎    | 105/197 [00:00<00:00, 4743.97it/s, Materializing param=roberta.encoder.layer.6.attention.output.dense.weight]Loading weights:  54%|█████▍    | 106/197 [00:00<00:00, 4743.12it/s, Materializing param=roberta.encoder.layer.6.attention.self.key.bias]      Loading weights:  54%|█████▍    | 106/197 [00:00<00:00, 4733.32it/s, Materializing param=roberta.encoder.layer.6.attention.self.key.bias]Loading weights:  54%|█████▍    | 107/197 [00:00<00:00, 4755.35it/s, Materializing param=roberta.encoder.layer.6.attention.self.key.weight]Loading weights:  54%|█████▍    | 107/197 [00:00<00:00, 4744.74it/s, Materializing param=roberta.encoder.layer.6.attention.self.key.weight]Loading weights:  55%|█████▍    | 108/197 [00:00<00:00, 4764.80it/s, Materializing param=roberta.encoder.layer.6.attention.self.query.bias]Loading weights:  55%|█████▍    | 108/197 [00:00<00:00, 4753.50it/s, Materializing param=roberta.encoder.layer.6.attention.self.query.bias]Loading weights:  55%|█████▌    | 109/197 [00:00<00:00, 4739.23it/s, Materializing param=roberta.encoder.layer.6.attention.self.query.weight]Loading weights:  55%|█████▌    | 109/197 [00:00<00:00, 4728.25it/s, Materializing param=roberta.encoder.layer.6.attention.self.query.weight]Loading weights:  56%|█████▌    | 110/197 [00:00<00:00, 4692.81it/s, Materializing param=roberta.encoder.layer.6.attention.self.value.bias]  Loading weights:  56%|█████▌    | 110/197 [00:00<00:00, 4640.84it/s, Materializing param=roberta.encoder.layer.6.attention.self.value.bias]Loading weights:  56%|█████▋    | 111/197 [00:00<00:00, 4639.53it/s, Materializing param=roberta.encoder.layer.6.attention.self.value.weight]Loading weights:  56%|█████▋    | 111/197 [00:00<00:00, 4630.07it/s, Materializing param=roberta.encoder.layer.6.attention.self.value.weight]Loading weights:  57%|█████▋    | 112/197 [00:00<00:00, 4620.32it/s, Materializing param=roberta.encoder.layer.6.intermediate.dense.bias]    Loading weights:  57%|█████▋    | 112/197 [00:00<00:00, 4592.54it/s, Materializing param=roberta.encoder.layer.6.intermediate.dense.bias]Loading weights:  57%|█████▋    | 113/197 [00:00<00:00, 4567.24it/s, Materializing param=roberta.encoder.layer.6.intermediate.dense.weight]Loading weights:  57%|█████▋    | 113/197 [00:00<00:00, 4557.36it/s, Materializing param=roberta.encoder.layer.6.intermediate.dense.weight]Loading weights:  58%|█████▊    | 114/197 [00:00<00:00, 4566.73it/s, Materializing param=roberta.encoder.layer.6.output.LayerNorm.bias]    Loading weights:  58%|█████▊    | 114/197 [00:00<00:00, 4558.55it/s, Materializing param=roberta.encoder.layer.6.output.LayerNorm.bias]Loading weights:  58%|█████▊    | 115/197 [00:00<00:00, 4566.58it/s, Materializing param=roberta.encoder.layer.6.output.LayerNorm.weight]Loading weights:  58%|█████▊    | 115/197 [00:00<00:00, 4547.55it/s, Materializing param=roberta.encoder.layer.6.output.LayerNorm.weight]Loading weights:  59%|█████▉    | 116/197 [00:00<00:00, 4457.32it/s, Materializing param=roberta.encoder.layer.6.output.dense.bias]      Loading weights:  59%|█████▉    | 116/197 [00:00<00:00, 4435.30it/s, Materializing param=roberta.encoder.layer.6.output.dense.bias]Loading weights:  59%|█████▉    | 117/197 [00:00<00:00, 4450.90it/s, Materializing param=roberta.encoder.layer.6.output.dense.weight]Loading weights:  59%|█████▉    | 117/197 [00:00<00:00, 4441.39it/s, Materializing param=roberta.encoder.layer.6.output.dense.weight]Loading weights:  60%|█████▉    | 118/197 [00:00<00:00, 4454.80it/s, Materializing param=roberta.encoder.layer.7.attention.output.LayerNorm.bias]Loading weights:  60%|█████▉    | 118/197 [00:00<00:00, 4444.52it/s, Materializing param=roberta.encoder.layer.7.attention.output.LayerNorm.bias]Loading weights:  60%|██████    | 119/197 [00:00<00:00, 4455.93it/s, Materializing param=roberta.encoder.layer.7.attention.output.LayerNorm.weight]Loading weights:  60%|██████    | 119/197 [00:00<00:00, 4445.14it/s, Materializing param=roberta.encoder.layer.7.attention.output.LayerNorm.weight]Loading weights:  61%|██████    | 120/197 [00:00<00:00, 4451.72it/s, Materializing param=roberta.encoder.layer.7.attention.output.dense.bias]      Loading weights:  61%|██████    | 120/197 [00:00<00:00, 4417.54it/s, Materializing param=roberta.encoder.layer.7.attention.output.dense.bias]Loading weights:  61%|██████▏   | 121/197 [00:00<00:00, 4246.10it/s, Materializing param=roberta.encoder.layer.7.attention.output.dense.weight]Loading weights:  61%|██████▏   | 121/197 [00:00<00:00, 4236.21it/s, Materializing param=roberta.encoder.layer.7.attention.output.dense.weight]Loading weights:  62%|██████▏   | 122/197 [00:00<00:00, 4240.29it/s, Materializing param=roberta.encoder.layer.7.attention.self.key.bias]      Loading weights:  62%|██████▏   | 122/197 [00:00<00:00, 4201.78it/s, Materializing param=roberta.encoder.layer.7.attention.self.key.bias]Loading weights:  62%|██████▏   | 123/197 [00:00<00:00, 4032.83it/s, Materializing param=roberta.encoder.layer.7.attention.self.key.weight]Loading weights:  62%|██████▏   | 123/197 [00:00<00:00, 3971.05it/s, Materializing param=roberta.encoder.layer.7.attention.self.key.weight]Loading weights:  63%|██████▎   | 124/197 [00:00<00:00, 3940.76it/s, Materializing param=roberta.encoder.layer.7.attention.self.query.bias]Loading weights:  63%|██████▎   | 124/197 [00:00<00:00, 3721.07it/s, Materializing param=roberta.encoder.layer.7.attention.self.query.bias]Loading weights:  63%|██████▎   | 125/197 [00:00<00:00, 3507.27it/s, Materializing param=roberta.encoder.layer.7.attention.self.query.weight]Loading weights:  63%|██████▎   | 125/197 [00:00<00:00, 3486.07it/s, Materializing param=roberta.encoder.layer.7.attention.self.query.weight]Loading weights:  64%|██████▍   | 126/197 [00:00<00:00, 3479.38it/s, Materializing param=roberta.encoder.layer.7.attention.self.value.bias]  Loading weights:  64%|██████▍   | 126/197 [00:00<00:00, 3453.28it/s, Materializing param=roberta.encoder.layer.7.attention.self.value.bias]Loading weights:  64%|██████▍   | 127/197 [00:00<00:00, 3423.42it/s, Materializing param=roberta.encoder.layer.7.attention.self.value.weight]Loading weights:  64%|██████▍   | 127/197 [00:00<00:00, 3408.76it/s, Materializing param=roberta.encoder.layer.7.attention.self.value.weight]Loading weights:  65%|██████▍   | 128/197 [00:00<00:00, 3411.78it/s, Materializing param=roberta.encoder.layer.7.intermediate.dense.bias]    Loading weights:  65%|██████▍   | 128/197 [00:00<00:00, 3406.37it/s, Materializing param=roberta.encoder.layer.7.intermediate.dense.bias]Loading weights:  65%|██████▌   | 129/197 [00:00<00:00, 3394.37it/s, Materializing param=roberta.encoder.layer.7.intermediate.dense.weight]Loading weights:  65%|██████▌   | 129/197 [00:00<00:00, 3390.20it/s, Materializing param=roberta.encoder.layer.7.intermediate.dense.weight]Loading weights:  66%|██████▌   | 130/197 [00:00<00:00, 3369.73it/s, Materializing param=roberta.encoder.layer.7.output.LayerNorm.bias]    Loading weights:  66%|██████▌   | 130/197 [00:00<00:00, 3347.10it/s, Materializing param=roberta.encoder.layer.7.output.LayerNorm.bias]Loading weights:  66%|██████▋   | 131/197 [00:00<00:00, 3344.94it/s, Materializing param=roberta.encoder.layer.7.output.LayerNorm.weight]Loading weights:  66%|██████▋   | 131/197 [00:00<00:00, 3339.45it/s, Materializing param=roberta.encoder.layer.7.output.LayerNorm.weight]Loading weights:  67%|██████▋   | 132/197 [00:00<00:00, 3334.55it/s, Materializing param=roberta.encoder.layer.7.output.dense.bias]      Loading weights:  67%|██████▋   | 132/197 [00:00<00:00, 3329.49it/s, Materializing param=roberta.encoder.layer.7.output.dense.bias]Loading weights:  68%|██████▊   | 133/197 [00:00<00:00, 3322.78it/s, Materializing param=roberta.encoder.layer.7.output.dense.weight]Loading weights:  68%|██████▊   | 133/197 [00:00<00:00, 3317.90it/s, Materializing param=roberta.encoder.layer.7.output.dense.weight]Loading weights:  68%|██████▊   | 134/197 [00:00<00:00, 3334.14it/s, Materializing param=roberta.encoder.layer.8.attention.output.LayerNorm.bias]Loading weights:  68%|██████▊   | 134/197 [00:00<00:00, 3329.80it/s, Materializing param=roberta.encoder.layer.8.attention.output.LayerNorm.bias]Loading weights:  69%|██████▊   | 135/197 [00:00<00:00, 3345.31it/s, Materializing param=roberta.encoder.layer.8.attention.output.LayerNorm.weight]Loading weights:  69%|██████▊   | 135/197 [00:00<00:00, 3341.03it/s, Materializing param=roberta.encoder.layer.8.attention.output.LayerNorm.weight]Loading weights:  69%|██████▉   | 136/197 [00:00<00:00, 3357.58it/s, Materializing param=roberta.encoder.layer.8.attention.output.dense.bias]      Loading weights:  69%|██████▉   | 136/197 [00:00<00:00, 3353.57it/s, Materializing param=roberta.encoder.layer.8.attention.output.dense.bias]Loading weights:  70%|██████▉   | 137/197 [00:00<00:00, 3370.12it/s, Materializing param=roberta.encoder.layer.8.attention.output.dense.weight]Loading weights:  70%|██████▉   | 137/197 [00:00<00:00, 3366.14it/s, Materializing param=roberta.encoder.layer.8.attention.output.dense.weight]Loading weights:  70%|███████   | 138/197 [00:00<00:00, 3382.84it/s, Materializing param=roberta.encoder.layer.8.attention.self.key.bias]      Loading weights:  70%|███████   | 138/197 [00:00<00:00, 3378.95it/s, Materializing param=roberta.encoder.layer.8.attention.self.key.bias]Loading weights:  71%|███████   | 139/197 [00:00<00:00, 3395.76it/s, Materializing param=roberta.encoder.layer.8.attention.self.key.weight]Loading weights:  71%|███████   | 139/197 [00:00<00:00, 3391.97it/s, Materializing param=roberta.encoder.layer.8.attention.self.key.weight]Loading weights:  71%|███████   | 140/197 [00:00<00:00, 3409.03it/s, Materializing param=roberta.encoder.layer.8.attention.self.query.bias]Loading weights:  71%|███████   | 140/197 [00:00<00:00, 3405.24it/s, Materializing param=roberta.encoder.layer.8.attention.self.query.bias]Loading weights:  72%|███████▏  | 141/197 [00:00<00:00, 3422.34it/s, Materializing param=roberta.encoder.layer.8.attention.self.query.weight]Loading weights:  72%|███████▏  | 141/197 [00:00<00:00, 3418.62it/s, Materializing param=roberta.encoder.layer.8.attention.self.query.weight]Loading weights:  72%|███████▏  | 142/197 [00:00<00:00, 3434.45it/s, Materializing param=roberta.encoder.layer.8.attention.self.value.bias]  Loading weights:  72%|███████▏  | 142/197 [00:00<00:00, 3429.82it/s, Materializing param=roberta.encoder.layer.8.attention.self.value.bias]Loading weights:  73%|███████▎  | 143/197 [00:00<00:00, 3445.78it/s, Materializing param=roberta.encoder.layer.8.attention.self.value.weight]Loading weights:  73%|███████▎  | 143/197 [00:00<00:00, 3441.94it/s, Materializing param=roberta.encoder.layer.8.attention.self.value.weight]Loading weights:  73%|███████▎  | 144/197 [00:00<00:00, 3458.88it/s, Materializing param=roberta.encoder.layer.8.intermediate.dense.bias]    Loading weights:  73%|███████▎  | 144/197 [00:00<00:00, 3455.32it/s, Materializing param=roberta.encoder.layer.8.intermediate.dense.bias]Loading weights:  74%|███████▎  | 145/197 [00:00<00:00, 3472.60it/s, Materializing param=roberta.encoder.layer.8.intermediate.dense.weight]Loading weights:  74%|███████▎  | 145/197 [00:00<00:00, 3469.12it/s, Materializing param=roberta.encoder.layer.8.intermediate.dense.weight]Loading weights:  74%|███████▍  | 146/197 [00:00<00:00, 3486.00it/s, Materializing param=roberta.encoder.layer.8.output.LayerNorm.bias]    Loading weights:  74%|███████▍  | 146/197 [00:00<00:00, 3482.49it/s, Materializing param=roberta.encoder.layer.8.output.LayerNorm.bias]Loading weights:  75%|███████▍  | 147/197 [00:00<00:00, 3499.66it/s, Materializing param=roberta.encoder.layer.8.output.LayerNorm.weight]Loading weights:  75%|███████▍  | 147/197 [00:00<00:00, 3496.24it/s, Materializing param=roberta.encoder.layer.8.output.LayerNorm.weight]Loading weights:  75%|███████▌  | 148/197 [00:00<00:00, 3513.63it/s, Materializing param=roberta.encoder.layer.8.output.dense.bias]      Loading weights:  75%|███████▌  | 148/197 [00:00<00:00, 3510.29it/s, Materializing param=roberta.encoder.layer.8.output.dense.bias]Loading weights:  76%|███████▌  | 149/197 [00:00<00:00, 3527.45it/s, Materializing param=roberta.encoder.layer.8.output.dense.weight]Loading weights:  76%|███████▌  | 149/197 [00:00<00:00, 3523.55it/s, Materializing param=roberta.encoder.layer.8.output.dense.weight]Loading weights:  76%|███████▌  | 150/197 [00:00<00:00, 3540.59it/s, Materializing param=roberta.encoder.layer.9.attention.output.LayerNorm.bias]Loading weights:  76%|███████▌  | 150/197 [00:00<00:00, 3537.09it/s, Materializing param=roberta.encoder.layer.9.attention.output.LayerNorm.bias]Loading weights:  77%|███████▋  | 151/197 [00:00<00:00, 3553.90it/s, Materializing param=roberta.encoder.layer.9.attention.output.LayerNorm.weight]Loading weights:  77%|███████▋  | 151/197 [00:00<00:00, 3550.45it/s, Materializing param=roberta.encoder.layer.9.attention.output.LayerNorm.weight]Loading weights:  77%|███████▋  | 152/197 [00:00<00:00, 3567.52it/s, Materializing param=roberta.encoder.layer.9.attention.output.dense.bias]      Loading weights:  77%|███████▋  | 152/197 [00:00<00:00, 3564.11it/s, Materializing param=roberta.encoder.layer.9.attention.output.dense.bias]Loading weights:  78%|███████▊  | 153/197 [00:00<00:00, 3581.07it/s, Materializing param=roberta.encoder.layer.9.attention.output.dense.weight]Loading weights:  78%|███████▊  | 153/197 [00:00<00:00, 3577.68it/s, Materializing param=roberta.encoder.layer.9.attention.output.dense.weight]Loading weights:  78%|███████▊  | 154/197 [00:00<00:00, 3594.59it/s, Materializing param=roberta.encoder.layer.9.attention.self.key.bias]      Loading weights:  78%|███████▊  | 154/197 [00:00<00:00, 3591.21it/s, Materializing param=roberta.encoder.layer.9.attention.self.key.bias]Loading weights:  79%|███████▊  | 155/197 [00:00<00:00, 3607.93it/s, Materializing param=roberta.encoder.layer.9.attention.self.key.weight]Loading weights:  79%|███████▊  | 155/197 [00:00<00:00, 3604.39it/s, Materializing param=roberta.encoder.layer.9.attention.self.key.weight]Loading weights:  79%|███████▉  | 156/197 [00:00<00:00, 3621.08it/s, Materializing param=roberta.encoder.layer.9.attention.self.query.bias]Loading weights:  79%|███████▉  | 156/197 [00:00<00:00, 3617.70it/s, Materializing param=roberta.encoder.layer.9.attention.self.query.bias]Loading weights:  80%|███████▉  | 157/197 [00:00<00:00, 3634.34it/s, Materializing param=roberta.encoder.layer.9.attention.self.query.weight]Loading weights:  80%|███████▉  | 157/197 [00:00<00:00, 3630.83it/s, Materializing param=roberta.encoder.layer.9.attention.self.query.weight]Loading weights:  80%|████████  | 158/197 [00:00<00:00, 3434.78it/s, Materializing param=roberta.encoder.layer.9.attention.self.value.bias]  Loading weights:  80%|████████  | 158/197 [00:00<00:00, 3412.50it/s, Materializing param=roberta.encoder.layer.9.attention.self.value.bias]Loading weights:  81%|████████  | 159/197 [00:00<00:00, 3422.38it/s, Materializing param=roberta.encoder.layer.9.attention.self.value.weight]Loading weights:  81%|████████  | 159/197 [00:00<00:00, 3418.27it/s, Materializing param=roberta.encoder.layer.9.attention.self.value.weight]Loading weights:  81%|████████  | 160/197 [00:00<00:00, 3429.84it/s, Materializing param=roberta.encoder.layer.9.intermediate.dense.bias]    Loading weights:  81%|████████  | 160/197 [00:00<00:00, 3425.02it/s, Materializing param=roberta.encoder.layer.9.intermediate.dense.bias]Loading weights:  82%|████████▏ | 161/197 [00:00<00:00, 3441.16it/s, Materializing param=roberta.encoder.layer.9.intermediate.dense.weight]Loading weights:  82%|████████▏ | 161/197 [00:00<00:00, 3438.95it/s, Materializing param=roberta.encoder.layer.9.intermediate.dense.weight]Loading weights:  82%|████████▏ | 162/197 [00:00<00:00, 3456.56it/s, Materializing param=roberta.encoder.layer.9.output.LayerNorm.bias]    Loading weights:  82%|████████▏ | 162/197 [00:00<00:00, 3454.75it/s, Materializing param=roberta.encoder.layer.9.output.LayerNorm.bias]Loading weights:  83%|████████▎ | 163/197 [00:00<00:00, 3472.49it/s, Materializing param=roberta.encoder.layer.9.output.LayerNorm.weight]Loading weights:  83%|████████▎ | 163/197 [00:00<00:00, 3470.54it/s, Materializing param=roberta.encoder.layer.9.output.LayerNorm.weight]Loading weights:  83%|████████▎ | 164/197 [00:00<00:00, 3488.45it/s, Materializing param=roberta.encoder.layer.9.output.dense.bias]      Loading weights:  83%|████████▎ | 164/197 [00:00<00:00, 3486.47it/s, Materializing param=roberta.encoder.layer.9.output.dense.bias]Loading weights:  84%|████████▍ | 165/197 [00:00<00:00, 3504.33it/s, Materializing param=roberta.encoder.layer.9.output.dense.weight]Loading weights:  84%|████████▍ | 165/197 [00:00<00:00, 3502.36it/s, Materializing param=roberta.encoder.layer.9.output.dense.weight]Loading weights:  84%|████████▍ | 166/197 [00:00<00:00, 3519.64it/s, Materializing param=roberta.encoder.layer.10.attention.output.LayerNorm.bias]Loading weights:  84%|████████▍ | 166/197 [00:00<00:00, 3517.32it/s, Materializing param=roberta.encoder.layer.10.attention.output.LayerNorm.bias]Loading weights:  85%|████████▍ | 167/197 [00:00<00:00, 3534.07it/s, Materializing param=roberta.encoder.layer.10.attention.output.LayerNorm.weight]Loading weights:  85%|████████▍ | 167/197 [00:00<00:00, 3530.65it/s, Materializing param=roberta.encoder.layer.10.attention.output.LayerNorm.weight]Loading weights:  85%|████████▌ | 168/197 [00:00<00:00, 3545.84it/s, Materializing param=roberta.encoder.layer.10.attention.output.dense.bias]      Loading weights:  85%|████████▌ | 168/197 [00:00<00:00, 3543.11it/s, Materializing param=roberta.encoder.layer.10.attention.output.dense.bias]Loading weights:  86%|████████▌ | 169/197 [00:00<00:00, 3559.15it/s, Materializing param=roberta.encoder.layer.10.attention.output.dense.weight]Loading weights:  86%|████████▌ | 169/197 [00:00<00:00, 3556.60it/s, Materializing param=roberta.encoder.layer.10.attention.output.dense.weight]Loading weights:  86%|████████▋ | 170/197 [00:00<00:00, 3573.65it/s, Materializing param=roberta.encoder.layer.10.attention.self.key.bias]      Loading weights:  86%|████████▋ | 170/197 [00:00<00:00, 3571.57it/s, Materializing param=roberta.encoder.layer.10.attention.self.key.bias]Loading weights:  87%|████████▋ | 171/197 [00:00<00:00, 3588.77it/s, Materializing param=roberta.encoder.layer.10.attention.self.key.weight]Loading weights:  87%|████████▋ | 171/197 [00:00<00:00, 3586.87it/s, Materializing param=roberta.encoder.layer.10.attention.self.key.weight]Loading weights:  87%|████████▋ | 172/197 [00:00<00:00, 3604.45it/s, Materializing param=roberta.encoder.layer.10.attention.self.query.bias]Loading weights:  87%|████████▋ | 172/197 [00:00<00:00, 3602.44it/s, Materializing param=roberta.encoder.layer.10.attention.self.query.bias]Loading weights:  88%|████████▊ | 173/197 [00:00<00:00, 3619.86it/s, Materializing param=roberta.encoder.layer.10.attention.self.query.weight]Loading weights:  88%|████████▊ | 173/197 [00:00<00:00, 3617.74it/s, Materializing param=roberta.encoder.layer.10.attention.self.query.weight]Loading weights:  88%|████████▊ | 174/197 [00:00<00:00, 3634.76it/s, Materializing param=roberta.encoder.layer.10.attention.self.value.bias]  Loading weights:  88%|████████▊ | 174/197 [00:00<00:00, 3632.63it/s, Materializing param=roberta.encoder.layer.10.attention.self.value.bias]Loading weights:  89%|████████▉ | 175/197 [00:00<00:00, 3649.67it/s, Materializing param=roberta.encoder.layer.10.attention.self.value.weight]Loading weights:  89%|████████▉ | 175/197 [00:00<00:00, 3647.75it/s, Materializing param=roberta.encoder.layer.10.attention.self.value.weight]Loading weights:  89%|████████▉ | 176/197 [00:00<00:00, 3664.55it/s, Materializing param=roberta.encoder.layer.10.intermediate.dense.bias]    Loading weights:  89%|████████▉ | 176/197 [00:00<00:00, 3662.51it/s, Materializing param=roberta.encoder.layer.10.intermediate.dense.bias]Loading weights:  90%|████████▉ | 177/197 [00:00<00:00, 3679.80it/s, Materializing param=roberta.encoder.layer.10.intermediate.dense.weight]Loading weights:  90%|████████▉ | 177/197 [00:00<00:00, 3677.85it/s, Materializing param=roberta.encoder.layer.10.intermediate.dense.weight]Loading weights:  90%|█████████ | 178/197 [00:00<00:00, 3695.39it/s, Materializing param=roberta.encoder.layer.10.output.LayerNorm.bias]    Loading weights:  90%|█████████ | 178/197 [00:00<00:00, 3693.54it/s, Materializing param=roberta.encoder.layer.10.output.LayerNorm.bias]Loading weights:  91%|█████████ | 179/197 [00:00<00:00, 3710.91it/s, Materializing param=roberta.encoder.layer.10.output.LayerNorm.weight]Loading weights:  91%|█████████ | 179/197 [00:00<00:00, 3709.19it/s, Materializing param=roberta.encoder.layer.10.output.LayerNorm.weight]Loading weights:  91%|█████████▏| 180/197 [00:00<00:00, 3725.51it/s, Materializing param=roberta.encoder.layer.10.output.dense.bias]      Loading weights:  91%|█████████▏| 180/197 [00:00<00:00, 3723.69it/s, Materializing param=roberta.encoder.layer.10.output.dense.bias]Loading weights:  92%|█████████▏| 181/197 [00:00<00:00, 3741.30it/s, Materializing param=roberta.encoder.layer.10.output.dense.weight]Loading weights:  92%|█████████▏| 181/197 [00:00<00:00, 3739.62it/s, Materializing param=roberta.encoder.layer.10.output.dense.weight]Loading weights:  92%|█████████▏| 182/197 [00:00<00:00, 3757.15it/s, Materializing param=roberta.encoder.layer.11.attention.output.LayerNorm.bias]Loading weights:  92%|█████████▏| 182/197 [00:00<00:00, 3755.32it/s, Materializing param=roberta.encoder.layer.11.attention.output.LayerNorm.bias]Loading weights:  93%|█████████▎| 183/197 [00:00<00:00, 3772.52it/s, Materializing param=roberta.encoder.layer.11.attention.output.LayerNorm.weight]Loading weights:  93%|█████████▎| 183/197 [00:00<00:00, 3770.65it/s, Materializing param=roberta.encoder.layer.11.attention.output.LayerNorm.weight]Loading weights:  93%|█████████▎| 184/197 [00:00<00:00, 3788.09it/s, Materializing param=roberta.encoder.layer.11.attention.output.dense.bias]      Loading weights:  93%|█████████▎| 184/197 [00:00<00:00, 3786.40it/s, Materializing param=roberta.encoder.layer.11.attention.output.dense.bias]Loading weights:  94%|█████████▍| 185/197 [00:00<00:00, 3803.79it/s, Materializing param=roberta.encoder.layer.11.attention.output.dense.weight]Loading weights:  94%|█████████▍| 185/197 [00:00<00:00, 3802.11it/s, Materializing param=roberta.encoder.layer.11.attention.output.dense.weight]Loading weights:  94%|█████████▍| 186/197 [00:00<00:00, 3819.56it/s, Materializing param=roberta.encoder.layer.11.attention.self.key.bias]      Loading weights:  94%|█████████▍| 186/197 [00:00<00:00, 3817.86it/s, Materializing param=roberta.encoder.layer.11.attention.self.key.bias]Loading weights:  95%|█████████▍| 187/197 [00:00<00:00, 3835.19it/s, Materializing param=roberta.encoder.layer.11.attention.self.key.weight]Loading weights:  95%|█████████▍| 187/197 [00:00<00:00, 3833.35it/s, Materializing param=roberta.encoder.layer.11.attention.self.key.weight]Loading weights:  95%|█████████▌| 188/197 [00:00<00:00, 3850.63it/s, Materializing param=roberta.encoder.layer.11.attention.self.query.bias]Loading weights:  95%|█████████▌| 188/197 [00:00<00:00, 3848.79it/s, Materializing param=roberta.encoder.layer.11.attention.self.query.bias]Loading weights:  96%|█████████▌| 189/197 [00:00<00:00, 3866.10it/s, Materializing param=roberta.encoder.layer.11.attention.self.query.weight]Loading weights:  96%|█████████▌| 189/197 [00:00<00:00, 3864.32it/s, Materializing param=roberta.encoder.layer.11.attention.self.query.weight]Loading weights:  96%|█████████▋| 190/197 [00:00<00:00, 3881.67it/s, Materializing param=roberta.encoder.layer.11.attention.self.value.bias]  Loading weights:  96%|█████████▋| 190/197 [00:00<00:00, 3879.83it/s, Materializing param=roberta.encoder.layer.11.attention.self.value.bias]Loading weights:  97%|█████████▋| 191/197 [00:00<00:00, 3896.90it/s, Materializing param=roberta.encoder.layer.11.attention.self.value.weight]Loading weights:  97%|█████████▋| 191/197 [00:00<00:00, 3895.04it/s, Materializing param=roberta.encoder.layer.11.attention.self.value.weight]Loading weights:  97%|█████████▋| 192/197 [00:00<00:00, 3912.25it/s, Materializing param=roberta.encoder.layer.11.intermediate.dense.bias]    Loading weights:  97%|█████████▋| 192/197 [00:00<00:00, 3910.39it/s, Materializing param=roberta.encoder.layer.11.intermediate.dense.bias]Loading weights:  98%|█████████▊| 193/197 [00:00<00:00, 3927.29it/s, Materializing param=roberta.encoder.layer.11.intermediate.dense.weight]Loading weights:  98%|█████████▊| 193/197 [00:00<00:00, 3925.46it/s, Materializing param=roberta.encoder.layer.11.intermediate.dense.weight]Loading weights:  98%|█████████▊| 194/197 [00:00<00:00, 3942.70it/s, Materializing param=roberta.encoder.layer.11.output.LayerNorm.bias]    Loading weights:  98%|█████████▊| 194/197 [00:00<00:00, 3940.83it/s, Materializing param=roberta.encoder.layer.11.output.LayerNorm.bias]Loading weights:  99%|█████████▉| 195/197 [00:00<00:00, 3957.98it/s, Materializing param=roberta.encoder.layer.11.output.LayerNorm.weight]Loading weights:  99%|█████████▉| 195/197 [00:00<00:00, 3955.68it/s, Materializing param=roberta.encoder.layer.11.output.LayerNorm.weight]Loading weights:  99%|█████████▉| 196/197 [00:00<00:00, 3972.61it/s, Materializing param=roberta.encoder.layer.11.output.dense.bias]      Loading weights:  99%|█████████▉| 196/197 [00:00<00:00, 3970.80it/s, Materializing param=roberta.encoder.layer.11.output.dense.bias]Loading weights: 100%|██████████| 197/197 [00:00<00:00, 3987.81it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]Loading weights: 100%|██████████| 197/197 [00:00<00:00, 3985.85it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]Loading weights: 100%|██████████| 197/197 [00:00<00:00, 3980.47it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]
XLMRobertaForSequenceClassification LOAD REPORT from: FacebookAI/xlm-roberta-base
Key                         | Status     | 
----------------------------+------------+-
lm_head.dense.bias          | UNEXPECTED | 
roberta.pooler.dense.weight | UNEXPECTED | 
lm_head.layer_norm.weight   | UNEXPECTED | 
lm_head.bias                | UNEXPECTED | 
lm_head.layer_norm.bias     | UNEXPECTED | 
roberta.pooler.dense.bias   | UNEXPECTED | 
lm_head.dense.weight        | UNEXPECTED | 
classifier.out_proj.weight  | MISSING    | 
classifier.out_proj.bias    | MISSING    | 
classifier.dense.weight     | MISSING    | 
classifier.dense.bias       | MISSING    | 

Notes:
- UNEXPECTED	:can be ignored when loading from different task/architecture; not ok if you expect identical arch.
- MISSING	:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.
2026-01-27 18:36:09,630 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/FacebookAI/xlm-roberta-base/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-01-27 18:36:09,870 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/FacebookAI/xlm-roberta-base/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
2026-01-27 18:36:11,269 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/FacebookAI/xlm-roberta-base "HTTP/1.1 200 OK"
2026-01-27 18:36:11,284 - sentiment_analysis.model_loader - INFO - Loaded model FacebookAI/xlm-roberta-base in 2365.46ms on cpu
2026-01-27 18:36:11,284 - sentiment_analysis.model_loader - INFO - [3/4] Loading ai4bharat/indic-bert...
2026-01-27 18:36:11,576 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/ai4bharat/indic-bert/resolve/main/config.json "HTTP/1.1 200 OK"
2026-01-27 18:36:11,883 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/ai4bharat/indic-bert/resolve/main/model.safetensors "HTTP/1.1 404 Not Found"
Loading weights:   0%|          | 0/25 [00:00<?, ?it/s]Loading weights:   4%|▍         | 1/25 [00:00<00:00, 68759.08it/s, Materializing param=albert.embeddings.LayerNorm.bias]Loading weights:   4%|▍         | 1/25 [00:00<00:00, 15887.52it/s, Materializing param=albert.embeddings.LayerNorm.bias]Loading weights:   8%|▊         | 2/25 [00:00<00:00, 13148.29it/s, Materializing param=albert.embeddings.LayerNorm.weight]Loading weights:   8%|▊         | 2/25 [00:00<00:00, 10852.02it/s, Materializing param=albert.embeddings.LayerNorm.weight]Loading weights:  12%|█▏        | 3/25 [00:00<00:00, 12972.07it/s, Materializing param=albert.embeddings.position_embeddings.weight]Loading weights:  12%|█▏        | 3/25 [00:00<00:00, 11491.24it/s, Materializing param=albert.embeddings.position_embeddings.weight]Loading weights:  16%|█▌        | 4/25 [00:00<00:00, 13056.20it/s, Materializing param=albert.embeddings.token_type_embeddings.weight]Loading weights:  16%|█▌        | 4/25 [00:00<00:00, 12139.81it/s, Materializing param=albert.embeddings.token_type_embeddings.weight]Loading weights:  20%|██        | 5/25 [00:00<00:00, 11422.40it/s, Materializing param=albert.embeddings.word_embeddings.weight]      Loading weights:  20%|██        | 5/25 [00:00<00:00, 10826.80it/s, Materializing param=albert.embeddings.word_embeddings.weight]Loading weights:  24%|██▍       | 6/25 [00:00<00:00, 11943.91it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias]Loading weights:  24%|██▍       | 6/25 [00:00<00:00, 11356.42it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.bias]Loading weights:  28%|██▊       | 7/25 [00:00<00:00, 12092.31it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight]Loading weights:  28%|██▊       | 7/25 [00:00<00:00, 11613.97it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.attention.LayerNorm.weight]Loading weights:  32%|███▏      | 8/25 [00:00<00:00, 12377.14it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias]      Loading weights:  32%|███▏      | 8/25 [00:00<00:00, 11919.87it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.bias]Loading weights:  36%|███▌      | 9/25 [00:00<00:00, 12570.34it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight]Loading weights:  36%|███▌      | 9/25 [00:00<00:00, 12165.24it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.attention.dense.weight]Loading weights:  40%|████      | 10/25 [00:00<00:00, 12764.16it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias]   Loading weights:  40%|████      | 10/25 [00:00<00:00, 12390.85it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.bias]Loading weights:  44%|████▍     | 11/25 [00:00<00:00, 12815.93it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight]Loading weights:  44%|████▍     | 11/25 [00:00<00:00, 12439.29it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.attention.key.weight]Loading weights:  48%|████▊     | 12/25 [00:00<00:00, 12908.86it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias]Loading weights:  48%|████▊     | 12/25 [00:00<00:00, 12576.62it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.bias]Loading weights:  52%|█████▏    | 13/25 [00:00<00:00, 13010.25it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight]Loading weights:  52%|█████▏    | 13/25 [00:00<00:00, 12707.05it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.attention.query.weight]Loading weights:  56%|█████▌    | 14/25 [00:00<00:00, 13121.84it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias]  Loading weights:  56%|█████▌    | 14/25 [00:00<00:00, 12849.07it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.bias]Loading weights:  60%|██████    | 15/25 [00:00<00:00, 13242.38it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight]Loading weights:  60%|██████    | 15/25 [00:00<00:00, 12969.40it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.attention.value.weight]Loading weights:  64%|██████▍   | 16/25 [00:00<00:00, 13336.42it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias]              Loading weights:  64%|██████▍   | 16/25 [00:00<00:00, 13079.10it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.bias]Loading weights:  68%|██████▊   | 17/25 [00:00<00:00, 13430.62it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.weight]Loading weights:  68%|██████▊   | 17/25 [00:00<00:00, 13196.96it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.ffn.weight]Loading weights:  72%|███████▏  | 18/25 [00:00<00:00, 13534.86it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias]Loading weights:  72%|███████▏  | 18/25 [00:00<00:00, 13303.52it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.bias]Loading weights:  76%|███████▌  | 19/25 [00:00<00:00, 13615.54it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight]Loading weights:  76%|███████▌  | 19/25 [00:00<00:00, 13386.83it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.ffn_output.weight]Loading weights:  80%|████████  | 20/25 [00:00<00:00, 13684.52it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias]Loading weights:  80%|████████  | 20/25 [00:00<00:00, 13464.86it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.bias]Loading weights:  84%|████████▍ | 21/25 [00:00<00:00, 13741.09it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight]Loading weights:  84%|████████▍ | 21/25 [00:00<00:00, 13534.17it/s, Materializing param=albert.encoder.albert_layer_groups.0.albert_layers.0.full_layer_layer_norm.weight]Loading weights:  88%|████████▊ | 22/25 [00:00<00:00, 12892.93it/s, Materializing param=albert.encoder.embedding_hidden_mapping_in.bias]                                  Loading weights:  88%|████████▊ | 22/25 [00:00<00:00, 12717.02it/s, Materializing param=albert.encoder.embedding_hidden_mapping_in.bias]Loading weights:  92%|█████████▏| 23/25 [00:00<00:00, 13013.49it/s, Materializing param=albert.encoder.embedding_hidden_mapping_in.weight]Loading weights:  92%|█████████▏| 23/25 [00:00<00:00, 12852.25it/s, Materializing param=albert.encoder.embedding_hidden_mapping_in.weight]Loading weights:  96%|█████████▌| 24/25 [00:00<00:00, 12957.05it/s, Materializing param=albert.pooler.bias]                               Loading weights:  96%|█████████▌| 24/25 [00:00<00:00, 12679.59it/s, Materializing param=albert.pooler.bias]Loading weights: 100%|██████████| 25/25 [00:00<00:00, 12795.31it/s, Materializing param=albert.pooler.weight]Loading weights: 100%|██████████| 25/25 [00:00<00:00, 12641.06it/s, Materializing param=albert.pooler.weight]Loading weights: 100%|██████████| 25/25 [00:00<00:00, 12330.39it/s, Materializing param=albert.pooler.weight]
AlbertForSequenceClassification LOAD REPORT from: ai4bharat/indic-bert
Key                              | Status     | 
---------------------------------+------------+-
predictions.decoder.bias         | UNEXPECTED | 
predictions.dense.weight         | UNEXPECTED | 
predictions.LayerNorm.weight     | UNEXPECTED | 
predictions.LayerNorm.bias       | UNEXPECTED | 
predictions.decoder.weight       | UNEXPECTED | 
sop_classifier.classifier.bias   | UNEXPECTED | 
predictions.dense.bias           | UNEXPECTED | 
sop_classifier.classifier.weight | UNEXPECTED | 
predictions.bias                 | UNEXPECTED | 
classifier.bias                  | MISSING    | 
classifier.weight                | MISSING    | 

Notes:
- UNEXPECTED	:can be ignored when loading from different task/architecture; not ok if you expect identical arch.
- MISSING	:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.
2026-01-27 18:36:12,123 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/ai4bharat/indic-bert "HTTP/1.1 200 OK"
2026-01-27 18:36:12,258 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/ai4bharat/indic-bert/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-01-27 18:36:12,497 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/ai4bharat/indic-bert/commits/main "HTTP/1.1 200 OK"
2026-01-27 18:36:12,505 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/ai4bharat/indic-bert/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
Could not extract SentencePiece model from /home/saipraneethmeduri/.cache/huggingface/hub/models--ai4bharat--indic-bert/snapshots/4842dd258ecc0546f0d660b76a3b22a9c632f401/spiece.model using sentencepiece library due to 
SentencePieceExtractor requires the SentencePiece library but it was not found in your environment. Check out the instructions on the
installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.
. Falling back to TikToken extractor.
2026-01-27 18:36:12,507 - sentiment_analysis.model_loader - ERROR - Failed to load model ai4bharat/indic-bert: `tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.
2026-01-27 18:36:12,514 - sentiment_analysis.model_loader - WARNING - Skipping failed model: ai4bharat/indic-bert
2026-01-27 18:36:12,515 - sentiment_analysis.model_loader - INFO - [4/4] Loading cardiffnlp/twitter-xlm-roberta-base-sentiment...
2026-01-27 18:36:12,805 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/ai4bharat/indic-bert/discussions?p=0 "HTTP/1.1 200 OK"
2026-01-27 18:36:12,807 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/config.json "HTTP/1.1 307 Temporary Redirect"
2026-01-27 18:36:12,821 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/cardiffnlp/twitter-xlm-roberta-base-sentiment/f2f1202b1bdeb07342385c3f807f9c07cd8f5cf8/config.json "HTTP/1.1 200 OK"
2026-01-27 18:36:13,112 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/main/model.safetensors "HTTP/1.1 404 Not Found"
2026-01-27 18:36:13,114 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/ai4bharat/indic-bert/commits/refs%2Fpr%2F8 "HTTP/1.1 200 OK"
2026-01-27 18:36:13,418 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/ai4bharat/indic-bert/resolve/refs%2Fpr%2F8/model.safetensors.index.json "HTTP/1.1 404 Not Found"
2026-01-27 18:36:13,419 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/cardiffnlp/twitter-xlm-roberta-base-sentiment "HTTP/1.1 200 OK"
Loading weights:   0%|          | 0/201 [00:00<?, ?it/s]Loading weights:   0%|          | 1/201 [00:00<00:00, 38479.85it/s, Materializing param=classifier.dense.bias]Loading weights:   0%|          | 1/201 [00:00<00:00, 499.02it/s, Materializing param=classifier.dense.bias]  Loading weights:   1%|          | 2/201 [00:00<00:00, 916.69it/s, Materializing param=classifier.dense.weight]Loading weights:   1%|          | 2/201 [00:00<00:00, 888.44it/s, Materializing param=classifier.dense.weight]Loading weights:   1%|▏         | 3/201 [00:00<00:00, 1271.90it/s, Materializing param=classifier.out_proj.bias]Loading weights:   1%|▏         | 3/201 [00:00<00:00, 1245.22it/s, Materializing param=classifier.out_proj.bias]Loading weights:   2%|▏         | 4/201 [00:00<00:00, 1598.90it/s, Materializing param=classifier.out_proj.weight]Loading weights:   2%|▏         | 4/201 [00:00<00:00, 1572.08it/s, Materializing param=classifier.out_proj.weight]Loading weights:   2%|▏         | 5/201 [00:00<00:00, 1904.42it/s, Materializing param=roberta.embeddings.LayerNorm.bias]Loading weights:   2%|▏         | 5/201 [00:00<00:00, 1874.97it/s, Materializing param=roberta.embeddings.LayerNorm.bias]Loading weights:   3%|▎         | 6/201 [00:00<00:00, 2185.67it/s, Materializing param=roberta.embeddings.LayerNorm.weight]Loading weights:   3%|▎         | 6/201 [00:00<00:00, 2153.69it/s, Materializing param=roberta.embeddings.LayerNorm.weight]Loading weights:   3%|▎         | 7/201 [00:00<00:00, 2447.70it/s, Materializing param=roberta.embeddings.position_embeddings.weight]Loading weights:   3%|▎         | 7/201 [00:00<00:00, 2411.91it/s, Materializing param=roberta.embeddings.position_embeddings.weight]Loading weights:   4%|▍         | 8/201 [00:00<00:00, 2686.50it/s, Materializing param=roberta.embeddings.token_type_embeddings.weight]Loading weights:   4%|▍         | 8/201 [00:00<00:00, 2650.01it/s, Materializing param=roberta.embeddings.token_type_embeddings.weight]Loading weights:   4%|▍         | 9/201 [00:00<00:00, 2910.02it/s, Materializing param=roberta.embeddings.word_embeddings.weight]      Loading weights:   4%|▍         | 9/201 [00:00<00:00, 2872.16it/s, Materializing param=roberta.embeddings.word_embeddings.weight]Loading weights:   5%|▍         | 10/201 [00:00<00:00, 3117.75it/s, Materializing param=roberta.encoder.layer.0.attention.output.LayerNorm.bias]Loading weights:   5%|▍         | 10/201 [00:00<00:00, 3070.95it/s, Materializing param=roberta.encoder.layer.0.attention.output.LayerNorm.bias]Loading weights:   5%|▌         | 11/201 [00:00<00:00, 3291.53it/s, Materializing param=roberta.encoder.layer.0.attention.output.LayerNorm.weight]Loading weights:   5%|▌         | 11/201 [00:00<00:00, 3250.25it/s, Materializing param=roberta.encoder.layer.0.attention.output.LayerNorm.weight]Loading weights:   6%|▌         | 12/201 [00:00<00:00, 3465.41it/s, Materializing param=roberta.encoder.layer.0.attention.output.dense.bias]      Loading weights:   6%|▌         | 12/201 [00:00<00:00, 3424.62it/s, Materializing param=roberta.encoder.layer.0.attention.output.dense.bias]Loading weights:   6%|▋         | 13/201 [00:00<00:00, 3630.22it/s, Materializing param=roberta.encoder.layer.0.attention.output.dense.weight]Loading weights:   6%|▋         | 13/201 [00:00<00:00, 3588.41it/s, Materializing param=roberta.encoder.layer.0.attention.output.dense.weight]Loading weights:   7%|▋         | 14/201 [00:00<00:00, 3784.50it/s, Materializing param=roberta.encoder.layer.0.attention.self.key.bias]      Loading weights:   7%|▋         | 14/201 [00:00<00:00, 3743.00it/s, Materializing param=roberta.encoder.layer.0.attention.self.key.bias]Loading weights:   7%|▋         | 15/201 [00:00<00:00, 3928.23it/s, Materializing param=roberta.encoder.layer.0.attention.self.key.weight]Loading weights:   7%|▋         | 15/201 [00:00<00:00, 3886.73it/s, Materializing param=roberta.encoder.layer.0.attention.self.key.weight]Loading weights:   8%|▊         | 16/201 [00:00<00:00, 4065.73it/s, Materializing param=roberta.encoder.layer.0.attention.self.query.bias]Loading weights:   8%|▊         | 16/201 [00:00<00:00, 4024.28it/s, Materializing param=roberta.encoder.layer.0.attention.self.query.bias]Loading weights:   8%|▊         | 17/201 [00:00<00:00, 4195.78it/s, Materializing param=roberta.encoder.layer.0.attention.self.query.weight]Loading weights:   8%|▊         | 17/201 [00:00<00:00, 4154.23it/s, Materializing param=roberta.encoder.layer.0.attention.self.query.weight]Loading weights:   9%|▉         | 18/201 [00:00<00:00, 4317.35it/s, Materializing param=roberta.encoder.layer.0.attention.self.value.bias]  Loading weights:   9%|▉         | 18/201 [00:00<00:00, 4275.54it/s, Materializing param=roberta.encoder.layer.0.attention.self.value.bias]Loading weights:   9%|▉         | 19/201 [00:00<00:00, 4422.41it/s, Materializing param=roberta.encoder.layer.0.attention.self.value.weight]Loading weights:   9%|▉         | 19/201 [00:00<00:00, 4379.39it/s, Materializing param=roberta.encoder.layer.0.attention.self.value.weight]Loading weights:  10%|▉         | 20/201 [00:00<00:00, 4524.11it/s, Materializing param=roberta.encoder.layer.0.intermediate.dense.bias]    Loading weights:  10%|▉         | 20/201 [00:00<00:00, 4477.75it/s, Materializing param=roberta.encoder.layer.0.intermediate.dense.bias]Loading weights:  10%|█         | 21/201 [00:00<00:00, 4590.39it/s, Materializing param=roberta.encoder.layer.0.intermediate.dense.weight]Loading weights:  10%|█         | 21/201 [00:00<00:00, 4536.48it/s, Materializing param=roberta.encoder.layer.0.intermediate.dense.weight]Loading weights:  11%|█         | 22/201 [00:00<00:00, 4637.15it/s, Materializing param=roberta.encoder.layer.0.output.LayerNorm.bias]    Loading weights:  11%|█         | 22/201 [00:00<00:00, 4575.53it/s, Materializing param=roberta.encoder.layer.0.output.LayerNorm.bias]Loading weights:  11%|█▏        | 23/201 [00:00<00:00, 4675.92it/s, Materializing param=roberta.encoder.layer.0.output.LayerNorm.weight]Loading weights:  11%|█▏        | 23/201 [00:00<00:00, 4626.14it/s, Materializing param=roberta.encoder.layer.0.output.LayerNorm.weight]Loading weights:  12%|█▏        | 24/201 [00:00<00:00, 4739.55it/s, Materializing param=roberta.encoder.layer.0.output.dense.bias]      Loading weights:  12%|█▏        | 24/201 [00:00<00:00, 4697.53it/s, Materializing param=roberta.encoder.layer.0.output.dense.bias]Loading weights:  12%|█▏        | 25/201 [00:00<00:00, 4809.10it/s, Materializing param=roberta.encoder.layer.0.output.dense.weight]Loading weights:  12%|█▏        | 25/201 [00:00<00:00, 4758.25it/s, Materializing param=roberta.encoder.layer.0.output.dense.weight]Loading weights:  13%|█▎        | 26/201 [00:00<00:00, 4864.91it/s, Materializing param=roberta.encoder.layer.1.attention.output.LayerNorm.bias]Loading weights:  13%|█▎        | 26/201 [00:00<00:00, 4824.88it/s, Materializing param=roberta.encoder.layer.1.attention.output.LayerNorm.bias]Loading weights:  13%|█▎        | 27/201 [00:00<00:00, 4927.39it/s, Materializing param=roberta.encoder.layer.1.attention.output.LayerNorm.weight]Loading weights:  13%|█▎        | 27/201 [00:00<00:00, 4881.93it/s, Materializing param=roberta.encoder.layer.1.attention.output.LayerNorm.weight]Loading weights:  14%|█▍        | 28/201 [00:00<00:00, 4951.33it/s, Materializing param=roberta.encoder.layer.1.attention.output.dense.bias]      Loading weights:  14%|█▍        | 28/201 [00:00<00:00, 4896.01it/s, Materializing param=roberta.encoder.layer.1.attention.output.dense.bias]Loading weights:  14%|█▍        | 29/201 [00:00<00:00, 4965.50it/s, Materializing param=roberta.encoder.layer.1.attention.output.dense.weight]Loading weights:  14%|█▍        | 29/201 [00:00<00:00, 4915.73it/s, Materializing param=roberta.encoder.layer.1.attention.output.dense.weight]Loading weights:  15%|█▍        | 30/201 [00:00<00:00, 4993.22it/s, Materializing param=roberta.encoder.layer.1.attention.self.key.bias]      Loading weights:  15%|█▍        | 30/201 [00:00<00:00, 4943.39it/s, Materializing param=roberta.encoder.layer.1.attention.self.key.bias]Loading weights:  15%|█▌        | 31/201 [00:00<00:00, 5015.56it/s, Materializing param=roberta.encoder.layer.1.attention.self.key.weight]Loading weights:  15%|█▌        | 31/201 [00:00<00:00, 4962.35it/s, Materializing param=roberta.encoder.layer.1.attention.self.key.weight]Loading weights:  16%|█▌        | 32/201 [00:00<00:00, 5035.56it/s, Materializing param=roberta.encoder.layer.1.attention.self.query.bias]Loading weights:  16%|█▌        | 32/201 [00:00<00:00, 4990.62it/s, Materializing param=roberta.encoder.layer.1.attention.self.query.bias]Loading weights:  16%|█▋        | 33/201 [00:00<00:00, 5063.55it/s, Materializing param=roberta.encoder.layer.1.attention.self.query.weight]Loading weights:  16%|█▋        | 33/201 [00:00<00:00, 5029.32it/s, Materializing param=roberta.encoder.layer.1.attention.self.query.weight]Loading weights:  17%|█▋        | 34/201 [00:00<00:00, 5123.83it/s, Materializing param=roberta.encoder.layer.1.attention.self.value.bias]  Loading weights:  17%|█▋        | 34/201 [00:00<00:00, 5093.08it/s, Materializing param=roberta.encoder.layer.1.attention.self.value.bias]Loading weights:  17%|█▋        | 35/201 [00:00<00:00, 5184.92it/s, Materializing param=roberta.encoder.layer.1.attention.self.value.weight]Loading weights:  17%|█▋        | 35/201 [00:00<00:00, 5154.34it/s, Materializing param=roberta.encoder.layer.1.attention.self.value.weight]Loading weights:  18%|█▊        | 36/201 [00:00<00:00, 5243.61it/s, Materializing param=roberta.encoder.layer.1.intermediate.dense.bias]    Loading weights:  18%|█▊        | 36/201 [00:00<00:00, 5212.83it/s, Materializing param=roberta.encoder.layer.1.intermediate.dense.bias]Loading weights:  18%|█▊        | 37/201 [00:00<00:00, 5300.54it/s, Materializing param=roberta.encoder.layer.1.intermediate.dense.weight]Loading weights:  18%|█▊        | 37/201 [00:00<00:00, 5270.12it/s, Materializing param=roberta.encoder.layer.1.intermediate.dense.weight]Loading weights:  19%|█▉        | 38/201 [00:00<00:00, 5355.99it/s, Materializing param=roberta.encoder.layer.1.output.LayerNorm.bias]    Loading weights:  19%|█▉        | 38/201 [00:00<00:00, 5325.74it/s, Materializing param=roberta.encoder.layer.1.output.LayerNorm.bias]Loading weights:  19%|█▉        | 39/201 [00:00<00:00, 5408.96it/s, Materializing param=roberta.encoder.layer.1.output.LayerNorm.weight]Loading weights:  19%|█▉        | 39/201 [00:00<00:00, 5348.65it/s, Materializing param=roberta.encoder.layer.1.output.LayerNorm.weight]Loading weights:  20%|█▉        | 40/201 [00:00<00:00, 5399.12it/s, Materializing param=roberta.encoder.layer.1.output.dense.bias]      Loading weights:  20%|█▉        | 40/201 [00:00<00:00, 5374.90it/s, Materializing param=roberta.encoder.layer.1.output.dense.bias]Loading weights:  20%|██        | 41/201 [00:00<00:00, 5468.45it/s, Materializing param=roberta.encoder.layer.1.output.dense.weight]Loading weights:  20%|██        | 41/201 [00:00<00:00, 5449.74it/s, Materializing param=roberta.encoder.layer.1.output.dense.weight]Loading weights:  21%|██        | 42/201 [00:00<00:00, 5547.15it/s, Materializing param=roberta.encoder.layer.2.attention.output.LayerNorm.bias]Loading weights:  21%|██        | 42/201 [00:00<00:00, 5529.39it/s, Materializing param=roberta.encoder.layer.2.attention.output.LayerNorm.bias]Loading weights:  21%|██▏       | 43/201 [00:00<00:00, 5624.32it/s, Materializing param=roberta.encoder.layer.2.attention.output.LayerNorm.weight]Loading weights:  21%|██▏       | 43/201 [00:00<00:00, 5606.84it/s, Materializing param=roberta.encoder.layer.2.attention.output.LayerNorm.weight]Loading weights:  22%|██▏       | 44/201 [00:00<00:00, 5703.36it/s, Materializing param=roberta.encoder.layer.2.attention.output.dense.bias]      Loading weights:  22%|██▏       | 44/201 [00:00<00:00, 5686.84it/s, Materializing param=roberta.encoder.layer.2.attention.output.dense.bias]Loading weights:  22%|██▏       | 45/201 [00:00<00:00, 5783.47it/s, Materializing param=roberta.encoder.layer.2.attention.output.dense.weight]Loading weights:  22%|██▏       | 45/201 [00:00<00:00, 5767.39it/s, Materializing param=roberta.encoder.layer.2.attention.output.dense.weight]Loading weights:  23%|██▎       | 46/201 [00:00<00:00, 5862.95it/s, Materializing param=roberta.encoder.layer.2.attention.self.key.bias]      Loading weights:  23%|██▎       | 46/201 [00:00<00:00, 5846.78it/s, Materializing param=roberta.encoder.layer.2.attention.self.key.bias]Loading weights:  23%|██▎       | 47/201 [00:00<00:00, 5941.30it/s, Materializing param=roberta.encoder.layer.2.attention.self.key.weight]Loading weights:  23%|██▎       | 47/201 [00:00<00:00, 5924.69it/s, Materializing param=roberta.encoder.layer.2.attention.self.key.weight]Loading weights:  24%|██▍       | 48/201 [00:00<00:00, 6018.73it/s, Materializing param=roberta.encoder.layer.2.attention.self.query.bias]Loading weights:  24%|██▍       | 48/201 [00:00<00:00, 6002.40it/s, Materializing param=roberta.encoder.layer.2.attention.self.query.bias]Loading weights:  24%|██▍       | 49/201 [00:00<00:00, 6093.84it/s, Materializing param=roberta.encoder.layer.2.attention.self.query.weight]Loading weights:  24%|██▍       | 49/201 [00:00<00:00, 6077.44it/s, Materializing param=roberta.encoder.layer.2.attention.self.query.weight]Loading weights:  25%|██▍       | 50/201 [00:00<00:00, 6168.82it/s, Materializing param=roberta.encoder.layer.2.attention.self.value.bias]  Loading weights:  25%|██▍       | 50/201 [00:00<00:00, 6151.63it/s, Materializing param=roberta.encoder.layer.2.attention.self.value.bias]Loading weights:  25%|██▌       | 51/201 [00:00<00:00, 6241.52it/s, Materializing param=roberta.encoder.layer.2.attention.self.value.weight]Loading weights:  25%|██▌       | 51/201 [00:00<00:00, 6224.63it/s, Materializing param=roberta.encoder.layer.2.attention.self.value.weight]Loading weights:  26%|██▌       | 52/201 [00:00<00:00, 6309.60it/s, Materializing param=roberta.encoder.layer.2.intermediate.dense.bias]    Loading weights:  26%|██▌       | 52/201 [00:00<00:00, 6292.12it/s, Materializing param=roberta.encoder.layer.2.intermediate.dense.bias]Loading weights:  26%|██▋       | 53/201 [00:00<00:00, 6378.35it/s, Materializing param=roberta.encoder.layer.2.intermediate.dense.weight]Loading weights:  26%|██▋       | 53/201 [00:00<00:00, 6360.82it/s, Materializing param=roberta.encoder.layer.2.intermediate.dense.weight]Loading weights:  27%|██▋       | 54/201 [00:00<00:00, 6448.18it/s, Materializing param=roberta.encoder.layer.2.output.LayerNorm.bias]    Loading weights:  27%|██▋       | 54/201 [00:00<00:00, 6431.52it/s, Materializing param=roberta.encoder.layer.2.output.LayerNorm.bias]Loading weights:  27%|██▋       | 55/201 [00:00<00:00, 6517.31it/s, Materializing param=roberta.encoder.layer.2.output.LayerNorm.weight]Loading weights:  27%|██▋       | 55/201 [00:00<00:00, 6500.60it/s, Materializing param=roberta.encoder.layer.2.output.LayerNorm.weight]Loading weights:  28%|██▊       | 56/201 [00:00<00:00, 6581.51it/s, Materializing param=roberta.encoder.layer.2.output.dense.bias]      Loading weights:  28%|██▊       | 56/201 [00:00<00:00, 6564.77it/s, Materializing param=roberta.encoder.layer.2.output.dense.bias]Loading weights:  28%|██▊       | 57/201 [00:00<00:00, 6648.37it/s, Materializing param=roberta.encoder.layer.2.output.dense.weight]Loading weights:  28%|██▊       | 57/201 [00:00<00:00, 6631.22it/s, Materializing param=roberta.encoder.layer.2.output.dense.weight]Loading weights:  29%|██▉       | 58/201 [00:00<00:00, 6714.78it/s, Materializing param=roberta.encoder.layer.3.attention.output.LayerNorm.bias]Loading weights:  29%|██▉       | 58/201 [00:00<00:00, 6697.95it/s, Materializing param=roberta.encoder.layer.3.attention.output.LayerNorm.bias]Loading weights:  29%|██▉       | 59/201 [00:00<00:00, 6777.23it/s, Materializing param=roberta.encoder.layer.3.attention.output.LayerNorm.weight]Loading weights:  29%|██▉       | 59/201 [00:00<00:00, 6760.94it/s, Materializing param=roberta.encoder.layer.3.attention.output.LayerNorm.weight]Loading weights:  30%|██▉       | 60/201 [00:00<00:00, 6841.89it/s, Materializing param=roberta.encoder.layer.3.attention.output.dense.bias]      Loading weights:  30%|██▉       | 60/201 [00:00<00:00, 6825.56it/s, Materializing param=roberta.encoder.layer.3.attention.output.dense.bias]Loading weights:  30%|███       | 61/201 [00:00<00:00, 6904.86it/s, Materializing param=roberta.encoder.layer.3.attention.output.dense.weight]Loading weights:  30%|███       | 61/201 [00:00<00:00, 6885.35it/s, Materializing param=roberta.encoder.layer.3.attention.output.dense.weight]Loading weights:  31%|███       | 62/201 [00:00<00:00, 6962.81it/s, Materializing param=roberta.encoder.layer.3.attention.self.key.bias]      Loading weights:  31%|███       | 62/201 [00:00<00:00, 6944.77it/s, Materializing param=roberta.encoder.layer.3.attention.self.key.bias]Loading weights:  31%|███▏      | 63/201 [00:00<00:00, 7022.65it/s, Materializing param=roberta.encoder.layer.3.attention.self.key.weight]Loading weights:  31%|███▏      | 63/201 [00:00<00:00, 7004.59it/s, Materializing param=roberta.encoder.layer.3.attention.self.key.weight]Loading weights:  32%|███▏      | 64/201 [00:00<00:00, 7081.61it/s, Materializing param=roberta.encoder.layer.3.attention.self.query.bias]Loading weights:  32%|███▏      | 64/201 [00:00<00:00, 7064.09it/s, Materializing param=roberta.encoder.layer.3.attention.self.query.bias]Loading weights:  32%|███▏      | 65/201 [00:00<00:00, 7140.83it/s, Materializing param=roberta.encoder.layer.3.attention.self.query.weight]Loading weights:  32%|███▏      | 65/201 [00:00<00:00, 7123.66it/s, Materializing param=roberta.encoder.layer.3.attention.self.query.weight]Loading weights:  33%|███▎      | 66/201 [00:00<00:00, 7199.96it/s, Materializing param=roberta.encoder.layer.3.attention.self.value.bias]  Loading weights:  33%|███▎      | 66/201 [00:00<00:00, 7182.21it/s, Materializing param=roberta.encoder.layer.3.attention.self.value.bias]Loading weights:  33%|███▎      | 67/201 [00:00<00:00, 7253.77it/s, Materializing param=roberta.encoder.layer.3.attention.self.value.weight]Loading weights:  33%|███▎      | 67/201 [00:00<00:00, 7235.84it/s, Materializing param=roberta.encoder.layer.3.attention.self.value.weight]Loading weights:  34%|███▍      | 68/201 [00:00<00:00, 7308.84it/s, Materializing param=roberta.encoder.layer.3.intermediate.dense.bias]    Loading weights:  34%|███▍      | 68/201 [00:00<00:00, 7290.34it/s, Materializing param=roberta.encoder.layer.3.intermediate.dense.bias]Loading weights:  34%|███▍      | 69/201 [00:00<00:00, 7362.55it/s, Materializing param=roberta.encoder.layer.3.intermediate.dense.weight]Loading weights:  34%|███▍      | 69/201 [00:00<00:00, 7344.61it/s, Materializing param=roberta.encoder.layer.3.intermediate.dense.weight]Loading weights:  35%|███▍      | 70/201 [00:00<00:00, 7418.48it/s, Materializing param=roberta.encoder.layer.3.output.LayerNorm.bias]    Loading weights:  35%|███▍      | 70/201 [00:00<00:00, 7400.53it/s, Materializing param=roberta.encoder.layer.3.output.LayerNorm.bias]Loading weights:  35%|███▌      | 71/201 [00:00<00:00, 7471.98it/s, Materializing param=roberta.encoder.layer.3.output.LayerNorm.weight]Loading weights:  35%|███▌      | 71/201 [00:00<00:00, 7454.58it/s, Materializing param=roberta.encoder.layer.3.output.LayerNorm.weight]Loading weights:  36%|███▌      | 72/201 [00:00<00:00, 7526.98it/s, Materializing param=roberta.encoder.layer.3.output.dense.bias]      Loading weights:  36%|███▌      | 72/201 [00:00<00:00, 7508.82it/s, Materializing param=roberta.encoder.layer.3.output.dense.bias]Loading weights:  36%|███▋      | 73/201 [00:00<00:00, 7579.75it/s, Materializing param=roberta.encoder.layer.3.output.dense.weight]Loading weights:  36%|███▋      | 73/201 [00:00<00:00, 7561.97it/s, Materializing param=roberta.encoder.layer.3.output.dense.weight]Loading weights:  37%|███▋      | 74/201 [00:00<00:00, 7631.07it/s, Materializing param=roberta.encoder.layer.4.attention.output.LayerNorm.bias]Loading weights:  37%|███▋      | 74/201 [00:00<00:00, 7612.73it/s, Materializing param=roberta.encoder.layer.4.attention.output.LayerNorm.bias]Loading weights:  37%|███▋      | 75/201 [00:00<00:00, 7678.31it/s, Materializing param=roberta.encoder.layer.4.attention.output.LayerNorm.weight]Loading weights:  37%|███▋      | 75/201 [00:00<00:00, 7658.50it/s, Materializing param=roberta.encoder.layer.4.attention.output.LayerNorm.weight]Loading weights:  38%|███▊      | 76/201 [00:00<00:00, 7723.38it/s, Materializing param=roberta.encoder.layer.4.attention.output.dense.bias]      Loading weights:  38%|███▊      | 76/201 [00:00<00:00, 7704.71it/s, Materializing param=roberta.encoder.layer.4.attention.output.dense.bias]Loading weights:  38%|███▊      | 77/201 [00:00<00:00, 7770.03it/s, Materializing param=roberta.encoder.layer.4.attention.output.dense.weight]Loading weights:  38%|███▊      | 77/201 [00:00<00:00, 7745.44it/s, Materializing param=roberta.encoder.layer.4.attention.output.dense.weight]Loading weights:  39%|███▉      | 78/201 [00:00<00:00, 7795.36it/s, Materializing param=roberta.encoder.layer.4.attention.self.key.bias]      Loading weights:  39%|███▉      | 78/201 [00:00<00:00, 7767.97it/s, Materializing param=roberta.encoder.layer.4.attention.self.key.bias]Loading weights:  39%|███▉      | 79/201 [00:00<00:00, 7821.87it/s, Materializing param=roberta.encoder.layer.4.attention.self.key.weight]Loading weights:  39%|███▉      | 79/201 [00:00<00:00, 7803.82it/s, Materializing param=roberta.encoder.layer.4.attention.self.key.weight]Loading weights:  40%|███▉      | 80/201 [00:00<00:00, 7871.08it/s, Materializing param=roberta.encoder.layer.4.attention.self.query.bias]Loading weights:  40%|███▉      | 80/201 [00:00<00:00, 7854.32it/s, Materializing param=roberta.encoder.layer.4.attention.self.query.bias]Loading weights:  40%|████      | 81/201 [00:00<00:00, 7915.81it/s, Materializing param=roberta.encoder.layer.4.attention.self.query.weight]Loading weights:  40%|████      | 81/201 [00:00<00:00, 7897.23it/s, Materializing param=roberta.encoder.layer.4.attention.self.query.weight]Loading weights:  41%|████      | 82/201 [00:00<00:00, 7959.57it/s, Materializing param=roberta.encoder.layer.4.attention.self.value.bias]  Loading weights:  41%|████      | 82/201 [00:00<00:00, 7941.56it/s, Materializing param=roberta.encoder.layer.4.attention.self.value.bias]Loading weights:  41%|████▏     | 83/201 [00:00<00:00, 8005.87it/s, Materializing param=roberta.encoder.layer.4.attention.self.value.weight]Loading weights:  41%|████▏     | 83/201 [00:00<00:00, 7986.03it/s, Materializing param=roberta.encoder.layer.4.attention.self.value.weight]Loading weights:  42%|████▏     | 84/201 [00:00<00:00, 8047.91it/s, Materializing param=roberta.encoder.layer.4.intermediate.dense.bias]    Loading weights:  42%|████▏     | 84/201 [00:00<00:00, 8028.84it/s, Materializing param=roberta.encoder.layer.4.intermediate.dense.bias]Loading weights:  42%|████▏     | 85/201 [00:00<00:00, 8089.58it/s, Materializing param=roberta.encoder.layer.4.intermediate.dense.weight]Loading weights:  42%|████▏     | 85/201 [00:00<00:00, 8069.26it/s, Materializing param=roberta.encoder.layer.4.intermediate.dense.weight]Loading weights:  43%|████▎     | 86/201 [00:00<00:00, 8129.05it/s, Materializing param=roberta.encoder.layer.4.output.LayerNorm.bias]    Loading weights:  43%|████▎     | 86/201 [00:00<00:00, 8109.49it/s, Materializing param=roberta.encoder.layer.4.output.LayerNorm.bias]Loading weights:  43%|████▎     | 87/201 [00:00<00:00, 8167.25it/s, Materializing param=roberta.encoder.layer.4.output.LayerNorm.weight]Loading weights:  43%|████▎     | 87/201 [00:00<00:00, 8147.92it/s, Materializing param=roberta.encoder.layer.4.output.LayerNorm.weight]Loading weights:  44%|████▍     | 88/201 [00:00<00:00, 8207.67it/s, Materializing param=roberta.encoder.layer.4.output.dense.bias]      Loading weights:  44%|████▍     | 88/201 [00:00<00:00, 8190.00it/s, Materializing param=roberta.encoder.layer.4.output.dense.bias]Loading weights:  44%|████▍     | 89/201 [00:00<00:00, 8251.21it/s, Materializing param=roberta.encoder.layer.4.output.dense.weight]Loading weights:  44%|████▍     | 89/201 [00:00<00:00, 8234.10it/s, Materializing param=roberta.encoder.layer.4.output.dense.weight]Loading weights:  45%|████▍     | 90/201 [00:00<00:00, 8296.61it/s, Materializing param=roberta.encoder.layer.5.attention.output.LayerNorm.bias]Loading weights:  45%|████▍     | 90/201 [00:00<00:00, 8278.78it/s, Materializing param=roberta.encoder.layer.5.attention.output.LayerNorm.bias]Loading weights:  45%|████▌     | 91/201 [00:00<00:00, 8331.48it/s, Materializing param=roberta.encoder.layer.5.attention.output.LayerNorm.weight]Loading weights:  45%|████▌     | 91/201 [00:00<00:00, 8310.44it/s, Materializing param=roberta.encoder.layer.5.attention.output.LayerNorm.weight]Loading weights:  46%|████▌     | 92/201 [00:00<00:00, 8364.24it/s, Materializing param=roberta.encoder.layer.5.attention.output.dense.bias]      Loading weights:  46%|████▌     | 92/201 [00:00<00:00, 8345.43it/s, Materializing param=roberta.encoder.layer.5.attention.output.dense.bias]Loading weights:  46%|████▋     | 93/201 [00:00<00:00, 8399.99it/s, Materializing param=roberta.encoder.layer.5.attention.output.dense.weight]Loading weights:  46%|████▋     | 93/201 [00:00<00:00, 8382.84it/s, Materializing param=roberta.encoder.layer.5.attention.output.dense.weight]Loading weights:  47%|████▋     | 94/201 [00:00<00:00, 8437.62it/s, Materializing param=roberta.encoder.layer.5.attention.self.key.bias]      Loading weights:  47%|████▋     | 94/201 [00:00<00:00, 8419.06it/s, Materializing param=roberta.encoder.layer.5.attention.self.key.bias]Loading weights:  47%|████▋     | 95/201 [00:00<00:00, 8469.74it/s, Materializing param=roberta.encoder.layer.5.attention.self.key.weight]Loading weights:  47%|████▋     | 95/201 [00:00<00:00, 8449.98it/s, Materializing param=roberta.encoder.layer.5.attention.self.key.weight]Loading weights:  48%|████▊     | 96/201 [00:00<00:00, 8501.61it/s, Materializing param=roberta.encoder.layer.5.attention.self.query.bias]Loading weights:  48%|████▊     | 96/201 [00:00<00:00, 8483.16it/s, Materializing param=roberta.encoder.layer.5.attention.self.query.bias]Loading weights:  48%|████▊     | 97/201 [00:00<00:00, 8537.17it/s, Materializing param=roberta.encoder.layer.5.attention.self.query.weight]Loading weights:  48%|████▊     | 97/201 [00:00<00:00, 8519.65it/s, Materializing param=roberta.encoder.layer.5.attention.self.query.weight]Loading weights:  49%|████▉     | 98/201 [00:00<00:00, 8574.80it/s, Materializing param=roberta.encoder.layer.5.attention.self.value.bias]  Loading weights:  49%|████▉     | 98/201 [00:00<00:00, 8558.20it/s, Materializing param=roberta.encoder.layer.5.attention.self.value.bias]Loading weights:  49%|████▉     | 99/201 [00:00<00:00, 8611.46it/s, Materializing param=roberta.encoder.layer.5.attention.self.value.weight]Loading weights:  49%|████▉     | 99/201 [00:00<00:00, 8594.35it/s, Materializing param=roberta.encoder.layer.5.attention.self.value.weight]Loading weights:  50%|████▉     | 100/201 [00:00<00:00, 8649.83it/s, Materializing param=roberta.encoder.layer.5.intermediate.dense.bias]   Loading weights:  50%|████▉     | 100/201 [00:00<00:00, 8633.28it/s, Materializing param=roberta.encoder.layer.5.intermediate.dense.bias]Loading weights:  50%|█████     | 101/201 [00:00<00:00, 8687.42it/s, Materializing param=roberta.encoder.layer.5.intermediate.dense.weight]Loading weights:  50%|█████     | 101/201 [00:00<00:00, 8670.00it/s, Materializing param=roberta.encoder.layer.5.intermediate.dense.weight]Loading weights:  51%|█████     | 102/201 [00:00<00:00, 8722.99it/s, Materializing param=roberta.encoder.layer.5.output.LayerNorm.bias]    Loading weights:  51%|█████     | 102/201 [00:00<00:00, 8706.48it/s, Materializing param=roberta.encoder.layer.5.output.LayerNorm.bias]Loading weights:  51%|█████     | 103/201 [00:00<00:00, 8758.68it/s, Materializing param=roberta.encoder.layer.5.output.LayerNorm.weight]Loading weights:  51%|█████     | 103/201 [00:00<00:00, 8742.55it/s, Materializing param=roberta.encoder.layer.5.output.LayerNorm.weight]Loading weights:  52%|█████▏    | 104/201 [00:00<00:00, 8796.46it/s, Materializing param=roberta.encoder.layer.5.output.dense.bias]      Loading weights:  52%|█████▏    | 104/201 [00:00<00:00, 8780.17it/s, Materializing param=roberta.encoder.layer.5.output.dense.bias]Loading weights:  52%|█████▏    | 105/201 [00:00<00:00, 8833.12it/s, Materializing param=roberta.encoder.layer.5.output.dense.weight]Loading weights:  52%|█████▏    | 105/201 [00:00<00:00, 8816.33it/s, Materializing param=roberta.encoder.layer.5.output.dense.weight]Loading weights:  53%|█████▎    | 106/201 [00:00<00:00, 8870.28it/s, Materializing param=roberta.encoder.layer.6.attention.output.LayerNorm.bias]Loading weights:  53%|█████▎    | 106/201 [00:00<00:00, 8854.03it/s, Materializing param=roberta.encoder.layer.6.attention.output.LayerNorm.bias]Loading weights:  53%|█████▎    | 107/201 [00:00<00:00, 8904.22it/s, Materializing param=roberta.encoder.layer.6.attention.output.LayerNorm.weight]Loading weights:  53%|█████▎    | 107/201 [00:00<00:00, 8887.29it/s, Materializing param=roberta.encoder.layer.6.attention.output.LayerNorm.weight]Loading weights:  54%|█████▎    | 108/201 [00:00<00:00, 8938.67it/s, Materializing param=roberta.encoder.layer.6.attention.output.dense.bias]      Loading weights:  54%|█████▎    | 108/201 [00:00<00:00, 8922.29it/s, Materializing param=roberta.encoder.layer.6.attention.output.dense.bias]Loading weights:  54%|█████▍    | 109/201 [00:00<00:00, 8972.92it/s, Materializing param=roberta.encoder.layer.6.attention.output.dense.weight]Loading weights:  54%|█████▍    | 109/201 [00:00<00:00, 8956.57it/s, Materializing param=roberta.encoder.layer.6.attention.output.dense.weight]Loading weights:  55%|█████▍    | 110/201 [00:00<00:00, 9005.04it/s, Materializing param=roberta.encoder.layer.6.attention.self.key.bias]      Loading weights:  55%|█████▍    | 110/201 [00:00<00:00, 8988.20it/s, Materializing param=roberta.encoder.layer.6.attention.self.key.bias]Loading weights:  55%|█████▌    | 111/201 [00:00<00:00, 9037.52it/s, Materializing param=roberta.encoder.layer.6.attention.self.key.weight]Loading weights:  55%|█████▌    | 111/201 [00:00<00:00, 9021.23it/s, Materializing param=roberta.encoder.layer.6.attention.self.key.weight]Loading weights:  56%|█████▌    | 112/201 [00:00<00:00, 9071.39it/s, Materializing param=roberta.encoder.layer.6.attention.self.query.bias]Loading weights:  56%|█████▌    | 112/201 [00:00<00:00, 9054.96it/s, Materializing param=roberta.encoder.layer.6.attention.self.query.bias]Loading weights:  56%|█████▌    | 113/201 [00:00<00:00, 9104.22it/s, Materializing param=roberta.encoder.layer.6.attention.self.query.weight]Loading weights:  56%|█████▌    | 113/201 [00:00<00:00, 9088.15it/s, Materializing param=roberta.encoder.layer.6.attention.self.query.weight]Loading weights:  57%|█████▋    | 114/201 [00:00<00:00, 9137.57it/s, Materializing param=roberta.encoder.layer.6.attention.self.value.bias]  Loading weights:  57%|█████▋    | 114/201 [00:00<00:00, 9121.01it/s, Materializing param=roberta.encoder.layer.6.attention.self.value.bias]Loading weights:  57%|█████▋    | 115/201 [00:00<00:00, 9168.84it/s, Materializing param=roberta.encoder.layer.6.attention.self.value.weight]Loading weights:  57%|█████▋    | 115/201 [00:00<00:00, 9151.61it/s, Materializing param=roberta.encoder.layer.6.attention.self.value.weight]Loading weights:  58%|█████▊    | 116/201 [00:00<00:00, 9199.25it/s, Materializing param=roberta.encoder.layer.6.intermediate.dense.bias]    Loading weights:  58%|█████▊    | 116/201 [00:00<00:00, 9182.41it/s, Materializing param=roberta.encoder.layer.6.intermediate.dense.bias]Loading weights:  58%|█████▊    | 117/201 [00:00<00:00, 9228.65it/s, Materializing param=roberta.encoder.layer.6.intermediate.dense.weight]Loading weights:  58%|█████▊    | 117/201 [00:00<00:00, 9213.06it/s, Materializing param=roberta.encoder.layer.6.intermediate.dense.weight]Loading weights:  59%|█████▊    | 118/201 [00:00<00:00, 9261.20it/s, Materializing param=roberta.encoder.layer.6.output.LayerNorm.bias]    Loading weights:  59%|█████▊    | 118/201 [00:00<00:00, 9245.63it/s, Materializing param=roberta.encoder.layer.6.output.LayerNorm.bias]Loading weights:  59%|█████▉    | 119/201 [00:00<00:00, 9293.43it/s, Materializing param=roberta.encoder.layer.6.output.LayerNorm.weight]Loading weights:  59%|█████▉    | 119/201 [00:00<00:00, 9277.71it/s, Materializing param=roberta.encoder.layer.6.output.LayerNorm.weight]Loading weights:  60%|█████▉    | 120/201 [00:00<00:00, 9324.82it/s, Materializing param=roberta.encoder.layer.6.output.dense.bias]      Loading weights:  60%|█████▉    | 120/201 [00:00<00:00, 9308.78it/s, Materializing param=roberta.encoder.layer.6.output.dense.bias]Loading weights:  60%|██████    | 121/201 [00:00<00:00, 9356.93it/s, Materializing param=roberta.encoder.layer.6.output.dense.weight]Loading weights:  60%|██████    | 121/201 [00:00<00:00, 9340.23it/s, Materializing param=roberta.encoder.layer.6.output.dense.weight]Loading weights:  61%|██████    | 122/201 [00:00<00:00, 9387.71it/s, Materializing param=roberta.encoder.layer.7.attention.output.LayerNorm.bias]Loading weights:  61%|██████    | 122/201 [00:00<00:00, 9371.03it/s, Materializing param=roberta.encoder.layer.7.attention.output.LayerNorm.bias]Loading weights:  61%|██████    | 123/201 [00:00<00:00, 9414.05it/s, Materializing param=roberta.encoder.layer.7.attention.output.LayerNorm.weight]Loading weights:  61%|██████    | 123/201 [00:00<00:00, 9396.73it/s, Materializing param=roberta.encoder.layer.7.attention.output.LayerNorm.weight]Loading weights:  62%|██████▏   | 124/201 [00:00<00:00, 9441.49it/s, Materializing param=roberta.encoder.layer.7.attention.output.dense.bias]      Loading weights:  62%|██████▏   | 124/201 [00:00<00:00, 9425.23it/s, Materializing param=roberta.encoder.layer.7.attention.output.dense.bias]Loading weights:  62%|██████▏   | 125/201 [00:00<00:00, 9469.84it/s, Materializing param=roberta.encoder.layer.7.attention.output.dense.weight]Loading weights:  62%|██████▏   | 125/201 [00:00<00:00, 9451.57it/s, Materializing param=roberta.encoder.layer.7.attention.output.dense.weight]Loading weights:  63%|██████▎   | 126/201 [00:00<00:00, 9495.17it/s, Materializing param=roberta.encoder.layer.7.attention.self.key.bias]      Loading weights:  63%|██████▎   | 126/201 [00:00<00:00, 9478.82it/s, Materializing param=roberta.encoder.layer.7.attention.self.key.bias]Loading weights:  63%|██████▎   | 127/201 [00:00<00:00, 9523.99it/s, Materializing param=roberta.encoder.layer.7.attention.self.key.weight]Loading weights:  63%|██████▎   | 127/201 [00:00<00:00, 9508.52it/s, Materializing param=roberta.encoder.layer.7.attention.self.key.weight]Loading weights:  64%|██████▎   | 128/201 [00:00<00:00, 9553.03it/s, Materializing param=roberta.encoder.layer.7.attention.self.query.bias]Loading weights:  64%|██████▎   | 128/201 [00:00<00:00, 9536.91it/s, Materializing param=roberta.encoder.layer.7.attention.self.query.bias]Loading weights:  64%|██████▍   | 129/201 [00:00<00:00, 9580.95it/s, Materializing param=roberta.encoder.layer.7.attention.self.query.weight]Loading weights:  64%|██████▍   | 129/201 [00:00<00:00, 9565.03it/s, Materializing param=roberta.encoder.layer.7.attention.self.query.weight]Loading weights:  65%|██████▍   | 130/201 [00:00<00:00, 9609.96it/s, Materializing param=roberta.encoder.layer.7.attention.self.value.bias]  Loading weights:  65%|██████▍   | 130/201 [00:00<00:00, 9594.07it/s, Materializing param=roberta.encoder.layer.7.attention.self.value.bias]Loading weights:  65%|██████▌   | 131/201 [00:00<00:00, 9638.36it/s, Materializing param=roberta.encoder.layer.7.attention.self.value.weight]Loading weights:  65%|██████▌   | 131/201 [00:00<00:00, 9622.49it/s, Materializing param=roberta.encoder.layer.7.attention.self.value.weight]Loading weights:  66%|██████▌   | 132/201 [00:00<00:00, 9666.32it/s, Materializing param=roberta.encoder.layer.7.intermediate.dense.bias]    Loading weights:  66%|██████▌   | 132/201 [00:00<00:00, 9649.98it/s, Materializing param=roberta.encoder.layer.7.intermediate.dense.bias]Loading weights:  66%|██████▌   | 133/201 [00:00<00:00, 9693.18it/s, Materializing param=roberta.encoder.layer.7.intermediate.dense.weight]Loading weights:  66%|██████▌   | 133/201 [00:00<00:00, 9677.54it/s, Materializing param=roberta.encoder.layer.7.intermediate.dense.weight]Loading weights:  67%|██████▋   | 134/201 [00:00<00:00, 9721.46it/s, Materializing param=roberta.encoder.layer.7.output.LayerNorm.bias]    Loading weights:  67%|██████▋   | 134/201 [00:00<00:00, 9705.52it/s, Materializing param=roberta.encoder.layer.7.output.LayerNorm.bias]Loading weights:  67%|██████▋   | 135/201 [00:00<00:00, 9747.98it/s, Materializing param=roberta.encoder.layer.7.output.LayerNorm.weight]Loading weights:  67%|██████▋   | 135/201 [00:00<00:00, 9733.07it/s, Materializing param=roberta.encoder.layer.7.output.LayerNorm.weight]Loading weights:  68%|██████▊   | 136/201 [00:00<00:00, 9776.43it/s, Materializing param=roberta.encoder.layer.7.output.dense.bias]      Loading weights:  68%|██████▊   | 136/201 [00:00<00:00, 9760.37it/s, Materializing param=roberta.encoder.layer.7.output.dense.bias]Loading weights:  68%|██████▊   | 137/201 [00:00<00:00, 9803.62it/s, Materializing param=roberta.encoder.layer.7.output.dense.weight]Loading weights:  68%|██████▊   | 137/201 [00:00<00:00, 9787.76it/s, Materializing param=roberta.encoder.layer.7.output.dense.weight]Loading weights:  69%|██████▊   | 138/201 [00:00<00:00, 9831.07it/s, Materializing param=roberta.encoder.layer.8.attention.output.LayerNorm.bias]Loading weights:  69%|██████▊   | 138/201 [00:00<00:00, 9815.40it/s, Materializing param=roberta.encoder.layer.8.attention.output.LayerNorm.bias]Loading weights:  69%|██████▉   | 139/201 [00:00<00:00, 9855.44it/s, Materializing param=roberta.encoder.layer.8.attention.output.LayerNorm.weight]Loading weights:  69%|██████▉   | 139/201 [00:00<00:00, 9839.47it/s, Materializing param=roberta.encoder.layer.8.attention.output.LayerNorm.weight]Loading weights:  70%|██████▉   | 140/201 [00:00<00:00, 9879.08it/s, Materializing param=roberta.encoder.layer.8.attention.output.dense.bias]      Loading weights:  70%|██████▉   | 140/201 [00:00<00:00, 9863.81it/s, Materializing param=roberta.encoder.layer.8.attention.output.dense.bias]Loading weights:  70%|███████   | 141/201 [00:00<00:00, 9900.84it/s, Materializing param=roberta.encoder.layer.8.attention.output.dense.weight]Loading weights:  70%|███████   | 141/201 [00:00<00:00, 9883.30it/s, Materializing param=roberta.encoder.layer.8.attention.output.dense.weight]Loading weights:  71%|███████   | 142/201 [00:00<00:00, 9923.05it/s, Materializing param=roberta.encoder.layer.8.attention.self.key.bias]      Loading weights:  71%|███████   | 142/201 [00:00<00:00, 9907.37it/s, Materializing param=roberta.encoder.layer.8.attention.self.key.bias]Loading weights:  71%|███████   | 143/201 [00:00<00:00, 9947.02it/s, Materializing param=roberta.encoder.layer.8.attention.self.key.weight]Loading weights:  71%|███████   | 143/201 [00:00<00:00, 9931.21it/s, Materializing param=roberta.encoder.layer.8.attention.self.key.weight]Loading weights:  72%|███████▏  | 144/201 [00:00<00:00, 9970.78it/s, Materializing param=roberta.encoder.layer.8.attention.self.query.bias]Loading weights:  72%|███████▏  | 144/201 [00:00<00:00, 9955.16it/s, Materializing param=roberta.encoder.layer.8.attention.self.query.bias]Loading weights:  72%|███████▏  | 145/201 [00:00<00:00, 9994.48it/s, Materializing param=roberta.encoder.layer.8.attention.self.query.weight]Loading weights:  72%|███████▏  | 145/201 [00:00<00:00, 9977.92it/s, Materializing param=roberta.encoder.layer.8.attention.self.query.weight]Loading weights:  73%|███████▎  | 146/201 [00:00<00:00, 10015.18it/s, Materializing param=roberta.encoder.layer.8.attention.self.value.bias] Loading weights:  73%|███████▎  | 146/201 [00:00<00:00, 9996.71it/s, Materializing param=roberta.encoder.layer.8.attention.self.value.bias] Loading weights:  73%|███████▎  | 147/201 [00:00<00:00, 10031.44it/s, Materializing param=roberta.encoder.layer.8.attention.self.value.weight]Loading weights:  73%|███████▎  | 147/201 [00:00<00:00, 10014.50it/s, Materializing param=roberta.encoder.layer.8.attention.self.value.weight]Loading weights:  74%|███████▎  | 148/201 [00:00<00:00, 10052.58it/s, Materializing param=roberta.encoder.layer.8.intermediate.dense.bias]    Loading weights:  74%|███████▎  | 148/201 [00:00<00:00, 10035.36it/s, Materializing param=roberta.encoder.layer.8.intermediate.dense.bias]Loading weights:  74%|███████▍  | 149/201 [00:00<00:00, 10070.11it/s, Materializing param=roberta.encoder.layer.8.intermediate.dense.weight]Loading weights:  74%|███████▍  | 149/201 [00:00<00:00, 10053.27it/s, Materializing param=roberta.encoder.layer.8.intermediate.dense.weight]Loading weights:  75%|███████▍  | 150/201 [00:00<00:00, 10089.58it/s, Materializing param=roberta.encoder.layer.8.output.LayerNorm.bias]    Loading weights:  75%|███████▍  | 150/201 [00:00<00:00, 10073.10it/s, Materializing param=roberta.encoder.layer.8.output.LayerNorm.bias]Loading weights:  75%|███████▌  | 151/201 [00:00<00:00, 10109.50it/s, Materializing param=roberta.encoder.layer.8.output.LayerNorm.weight]Loading weights:  75%|███████▌  | 151/201 [00:00<00:00, 10093.23it/s, Materializing param=roberta.encoder.layer.8.output.LayerNorm.weight]Loading weights:  76%|███████▌  | 152/201 [00:00<00:00, 10130.04it/s, Materializing param=roberta.encoder.layer.8.output.dense.bias]      Loading weights:  76%|███████▌  | 152/201 [00:00<00:00, 10114.77it/s, Materializing param=roberta.encoder.layer.8.output.dense.bias]Loading weights:  76%|███████▌  | 153/201 [00:00<00:00, 10150.24it/s, Materializing param=roberta.encoder.layer.8.output.dense.weight]Loading weights:  76%|███████▌  | 153/201 [00:00<00:00, 10134.37it/s, Materializing param=roberta.encoder.layer.8.output.dense.weight]Loading weights:  77%|███████▋  | 154/201 [00:00<00:00, 10172.17it/s, Materializing param=roberta.encoder.layer.9.attention.output.LayerNorm.bias]Loading weights:  77%|███████▋  | 154/201 [00:00<00:00, 10156.34it/s, Materializing param=roberta.encoder.layer.9.attention.output.LayerNorm.bias]Loading weights:  77%|███████▋  | 155/201 [00:00<00:00, 10192.00it/s, Materializing param=roberta.encoder.layer.9.attention.output.LayerNorm.weight]Loading weights:  77%|███████▋  | 155/201 [00:00<00:00, 10173.82it/s, Materializing param=roberta.encoder.layer.9.attention.output.LayerNorm.weight]Loading weights:  78%|███████▊  | 156/201 [00:00<00:00, 10209.90it/s, Materializing param=roberta.encoder.layer.9.attention.output.dense.bias]      Loading weights:  78%|███████▊  | 156/201 [00:00<00:00, 10194.31it/s, Materializing param=roberta.encoder.layer.9.attention.output.dense.bias]Loading weights:  78%|███████▊  | 157/201 [00:00<00:00, 10230.65it/s, Materializing param=roberta.encoder.layer.9.attention.output.dense.weight]Loading weights:  78%|███████▊  | 157/201 [00:00<00:00, 10215.73it/s, Materializing param=roberta.encoder.layer.9.attention.output.dense.weight]Loading weights:  79%|███████▊  | 158/201 [00:00<00:00, 10252.64it/s, Materializing param=roberta.encoder.layer.9.attention.self.key.bias]      Loading weights:  79%|███████▊  | 158/201 [00:00<00:00, 10237.44it/s, Materializing param=roberta.encoder.layer.9.attention.self.key.bias]Loading weights:  79%|███████▉  | 159/201 [00:00<00:00, 10272.87it/s, Materializing param=roberta.encoder.layer.9.attention.self.key.weight]Loading weights:  79%|███████▉  | 159/201 [00:00<00:00, 10257.70it/s, Materializing param=roberta.encoder.layer.9.attention.self.key.weight]Loading weights:  80%|███████▉  | 160/201 [00:00<00:00, 10293.56it/s, Materializing param=roberta.encoder.layer.9.attention.self.query.bias]Loading weights:  80%|███████▉  | 160/201 [00:00<00:00, 10277.95it/s, Materializing param=roberta.encoder.layer.9.attention.self.query.bias]Loading weights:  80%|████████  | 161/201 [00:00<00:00, 10314.39it/s, Materializing param=roberta.encoder.layer.9.attention.self.query.weight]Loading weights:  80%|████████  | 161/201 [00:00<00:00, 10298.97it/s, Materializing param=roberta.encoder.layer.9.attention.self.query.weight]Loading weights:  81%|████████  | 162/201 [00:00<00:00, 10334.73it/s, Materializing param=roberta.encoder.layer.9.attention.self.value.bias]  Loading weights:  81%|████████  | 162/201 [00:00<00:00, 10318.72it/s, Materializing param=roberta.encoder.layer.9.attention.self.value.bias]Loading weights:  81%|████████  | 163/201 [00:00<00:00, 10354.27it/s, Materializing param=roberta.encoder.layer.9.attention.self.value.weight]Loading weights:  81%|████████  | 163/201 [00:00<00:00, 10338.45it/s, Materializing param=roberta.encoder.layer.9.attention.self.value.weight]Loading weights:  82%|████████▏ | 164/201 [00:00<00:00, 10371.30it/s, Materializing param=roberta.encoder.layer.9.intermediate.dense.bias]    Loading weights:  82%|████████▏ | 164/201 [00:00<00:00, 10355.53it/s, Materializing param=roberta.encoder.layer.9.intermediate.dense.bias]Loading weights:  82%|████████▏ | 165/201 [00:00<00:00, 10385.99it/s, Materializing param=roberta.encoder.layer.9.intermediate.dense.weight]Loading weights:  82%|████████▏ | 165/201 [00:00<00:00, 10369.81it/s, Materializing param=roberta.encoder.layer.9.intermediate.dense.weight]Loading weights:  83%|████████▎ | 166/201 [00:00<00:00, 10400.86it/s, Materializing param=roberta.encoder.layer.9.output.LayerNorm.bias]    Loading weights:  83%|████████▎ | 166/201 [00:00<00:00, 10385.04it/s, Materializing param=roberta.encoder.layer.9.output.LayerNorm.bias]Loading weights:  83%|████████▎ | 167/201 [00:00<00:00, 10410.33it/s, Materializing param=roberta.encoder.layer.9.output.LayerNorm.weight]Loading weights:  83%|████████▎ | 167/201 [00:00<00:00, 10393.34it/s, Materializing param=roberta.encoder.layer.9.output.LayerNorm.weight]Loading weights:  84%|████████▎ | 168/201 [00:00<00:00, 10421.86it/s, Materializing param=roberta.encoder.layer.9.output.dense.bias]      Loading weights:  84%|████████▎ | 168/201 [00:00<00:00, 10406.01it/s, Materializing param=roberta.encoder.layer.9.output.dense.bias]Loading weights:  84%|████████▍ | 169/201 [00:00<00:00, 10435.59it/s, Materializing param=roberta.encoder.layer.9.output.dense.weight]Loading weights:  84%|████████▍ | 169/201 [00:00<00:00, 10418.26it/s, Materializing param=roberta.encoder.layer.9.output.dense.weight]Loading weights:  85%|████████▍ | 170/201 [00:00<00:00, 10451.33it/s, Materializing param=roberta.encoder.layer.10.attention.output.LayerNorm.bias]Loading weights:  85%|████████▍ | 170/201 [00:00<00:00, 10433.74it/s, Materializing param=roberta.encoder.layer.10.attention.output.LayerNorm.bias]Loading weights:  85%|████████▌ | 171/201 [00:00<00:00, 10462.05it/s, Materializing param=roberta.encoder.layer.10.attention.output.LayerNorm.weight]Loading weights:  85%|████████▌ | 171/201 [00:00<00:00, 10446.36it/s, Materializing param=roberta.encoder.layer.10.attention.output.LayerNorm.weight]Loading weights:  86%|████████▌ | 172/201 [00:00<00:00, 10475.41it/s, Materializing param=roberta.encoder.layer.10.attention.output.dense.bias]      Loading weights:  86%|████████▌ | 172/201 [00:00<00:00, 10458.85it/s, Materializing param=roberta.encoder.layer.10.attention.output.dense.bias]Loading weights:  86%|████████▌ | 173/201 [00:00<00:00, 10486.52it/s, Materializing param=roberta.encoder.layer.10.attention.output.dense.weight]Loading weights:  86%|████████▌ | 173/201 [00:00<00:00, 10470.48it/s, Materializing param=roberta.encoder.layer.10.attention.output.dense.weight]Loading weights:  87%|████████▋ | 174/201 [00:00<00:00, 10499.49it/s, Materializing param=roberta.encoder.layer.10.attention.self.key.bias]      Loading weights:  87%|████████▋ | 174/201 [00:00<00:00, 10484.10it/s, Materializing param=roberta.encoder.layer.10.attention.self.key.bias]Loading weights:  87%|████████▋ | 175/201 [00:00<00:00, 10515.05it/s, Materializing param=roberta.encoder.layer.10.attention.self.key.weight]Loading weights:  87%|████████▋ | 175/201 [00:00<00:00, 10499.86it/s, Materializing param=roberta.encoder.layer.10.attention.self.key.weight]Loading weights:  88%|████████▊ | 176/201 [00:00<00:00, 10531.09it/s, Materializing param=roberta.encoder.layer.10.attention.self.query.bias]Loading weights:  88%|████████▊ | 176/201 [00:00<00:00, 10516.53it/s, Materializing param=roberta.encoder.layer.10.attention.self.query.bias]Loading weights:  88%|████████▊ | 177/201 [00:00<00:00, 10546.39it/s, Materializing param=roberta.encoder.layer.10.attention.self.query.weight]Loading weights:  88%|████████▊ | 177/201 [00:00<00:00, 10532.02it/s, Materializing param=roberta.encoder.layer.10.attention.self.query.weight]Loading weights:  89%|████████▊ | 178/201 [00:00<00:00, 10562.31it/s, Materializing param=roberta.encoder.layer.10.attention.self.value.bias]  Loading weights:  89%|████████▊ | 178/201 [00:00<00:00, 10548.13it/s, Materializing param=roberta.encoder.layer.10.attention.self.value.bias]Loading weights:  89%|████████▉ | 179/201 [00:00<00:00, 10579.59it/s, Materializing param=roberta.encoder.layer.10.attention.self.value.weight]Loading weights:  89%|████████▉ | 179/201 [00:00<00:00, 10565.44it/s, Materializing param=roberta.encoder.layer.10.attention.self.value.weight]Loading weights:  90%|████████▉ | 180/201 [00:00<00:00, 10599.11it/s, Materializing param=roberta.encoder.layer.10.intermediate.dense.bias]    Loading weights:  90%|████████▉ | 180/201 [00:00<00:00, 10583.81it/s, Materializing param=roberta.encoder.layer.10.intermediate.dense.bias]Loading weights:  90%|█████████ | 181/201 [00:00<00:00, 10615.08it/s, Materializing param=roberta.encoder.layer.10.intermediate.dense.weight]Loading weights:  90%|█████████ | 181/201 [00:00<00:00, 10599.96it/s, Materializing param=roberta.encoder.layer.10.intermediate.dense.weight]Loading weights:  91%|█████████ | 182/201 [00:00<00:00, 10632.10it/s, Materializing param=roberta.encoder.layer.10.output.LayerNorm.bias]    Loading weights:  91%|█████████ | 182/201 [00:00<00:00, 10617.46it/s, Materializing param=roberta.encoder.layer.10.output.LayerNorm.bias]Loading weights:  91%|█████████ | 183/201 [00:00<00:00, 10649.13it/s, Materializing param=roberta.encoder.layer.10.output.LayerNorm.weight]Loading weights:  91%|█████████ | 183/201 [00:00<00:00, 10634.67it/s, Materializing param=roberta.encoder.layer.10.output.LayerNorm.weight]Loading weights:  92%|█████████▏| 184/201 [00:00<00:00, 10663.24it/s, Materializing param=roberta.encoder.layer.10.output.dense.bias]      Loading weights:  92%|█████████▏| 184/201 [00:00<00:00, 10648.38it/s, Materializing param=roberta.encoder.layer.10.output.dense.bias]Loading weights:  92%|█████████▏| 185/201 [00:00<00:00, 10680.46it/s, Materializing param=roberta.encoder.layer.10.output.dense.weight]Loading weights:  92%|█████████▏| 185/201 [00:00<00:00, 10666.07it/s, Materializing param=roberta.encoder.layer.10.output.dense.weight]Loading weights:  93%|█████████▎| 186/201 [00:00<00:00, 10699.31it/s, Materializing param=roberta.encoder.layer.11.attention.output.LayerNorm.bias]Loading weights:  93%|█████████▎| 186/201 [00:00<00:00, 10683.78it/s, Materializing param=roberta.encoder.layer.11.attention.output.LayerNorm.bias]Loading weights:  93%|█████████▎| 187/201 [00:00<00:00, 10713.20it/s, Materializing param=roberta.encoder.layer.11.attention.output.LayerNorm.weight]Loading weights:  93%|█████████▎| 187/201 [00:00<00:00, 10699.46it/s, Materializing param=roberta.encoder.layer.11.attention.output.LayerNorm.weight]Loading weights:  94%|█████████▎| 188/201 [00:00<00:00, 10730.04it/s, Materializing param=roberta.encoder.layer.11.attention.output.dense.bias]      Loading weights:  94%|█████████▎| 188/201 [00:00<00:00, 10715.75it/s, Materializing param=roberta.encoder.layer.11.attention.output.dense.bias]Loading weights:  94%|█████████▍| 189/201 [00:00<00:00, 10746.61it/s, Materializing param=roberta.encoder.layer.11.attention.output.dense.weight]Loading weights:  94%|█████████▍| 189/201 [00:00<00:00, 10732.49it/s, Materializing param=roberta.encoder.layer.11.attention.output.dense.weight]Loading weights:  95%|█████████▍| 190/201 [00:00<00:00, 10764.21it/s, Materializing param=roberta.encoder.layer.11.attention.self.key.bias]      Loading weights:  95%|█████████▍| 190/201 [00:00<00:00, 10749.55it/s, Materializing param=roberta.encoder.layer.11.attention.self.key.bias]Loading weights:  95%|█████████▌| 191/201 [00:00<00:00, 10781.11it/s, Materializing param=roberta.encoder.layer.11.attention.self.key.weight]Loading weights:  95%|█████████▌| 191/201 [00:00<00:00, 10766.19it/s, Materializing param=roberta.encoder.layer.11.attention.self.key.weight]Loading weights:  96%|█████████▌| 192/201 [00:00<00:00, 10796.58it/s, Materializing param=roberta.encoder.layer.11.attention.self.query.bias]Loading weights:  96%|█████████▌| 192/201 [00:00<00:00, 10782.71it/s, Materializing param=roberta.encoder.layer.11.attention.self.query.bias]Loading weights:  96%|█████████▌| 193/201 [00:00<00:00, 10814.11it/s, Materializing param=roberta.encoder.layer.11.attention.self.query.weight]Loading weights:  96%|█████████▌| 193/201 [00:00<00:00, 10799.82it/s, Materializing param=roberta.encoder.layer.11.attention.self.query.weight]Loading weights:  97%|█████████▋| 194/201 [00:00<00:00, 10831.07it/s, Materializing param=roberta.encoder.layer.11.attention.self.value.bias]  Loading weights:  97%|█████████▋| 194/201 [00:00<00:00, 10816.24it/s, Materializing param=roberta.encoder.layer.11.attention.self.value.bias]Loading weights:  97%|█████████▋| 195/201 [00:00<00:00, 10846.76it/s, Materializing param=roberta.encoder.layer.11.attention.self.value.weight]Loading weights:  97%|█████████▋| 195/201 [00:00<00:00, 10831.97it/s, Materializing param=roberta.encoder.layer.11.attention.self.value.weight]Loading weights:  98%|█████████▊| 196/201 [00:00<00:00, 10862.77it/s, Materializing param=roberta.encoder.layer.11.intermediate.dense.bias]    Loading weights:  98%|█████████▊| 196/201 [00:00<00:00, 10848.29it/s, Materializing param=roberta.encoder.layer.11.intermediate.dense.bias]Loading weights:  98%|█████████▊| 197/201 [00:00<00:00, 10879.38it/s, Materializing param=roberta.encoder.layer.11.intermediate.dense.weight]Loading weights:  98%|█████████▊| 197/201 [00:00<00:00, 10864.93it/s, Materializing param=roberta.encoder.layer.11.intermediate.dense.weight]Loading weights:  99%|█████████▊| 198/201 [00:00<00:00, 10894.87it/s, Materializing param=roberta.encoder.layer.11.output.LayerNorm.bias]    Loading weights:  99%|█████████▊| 198/201 [00:00<00:00, 10880.17it/s, Materializing param=roberta.encoder.layer.11.output.LayerNorm.bias]Loading weights:  99%|█████████▉| 199/201 [00:00<00:00, 10905.97it/s, Materializing param=roberta.encoder.layer.11.output.LayerNorm.weight]Loading weights:  99%|█████████▉| 199/201 [00:00<00:00, 10891.88it/s, Materializing param=roberta.encoder.layer.11.output.LayerNorm.weight]Loading weights: 100%|█████████▉| 200/201 [00:00<00:00, 10922.10it/s, Materializing param=roberta.encoder.layer.11.output.dense.bias]      Loading weights: 100%|█████████▉| 200/201 [00:00<00:00, 10907.47it/s, Materializing param=roberta.encoder.layer.11.output.dense.bias]Loading weights: 100%|██████████| 201/201 [00:00<00:00, 10937.69it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]Loading weights: 100%|██████████| 201/201 [00:00<00:00, 10923.37it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]Loading weights: 100%|██████████| 201/201 [00:00<00:00, 10889.23it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]
XLMRobertaForSequenceClassification LOAD REPORT from: cardiffnlp/twitter-xlm-roberta-base-sentiment
Key                             | Status     |  | 
--------------------------------+------------+--+-
roberta.embeddings.position_ids | UNEXPECTED |  | 

Notes:
- UNEXPECTED	:can be ignored when loading from different task/architecture; not ok if you expect identical arch.
2026-01-27 18:36:13,677 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/cardiffnlp/twitter-xlm-roberta-base-sentiment/commits/main "HTTP/1.1 200 OK"
2026-01-27 18:36:13,710 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/ai4bharat/indic-bert/resolve/refs%2Fpr%2F8/model.safetensors "HTTP/1.1 302 Found"
2026-01-27 18:36:13,765 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/cardiffnlp/twitter-xlm-roberta-base-sentiment/tree/main/additional_chat_templates?recursive=false&expand=false "HTTP/1.1 404 Not Found"
2026-01-27 18:36:14,033 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/cardiffnlp/twitter-xlm-roberta-base-sentiment/discussions?p=0 "HTTP/1.1 200 OK"
2026-01-27 18:36:14,034 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/cardiffnlp/twitter-xlm-roberta-base-sentiment/tree/main?recursive=true&expand=false "HTTP/1.1 200 OK"
Could not extract SentencePiece model from /home/saipraneethmeduri/.cache/huggingface/hub/models--cardiffnlp--twitter-xlm-roberta-base-sentiment/snapshots/f2f1202b1bdeb07342385c3f807f9c07cd8f5cf8/sentencepiece.bpe.model using sentencepiece library due to 
SentencePieceExtractor requires the SentencePiece library but it was not found in your environment. Check out the instructions on the
installation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.
. Falling back to TikToken extractor.
2026-01-27 18:36:14,035 - sentiment_analysis.model_loader - ERROR - Failed to load model cardiffnlp/twitter-xlm-roberta-base-sentiment: `tiktoken` is required to read a `tiktoken` file. Install it with `pip install tiktoken`.
2026-01-27 18:36:14,035 - sentiment_analysis.model_loader - WARNING - Skipping failed model: cardiffnlp/twitter-xlm-roberta-base-sentiment
2026-01-27 18:36:14,035 - sentiment_analysis.model_loader - INFO - Successfully loaded 2/4 models
2026-01-27 18:36:14,035 - sentiment_analysis.model_loader - INFO - Total model loading time: 4547.99ms
2026-01-27 18:36:14,035 - __main__ - INFO - 
[Step 3/5] Running batch inference...
2026-01-27 18:36:14,036 - sentiment_analysis.batch_inference - INFO - Starting inference on 2113 comments with 2 models
Processing models:   0%|          | 0/2 [00:00<?, ?model/s]2026-01-27 18:36:14,036 - sentiment_analysis.batch_inference - INFO - Running inference with google/muril-base-cased...

Inferencing with google/muril-base-cased:   0%|          | 0/133 [00:00<?, ?batch/s][A2026-01-27 18:36:14,342 - httpx - INFO - HTTP Request: GET https://huggingface.co/api/models/cardiffnlp/twitter-xlm-roberta-base-sentiment/commits/refs%2Fpr%2F15 "HTTP/1.1 200 OK"

Inferencing with google/muril-base-cased:   1%|          | 1/133 [00:00<00:50,  2.61batch/s][A2026-01-27 18:36:14,576 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/refs%2Fpr%2F15/model.safetensors.index.json "HTTP/1.1 404 Not Found"
2026-01-27 18:36:14,852 - httpx - INFO - HTTP Request: HEAD https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment/resolve/refs%2Fpr%2F15/model.safetensors "HTTP/1.1 302 Found"

Inferencing with google/muril-base-cased:   2%|▏         | 2/133 [00:00<00:56,  2.32batch/s][A
Inferencing with google/muril-base-cased:   2%|▏         | 3/133 [00:01<00:52,  2.47batch/s][A
Inferencing with google/muril-base-cased:   3%|▎         | 4/133 [00:01<00:51,  2.50batch/s][A
Inferencing with google/muril-base-cased:   4%|▍         | 5/133 [00:01<00:50,  2.55batch/s][A
Inferencing with google/muril-base-cased:   5%|▍         | 6/133 [00:02<00:47,  2.65batch/s][A
Inferencing with google/muril-base-cased:   5%|▌         | 7/133 [00:02<00:46,  2.68batch/s][A
Inferencing with google/muril-base-cased:   6%|▌         | 8/133 [00:03<00:47,  2.63batch/s][A
Inferencing with google/muril-base-cased:   7%|▋         | 9/133 [00:03<00:47,  2.61batch/s][A
Inferencing with google/muril-base-cased:   8%|▊         | 10/133 [00:03<00:46,  2.63batch/s][A
Inferencing with google/muril-base-cased:   8%|▊         | 11/133 [00:04<00:45,  2.65batch/s][A
Inferencing with google/muril-base-cased:   9%|▉         | 12/133 [00:04<00:45,  2.67batch/s][A
Inferencing with google/muril-base-cased:  10%|▉         | 13/133 [00:04<00:43,  2.77batch/s][A
Inferencing with google/muril-base-cased:  11%|█         | 14/133 [00:05<00:42,  2.79batch/s][A
Inferencing with google/muril-base-cased:  11%|█▏        | 15/133 [00:05<00:45,  2.58batch/s][A
Inferencing with google/muril-base-cased:  12%|█▏        | 16/133 [00:06<00:45,  2.57batch/s][A
Inferencing with google/muril-base-cased:  13%|█▎        | 17/133 [00:06<00:43,  2.70batch/s][A
Inferencing with google/muril-base-cased:  14%|█▎        | 18/133 [00:06<00:42,  2.73batch/s][A
Inferencing with google/muril-base-cased:  14%|█▍        | 19/133 [00:07<00:41,  2.75batch/s][A
Inferencing with google/muril-base-cased:  15%|█▌        | 20/133 [00:07<00:40,  2.78batch/s][A
Inferencing with google/muril-base-cased:  16%|█▌        | 21/133 [00:07<00:41,  2.72batch/s][A
Inferencing with google/muril-base-cased:  17%|█▋        | 22/133 [00:08<00:40,  2.73batch/s][A
Inferencing with google/muril-base-cased:  17%|█▋        | 23/133 [00:08<00:39,  2.79batch/s][A
Inferencing with google/muril-base-cased:  18%|█▊        | 24/133 [00:08<00:38,  2.80batch/s][A
Inferencing with google/muril-base-cased:  19%|█▉        | 25/133 [00:09<00:38,  2.78batch/s][A
Inferencing with google/muril-base-cased:  20%|█▉        | 26/133 [00:09<00:38,  2.79batch/s][A
Inferencing with google/muril-base-cased:  20%|██        | 27/133 [00:10<00:39,  2.69batch/s][A
Inferencing with google/muril-base-cased:  21%|██        | 28/133 [00:10<00:40,  2.60batch/s][A
Inferencing with google/muril-base-cased:  22%|██▏       | 29/133 [00:10<00:39,  2.64batch/s][A
Inferencing with google/muril-base-cased:  23%|██▎       | 30/133 [00:11<00:38,  2.65batch/s][A
Inferencing with google/muril-base-cased:  23%|██▎       | 31/133 [00:11<00:38,  2.62batch/s][A
Inferencing with google/muril-base-cased:  24%|██▍       | 32/133 [00:12<00:39,  2.56batch/s][A
Inferencing with google/muril-base-cased:  25%|██▍       | 33/133 [00:12<00:41,  2.42batch/s][A2026-01-27 18:36:26,572 - sentiment_analysis.batch_inference - ERROR - Error in batch inference at index 528: text input must be of type `str` (single example), `list[str]` (batch or single pretokenized example) or `list[list[str]]` (batch of pretokenized examples) or `list[tuple[list[str], list[str]]]` (batch of pretokenized sequence pairs).

Inferencing with google/muril-base-cased:  26%|██▋       | 35/133 [00:13<00:36,  2.72batch/s][A
Inferencing with google/muril-base-cased:  27%|██▋       | 36/133 [00:13<00:38,  2.50batch/s][A
Inferencing with google/muril-base-cased:  28%|██▊       | 37/133 [00:14<00:39,  2.41batch/s][A
Inferencing with google/muril-base-cased:  29%|██▊       | 38/133 [00:14<00:42,  2.26batch/s][A
Inferencing with google/muril-base-cased:  29%|██▉       | 39/133 [00:15<00:45,  2.05batch/s][A
Inferencing with google/muril-base-cased:  30%|███       | 40/133 [00:15<00:46,  1.98batch/s][A
Inferencing with google/muril-base-cased:  31%|███       | 41/133 [00:16<00:45,  2.04batch/s][A
Inferencing with google/muril-base-cased:  32%|███▏      | 42/133 [00:16<00:42,  2.16batch/s][A
Inferencing with google/muril-base-cased:  32%|███▏      | 43/133 [00:17<00:41,  2.19batch/s][A
Inferencing with google/muril-base-cased:  33%|███▎      | 44/133 [00:17<00:38,  2.31batch/s][A
Inferencing with google/muril-base-cased:  34%|███▍      | 45/133 [00:17<00:38,  2.31batch/s][A
Inferencing with google/muril-base-cased:  35%|███▍      | 46/133 [00:18<00:36,  2.38batch/s][A
Inferencing with google/muril-base-cased:  35%|███▌      | 47/133 [00:18<00:35,  2.43batch/s][A
Inferencing with google/muril-base-cased:  36%|███▌      | 48/133 [00:19<00:34,  2.46batch/s][A
Inferencing with google/muril-base-cased:  37%|███▋      | 49/133 [00:19<00:35,  2.38batch/s][A
Inferencing with google/muril-base-cased:  38%|███▊      | 50/133 [00:19<00:34,  2.43batch/s][A
Inferencing with google/muril-base-cased:  38%|███▊      | 51/133 [00:20<00:33,  2.48batch/s][A
Inferencing with google/muril-base-cased:  39%|███▉      | 52/133 [00:20<00:32,  2.53batch/s][A
Inferencing with google/muril-base-cased:  40%|███▉      | 53/133 [00:21<00:31,  2.56batch/s][A
Inferencing with google/muril-base-cased:  41%|████      | 54/133 [00:21<00:29,  2.64batch/s][A
Inferencing with google/muril-base-cased:  41%|████▏     | 55/133 [00:21<00:30,  2.59batch/s][A
Inferencing with google/muril-base-cased:  42%|████▏     | 56/133 [00:22<00:30,  2.52batch/s][A
Inferencing with google/muril-base-cased:  43%|████▎     | 57/133 [00:22<00:30,  2.52batch/s][A
Inferencing with google/muril-base-cased:  44%|████▎     | 58/133 [00:23<00:31,  2.41batch/s][A
Inferencing with google/muril-base-cased:  44%|████▍     | 59/133 [00:23<00:31,  2.38batch/s][A
Inferencing with google/muril-base-cased:  45%|████▌     | 60/133 [00:23<00:30,  2.40batch/s][A
Inferencing with google/muril-base-cased:  46%|████▌     | 61/133 [00:24<00:30,  2.37batch/s][A
Inferencing with google/muril-base-cased:  47%|████▋     | 62/133 [00:24<00:29,  2.39batch/s][A
Inferencing with google/muril-base-cased:  47%|████▋     | 63/133 [00:25<00:30,  2.33batch/s][A
Inferencing with google/muril-base-cased:  48%|████▊     | 64/133 [00:25<00:28,  2.45batch/s][A
Inferencing with google/muril-base-cased:  49%|████▉     | 65/133 [00:25<00:26,  2.60batch/s][A
Inferencing with google/muril-base-cased:  50%|████▉     | 66/133 [00:26<00:24,  2.72batch/s][A
Inferencing with google/muril-base-cased:  50%|█████     | 67/133 [00:26<00:23,  2.81batch/s][A
Inferencing with google/muril-base-cased:  51%|█████     | 68/133 [00:26<00:22,  2.89batch/s][A
Inferencing with google/muril-base-cased:  52%|█████▏    | 69/133 [00:27<00:22,  2.88batch/s][A
Inferencing with google/muril-base-cased:  53%|█████▎    | 70/133 [00:27<00:22,  2.80batch/s][A
Inferencing with google/muril-base-cased:  53%|█████▎    | 71/133 [00:27<00:22,  2.76batch/s][A
Inferencing with google/muril-base-cased:  54%|█████▍    | 72/133 [00:28<00:21,  2.85batch/s][A
Inferencing with google/muril-base-cased:  55%|█████▍    | 73/133 [00:28<00:20,  2.87batch/s][A
Inferencing with google/muril-base-cased:  56%|█████▌    | 74/133 [00:28<00:20,  2.92batch/s][A
Inferencing with google/muril-base-cased:  56%|█████▋    | 75/133 [00:29<00:19,  2.96batch/s][A
Inferencing with google/muril-base-cased:  57%|█████▋    | 76/133 [00:29<00:19,  2.99batch/s][A
Inferencing with google/muril-base-cased:  58%|█████▊    | 77/133 [00:29<00:19,  2.93batch/s][A
Inferencing with google/muril-base-cased:  59%|█████▊    | 78/133 [00:30<00:19,  2.87batch/s][A
Inferencing with google/muril-base-cased:  59%|█████▉    | 79/133 [00:30<00:18,  2.91batch/s][A
Inferencing with google/muril-base-cased:  60%|██████    | 80/133 [00:31<00:18,  2.84batch/s][A
Inferencing with google/muril-base-cased:  61%|██████    | 81/133 [00:31<00:19,  2.73batch/s][A
Inferencing with google/muril-base-cased:  62%|██████▏   | 82/133 [00:31<00:18,  2.70batch/s][A
Inferencing with google/muril-base-cased:  62%|██████▏   | 83/133 [00:32<00:19,  2.61batch/s][A
Inferencing with google/muril-base-cased:  63%|██████▎   | 84/133 [00:32<00:18,  2.62batch/s][A
Inferencing with google/muril-base-cased:  64%|██████▍   | 85/133 [00:33<00:18,  2.57batch/s][A
Inferencing with google/muril-base-cased:  65%|██████▍   | 86/133 [00:33<00:18,  2.58batch/s][A2026-01-27 18:36:47,602 - sentiment_analysis.batch_inference - ERROR - Error in batch inference at index 1376: text input must be of type `str` (single example), `list[str]` (batch or single pretokenized example) or `list[list[str]]` (batch of pretokenized examples) or `list[tuple[list[str], list[str]]]` (batch of pretokenized sequence pairs).

Inferencing with google/muril-base-cased:  65%|██████▌   | 87/133 [00:33<00:14,  3.14batch/s][A
Inferencing with google/muril-base-cased:  66%|██████▌   | 88/133 [00:34<00:20,  2.23batch/s][A
Inferencing with google/muril-base-cased:  67%|██████▋   | 89/133 [00:34<00:19,  2.23batch/s][A
Inferencing with google/muril-base-cased:  68%|██████▊   | 90/133 [00:35<00:18,  2.36batch/s][A
Inferencing with google/muril-base-cased:  68%|██████▊   | 91/133 [00:35<00:17,  2.41batch/s][A
Inferencing with google/muril-base-cased:  69%|██████▉   | 92/133 [00:35<00:16,  2.42batch/s][A
Inferencing with google/muril-base-cased:  70%|██████▉   | 93/133 [00:36<00:16,  2.42batch/s][A
Inferencing with google/muril-base-cased:  71%|███████   | 94/133 [00:36<00:16,  2.37batch/s][A
Inferencing with google/muril-base-cased:  71%|███████▏  | 95/133 [00:37<00:15,  2.43batch/s][A
Inferencing with google/muril-base-cased:  72%|███████▏  | 96/133 [00:37<00:15,  2.43batch/s][A
Inferencing with google/muril-base-cased:  73%|███████▎  | 97/133 [00:38<00:14,  2.43batch/s][A
Inferencing with google/muril-base-cased:  74%|███████▎  | 98/133 [00:38<00:14,  2.50batch/s][A
Inferencing with google/muril-base-cased:  74%|███████▍  | 99/133 [00:38<00:13,  2.60batch/s][A
Inferencing with google/muril-base-cased:  75%|███████▌  | 100/133 [00:39<00:12,  2.73batch/s][A
Inferencing with google/muril-base-cased:  76%|███████▌  | 101/133 [00:39<00:11,  2.82batch/s][A
Inferencing with google/muril-base-cased:  77%|███████▋  | 102/133 [00:39<00:10,  2.87batch/s][A
Inferencing with google/muril-base-cased:  77%|███████▋  | 103/133 [00:40<00:10,  2.91batch/s][A
Inferencing with google/muril-base-cased:  78%|███████▊  | 104/133 [00:40<00:09,  2.99batch/s][A
Inferencing with google/muril-base-cased:  79%|███████▉  | 105/133 [00:40<00:09,  2.98batch/s][A
Inferencing with google/muril-base-cased:  80%|███████▉  | 106/133 [00:41<00:09,  2.84batch/s][A
Inferencing with google/muril-base-cased:  80%|████████  | 107/133 [00:41<00:09,  2.81batch/s][A
Inferencing with google/muril-base-cased:  81%|████████  | 108/133 [00:41<00:08,  2.80batch/s][A
Inferencing with google/muril-base-cased:  82%|████████▏ | 109/133 [00:42<00:09,  2.64batch/s][A
Inferencing with google/muril-base-cased:  83%|████████▎ | 110/133 [00:42<00:08,  2.70batch/s][A
Inferencing with google/muril-base-cased:  83%|████████▎ | 111/133 [00:42<00:08,  2.68batch/s][A
Inferencing with google/muril-base-cased:  84%|████████▍ | 112/133 [00:43<00:07,  2.82batch/s][A
Inferencing with google/muril-base-cased:  85%|████████▍ | 113/133 [00:43<00:07,  2.79batch/s][A
Inferencing with google/muril-base-cased:  86%|████████▌ | 114/133 [00:44<00:06,  2.76batch/s][A
Inferencing with google/muril-base-cased:  86%|████████▋ | 115/133 [00:44<00:06,  2.78batch/s][A
Inferencing with google/muril-base-cased:  87%|████████▋ | 116/133 [00:44<00:06,  2.82batch/s][A
Inferencing with google/muril-base-cased:  88%|████████▊ | 117/133 [00:45<00:05,  2.89batch/s][A
Inferencing with google/muril-base-cased:  89%|████████▊ | 118/133 [00:45<00:05,  2.81batch/s][A
Inferencing with google/muril-base-cased:  89%|████████▉ | 119/133 [00:45<00:05,  2.70batch/s][A
Inferencing with google/muril-base-cased:  90%|█████████ | 120/133 [00:46<00:04,  2.75batch/s][A
Inferencing with google/muril-base-cased:  91%|█████████ | 121/133 [00:46<00:04,  2.66batch/s][A
Inferencing with google/muril-base-cased:  92%|█████████▏| 122/133 [00:46<00:04,  2.57batch/s][A
Inferencing with google/muril-base-cased:  92%|█████████▏| 123/133 [00:47<00:03,  2.56batch/s][A
Inferencing with google/muril-base-cased:  93%|█████████▎| 124/133 [00:47<00:03,  2.52batch/s][A
Inferencing with google/muril-base-cased:  94%|█████████▍| 125/133 [00:48<00:03,  2.57batch/s][A
Inferencing with google/muril-base-cased:  95%|█████████▍| 126/133 [00:48<00:02,  2.54batch/s][A
Inferencing with google/muril-base-cased:  95%|█████████▌| 127/133 [00:48<00:02,  2.50batch/s][A
Inferencing with google/muril-base-cased:  96%|█████████▌| 128/133 [00:49<00:02,  2.49batch/s][A
Inferencing with google/muril-base-cased:  97%|█████████▋| 129/133 [00:49<00:01,  2.54batch/s][A
Inferencing with google/muril-base-cased:  98%|█████████▊| 130/133 [00:50<00:01,  2.52batch/s][A
Inferencing with google/muril-base-cased:  98%|█████████▊| 131/133 [00:50<00:00,  2.52batch/s][A
Inferencing with google/muril-base-cased:  99%|█████████▉| 132/133 [00:50<00:00,  2.57batch/s][AInferencing with google/muril-base-cased: 100%|██████████| 133/133 [00:50<00:00,  2.61batch/s]
2026-01-27 18:37:05,003 - sentiment_analysis.batch_inference - INFO - google/muril-base-cased: Processed 2113 texts in 50967.40ms (avg 24.01ms per text)
Processing models:  50%|█████     | 1/2 [00:50<00:50, 50.97s/model]2026-01-27 18:37:05,004 - sentiment_analysis.batch_inference - INFO - Running inference with FacebookAI/xlm-roberta-base...

Inferencing with FacebookAI/xlm-roberta-base:   0%|          | 0/133 [00:00<?, ?batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:   1%|          | 1/133 [00:00<00:58,  2.27batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:   2%|▏         | 2/133 [00:00<00:59,  2.21batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:   2%|▏         | 3/133 [00:01<00:55,  2.34batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:   3%|▎         | 4/133 [00:01<00:53,  2.40batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:   4%|▍         | 5/133 [00:02<00:51,  2.48batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:   5%|▍         | 6/133 [00:02<00:49,  2.55batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:   5%|▌         | 7/133 [00:02<00:49,  2.57batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:   6%|▌         | 8/133 [00:03<00:50,  2.47batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:   7%|▋         | 9/133 [00:03<00:49,  2.51batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:   8%|▊         | 10/133 [00:04<00:48,  2.54batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:   8%|▊         | 11/133 [00:04<00:47,  2.56batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:   9%|▉         | 12/133 [00:04<00:46,  2.62batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  10%|▉         | 13/133 [00:05<00:44,  2.68batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  11%|█         | 14/133 [00:05<00:43,  2.74batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  11%|█▏        | 15/133 [00:06<00:48,  2.41batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  12%|█▏        | 16/133 [00:06<00:49,  2.35batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  13%|█▎        | 17/133 [00:06<00:48,  2.38batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  14%|█▎        | 18/133 [00:07<00:48,  2.40batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  14%|█▍        | 19/133 [00:07<00:48,  2.37batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  15%|█▌        | 20/133 [00:08<00:46,  2.42batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  16%|█▌        | 21/133 [00:08<00:46,  2.43batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  17%|█▋        | 22/133 [00:08<00:44,  2.52batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  17%|█▋        | 23/133 [00:09<00:44,  2.49batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  18%|█▊        | 24/133 [00:09<00:42,  2.54batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  19%|█▉        | 25/133 [00:10<00:42,  2.57batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  20%|█▉        | 26/133 [00:10<00:41,  2.55batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  20%|██        | 27/133 [00:10<00:43,  2.46batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  21%|██        | 28/133 [00:11<00:44,  2.35batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  22%|██▏       | 29/133 [00:11<00:45,  2.28batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  23%|██▎       | 30/133 [00:12<00:47,  2.18batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  23%|██▎       | 31/133 [00:12<00:47,  2.13batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  24%|██▍       | 32/133 [00:13<00:49,  2.04batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  25%|██▍       | 33/133 [00:13<00:53,  1.88batch/s][A2026-01-27 18:37:19,026 - sentiment_analysis.batch_inference - ERROR - Error in batch inference at index 528: text input must be of type `str` (single example), `list[str]` (batch or single pretokenized example) or `list[list[str]]` (batch of pretokenized examples) or `list[tuple[list[str], list[str]]]` (batch of pretokenized sequence pairs).

Inferencing with FacebookAI/xlm-roberta-base:  26%|██▋       | 35/133 [00:14<00:40,  2.44batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  27%|██▋       | 36/133 [00:15<00:43,  2.25batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  28%|██▊       | 37/133 [00:15<00:44,  2.17batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  29%|██▊       | 38/133 [00:16<00:47,  2.00batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  29%|██▉       | 39/133 [00:16<00:49,  1.90batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  30%|███       | 40/133 [00:17<00:47,  1.94batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  31%|███       | 41/133 [00:17<00:45,  2.02batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  32%|███▏      | 42/133 [00:18<00:42,  2.13batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  32%|███▏      | 43/133 [00:18<00:41,  2.19batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  33%|███▎      | 44/133 [00:18<00:39,  2.28batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  34%|███▍      | 45/133 [00:19<00:39,  2.25batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  35%|███▍      | 46/133 [00:19<00:38,  2.24batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  35%|███▌      | 47/133 [00:20<00:37,  2.29batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  36%|███▌      | 48/133 [00:20<00:37,  2.29batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  37%|███▋      | 49/133 [00:21<00:38,  2.19batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  38%|███▊      | 50/133 [00:21<00:37,  2.23batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  38%|███▊      | 51/133 [00:22<00:36,  2.25batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  39%|███▉      | 52/133 [00:22<00:35,  2.27batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  40%|███▉      | 53/133 [00:23<00:37,  2.14batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  41%|████      | 54/133 [00:23<00:37,  2.08batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  41%|████▏     | 55/133 [00:24<00:37,  2.05batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  42%|████▏     | 56/133 [00:24<00:37,  2.03batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  43%|████▎     | 57/133 [00:25<00:36,  2.07batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  44%|████▎     | 58/133 [00:25<00:35,  2.11batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  44%|████▍     | 59/133 [00:25<00:36,  2.04batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  45%|████▌     | 60/133 [00:26<00:34,  2.14batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  46%|████▌     | 61/133 [00:26<00:33,  2.18batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  47%|████▋     | 62/133 [00:27<00:31,  2.24batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  47%|████▋     | 63/133 [00:27<00:31,  2.21batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  48%|████▊     | 64/133 [00:28<00:31,  2.21batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  49%|████▉     | 65/133 [00:28<00:30,  2.19batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  50%|████▉     | 66/133 [00:29<00:30,  2.17batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  50%|█████     | 67/133 [00:29<00:30,  2.18batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  51%|█████     | 68/133 [00:30<00:29,  2.19batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  52%|█████▏    | 69/133 [00:30<00:29,  2.16batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  53%|█████▎    | 70/133 [00:30<00:28,  2.20batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  53%|█████▎    | 71/133 [00:31<00:28,  2.15batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  54%|█████▍    | 72/133 [00:31<00:28,  2.14batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  55%|█████▍    | 73/133 [00:32<00:27,  2.15batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  56%|█████▌    | 74/133 [00:32<00:27,  2.15batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  56%|█████▋    | 75/133 [00:33<00:26,  2.16batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  57%|█████▋    | 76/133 [00:33<00:27,  2.10batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  58%|█████▊    | 77/133 [00:34<00:26,  2.08batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  59%|█████▊    | 78/133 [00:34<00:25,  2.13batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  59%|█████▉    | 79/133 [00:35<00:26,  2.06batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  60%|██████    | 80/133 [00:35<00:25,  2.04batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  61%|██████    | 81/133 [00:36<00:25,  2.01batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  62%|██████▏   | 82/133 [00:36<00:26,  1.95batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  62%|██████▏   | 83/133 [00:37<00:25,  1.95batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  63%|██████▎   | 84/133 [00:37<00:25,  1.93batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  64%|██████▍   | 85/133 [00:38<00:24,  1.92batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  65%|██████▍   | 86/133 [00:38<00:25,  1.84batch/s][A2026-01-27 18:37:44,285 - sentiment_analysis.batch_inference - ERROR - Error in batch inference at index 1376: text input must be of type `str` (single example), `list[str]` (batch or single pretokenized example) or `list[list[str]]` (batch of pretokenized examples) or `list[tuple[list[str], list[str]]]` (batch of pretokenized sequence pairs).

Inferencing with FacebookAI/xlm-roberta-base:  65%|██████▌   | 87/133 [00:39<00:21,  2.11batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  66%|██████▌   | 88/133 [00:40<00:30,  1.46batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  67%|██████▋   | 89/133 [00:41<00:30,  1.45batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  68%|██████▊   | 90/133 [00:41<00:27,  1.57batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  68%|██████▊   | 91/133 [00:42<00:25,  1.67batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  69%|██████▉   | 92/133 [00:42<00:23,  1.77batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  70%|██████▉   | 93/133 [00:43<00:22,  1.80batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  71%|███████   | 94/133 [00:43<00:21,  1.80batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  71%|███████▏  | 95/133 [00:44<00:20,  1.90batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  72%|███████▏  | 96/133 [00:44<00:18,  2.03batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  73%|███████▎  | 97/133 [00:45<00:16,  2.12batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  74%|███████▎  | 98/133 [00:45<00:15,  2.21batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  74%|███████▍  | 99/133 [00:45<00:15,  2.22batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  75%|███████▌  | 100/133 [00:46<00:14,  2.30batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  76%|███████▌  | 101/133 [00:46<00:15,  2.12batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  77%|███████▋  | 102/133 [00:47<00:16,  1.93batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  77%|███████▋  | 103/133 [00:48<00:18,  1.62batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  78%|███████▊  | 104/133 [00:49<00:18,  1.57batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  79%|███████▉  | 105/133 [00:49<00:19,  1.46batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  80%|███████▉  | 106/133 [00:50<00:17,  1.52batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  80%|████████  | 107/133 [00:51<00:18,  1.40batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  81%|████████  | 108/133 [00:51<00:17,  1.43batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  82%|████████▏ | 109/133 [00:52<00:17,  1.39batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  83%|████████▎ | 110/133 [00:53<00:17,  1.34batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  83%|████████▎ | 111/133 [00:54<00:15,  1.40batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  84%|████████▍ | 112/133 [00:54<00:14,  1.45batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  85%|████████▍ | 113/133 [00:55<00:12,  1.57batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  86%|████████▌ | 114/133 [00:55<00:12,  1.58batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  86%|████████▋ | 115/133 [00:56<00:11,  1.59batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  87%|████████▋ | 116/133 [00:57<00:10,  1.69batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  88%|████████▊ | 117/133 [00:57<00:08,  1.88batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  89%|████████▊ | 118/133 [00:57<00:07,  1.99batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  89%|████████▉ | 119/133 [00:58<00:06,  2.03batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  90%|█████████ | 120/133 [00:58<00:05,  2.17batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  91%|█████████ | 121/133 [00:59<00:05,  2.17batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  92%|█████████▏| 122/133 [00:59<00:04,  2.22batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  92%|█████████▏| 123/133 [01:00<00:04,  2.25batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  93%|█████████▎| 124/133 [01:00<00:03,  2.26batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  94%|█████████▍| 125/133 [01:00<00:03,  2.27batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  95%|█████████▍| 126/133 [01:01<00:03,  2.33batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  95%|█████████▌| 127/133 [01:01<00:02,  2.29batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  96%|█████████▌| 128/133 [01:02<00:02,  2.32batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  97%|█████████▋| 129/133 [01:02<00:01,  2.30batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  98%|█████████▊| 130/133 [01:03<00:01,  2.28batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  98%|█████████▊| 131/133 [01:03<00:00,  2.24batch/s][A
Inferencing with FacebookAI/xlm-roberta-base:  99%|█████████▉| 132/133 [01:04<00:00,  2.20batch/s][AInferencing with FacebookAI/xlm-roberta-base: 100%|██████████| 133/133 [01:04<00:00,  2.08batch/s]
2026-01-27 18:38:09,057 - sentiment_analysis.batch_inference - INFO - FacebookAI/xlm-roberta-base: Processed 2113 texts in 64053.78ms (avg 30.13ms per text)
Processing models: 100%|██████████| 2/2 [01:55<00:00, 58.67s/model]Processing models: 100%|██████████| 2/2 [01:55<00:00, 57.51s/model]
2026-01-27 18:38:09,058 - sentiment_analysis.batch_inference - INFO - Inference complete for all models
2026-01-27 18:38:09,058 - __main__ - INFO - 
[Step 4/5] Normalizing sentiment predictions...
2026-01-27 18:38:09,063 - __main__ - INFO - 
[Step 5/5] Preparing and saving results...
2026-01-27 18:38:09,079 - sentiment_analysis.results_storage - INFO - Prepared results dataframe with 4226 rows
2026-01-27 18:38:09,080 - __main__ - INFO - 
Saving individual model results...
2026-01-27 18:38:09,116 - sentiment_analysis.results_storage - INFO - Results for google/muril-base-cased saved to /home/saipraneethmeduri/Desktop/Projects/sentiment_analysis_comments/sentiment_analysis/outputs/sentiment_results_google_muril_base_cased.csv
2026-01-27 18:38:09,116 - sentiment_analysis.results_storage - INFO -   - Rows: 2113, Columns: 13
2026-01-27 18:38:09,161 - sentiment_analysis.results_storage - INFO - Results for FacebookAI/xlm-roberta-base saved to /home/saipraneethmeduri/Desktop/Projects/sentiment_analysis_comments/sentiment_analysis/outputs/sentiment_results_FacebookAI_xlm_roberta_base.csv
2026-01-27 18:38:09,161 - sentiment_analysis.results_storage - INFO -   - Rows: 2113, Columns: 13
2026-01-27 18:38:09,161 - __main__ - INFO - Successfully saved 2 model result files
2026-01-27 18:38:11,142 - sentiment_analysis.aggregator - INFO - Comparison CSV saved to /home/saipraneethmeduri/Desktop/Projects/sentiment_analysis_comments/sentiment_analysis/outputs/sentiment_comparison.csv
2026-01-27 18:38:11,142 - sentiment_analysis.aggregator - INFO - Total comments compared: 1383
2026-01-27 18:38:11,187 - __main__ - INFO - 
================================================================================
2026-01-27 18:38:11,188 - __main__ - INFO - SENTIMENT ANALYSIS PIPELINE COMPLETED SUCCESSFULLY
2026-01-27 18:38:11,188 - __main__ - INFO - Results saved to: /home/saipraneethmeduri/Desktop/Projects/sentiment_analysis_comments/sentiment_analysis/outputs
2026-01-27 18:38:11,188 - __main__ - INFO - Per-model result files generated:
2026-01-27 18:38:11,188 - __main__ - INFO -   - google/muril-base-cased: /home/saipraneethmeduri/Desktop/Projects/sentiment_analysis_comments/sentiment_analysis/outputs/sentiment_results_google_muril_base_cased.csv
2026-01-27 18:38:11,188 - __main__ - INFO -   - FacebookAI/xlm-roberta-base: /home/saipraneethmeduri/Desktop/Projects/sentiment_analysis_comments/sentiment_analysis/outputs/sentiment_results_FacebookAI_xlm_roberta_base.csv
2026-01-27 18:38:11,188 - __main__ - INFO - ================================================================================

================================================================================
SENTIMENT ANALYSIS RESULTS SUMMARY
================================================================================
Total Comments Analyzed: 20
Total Models Used: 2
Average Confidence Score: 0.4952
Total Processing Time: 64.05s

Sentiment Distribution (across all models):
  positive: 2082 (49.3%)
  neutral: 2080 (49.2%)
  negative: 64 (1.5%)

Per-Model Metrics:

  google/muril-base-cased
    Device: cpu
    Load Time: 2182.53ms
    Avg Latency: 24.01ms/comment
    Throughput: 41.5 comments/sec
    Avg Confidence: 0.4926

  FacebookAI/xlm-roberta-base
    Device: cpu
    Load Time: 2365.46ms
    Avg Latency: 30.13ms/comment
    Throughput: 33.0 comments/sec
    Avg Confidence: 0.4979

================================================================================

================================================================================
MODEL AGREEMENT AND CONSENSUS ANALYSIS
================================================================================

Average Model Agreement Score: 55.00%

Consensus Statistics:
  Unanimous Agreement (all models agree): 1/20 (5.0%)
  Majority Agreement (>50% models agree): 10/20 (50.0%)
  High Confidence Predictions (>0.8): 0/20 (0.0%)
================================================================================
